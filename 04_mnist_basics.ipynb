{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "! [ -e /content ] && pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[chapter_mnist_basics]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood: Training a Digit Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having seen what it looks like to actually train a variety of models in Chapter 2, **let’s now look under the hood and see exactly what is going on**.  \n",
    "We’ll start by **using computer vision** to introduce fundamental tools and concepts for deep learning.\n",
    "\n",
    "To be exact, we'll discuss the **roles of arrays and tensors and of broadcasting**, a powerful technique for using them expressively.  \n",
    "We'll explain **stochastic gradient descent (SGD)**, the mechanism for learning by updating weights automatically.  \n",
    "We'll discuss the choice of a **loss function** for our basic classification task, and the role of mini-batches.  \n",
    "We'll also describe **the math that a basic neural network** is actually doing.  \n",
    "Finally, we'll put all these pieces together.\n",
    "\n",
    "In future chapters we’ll do deep dives into other applications as well, and see how these concepts and tools generalize.  \n",
    "But this chapter is about **laying foundation stones**. To be frank, that also makes this one of the hardest chapters, because of how these concepts all depend on each other.  \n",
    "Like an arch, all the stones need to be in place for the structure to stay up. Also like an arch, once that happens, it's a powerful structure that can support other things.  \n",
    "But it requires some patience to assemble.\n",
    "\n",
    "Let's begin. The first step is to consider how images are represented in a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixels: The Foundations of Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand what happens in a **computer vision model**, we first have to understand **how computers handle images**.  \n",
    "We'll use one of **the most famous datasets in computer vision, [MNIST](https://en.wikipedia.org/wiki/MNIST_database)**, for our experiments.  \n",
    "MNIST contains images of handwritten digits, collected by the National Institute of Standards and Technology and collated into a machine learning dataset by Yann Lecun and his colleagues.  \n",
    "Lecun used MNIST in 1998 in [Lenet-5](http://yann.lecun.com/exdb/lenet/), the first computer system to demonstrate practically useful recognition of handwritten digit sequences.  \n",
    "This was one of the most important breakthroughs in the history of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sidebar: Tenacity and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The story of deep learning is one of tenacity and grit by a handful of dedicated researchers.  \n",
    "After early hopes (and hype!) neural networks went out of favor in the 1990's and 2000's, and just a handful of researchers kept trying to make them work well.  \n",
    "Three of them, Yann Lecun, Yoshua Bengio, and Geoffrey Hinton, were awarded the highest honor in computer science, the Turing Award (generally considered the \"Nobel Prize of computer science\"),  \n",
    "in 2018 after triumphing despite the deep skepticism and disinterest of the wider machine learning and statistics community.\n",
    "\n",
    "Geoff Hinton has told of how even academic papers showing dramatically better results than anything previously published would be rejected by top journals and conferences, just because they used a neural network.  \n",
    "Yann Lecun's work on convolutional neural networks, which we will study in the next section, showed that these models could read handwritten text—something that had never been achieved before.  \n",
    "However, his breakthrough was ignored by most researchers, even as it was used commercially to read 10% of the checks in the US!\n",
    "\n",
    "In addition to these three Turing Award winners, there are many other researchers who have battled to get us to where we are today.  \n",
    "For instance, Jurgen Schmidhuber (who many believe should have shared in the Turing Award) pioneered many important ideas, \n",
    "including working with his student Sepp Hochreiter on the long short-term memory (LSTM) architecture (widely used for speech recognition and other text modeling tasks, and used in the IMDb example in <<chapter_intro>>).  \n",
    "Perhaps most important of all, Paul Werbos in 1974 invented back-propagation for neural networks, the technique shown in this chapter and used universally for training neural networks ([Werbos 1994](https://books.google.com/books/about/The_Roots_of_Backpropagation.html?id=WdR3OOM2gBwC)).  \n",
    "His development was almost entirely ignored for decades, but today it is considered the most important foundation of modern AI.\n",
    "\n",
    "There is a lesson here for all of us! On your deep learning journey you will face many obstacles, both technical, and (even more difficult) posed by people around you who don't believe you'll be successful.  \n",
    "There's one *guaranteed* way to fail, and that's to stop trying.  \n",
    "We've seen that the only consistent trait amongst every fast.ai student that's gone on to be a world-class practitioner is that they are all very tenacious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End sidebar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this initial tutorial we are just going to try to **create a model that can classify any image as a 3 or a 7**.  \n",
    "So let's download a **sample of MNIST that contains images of just these digits:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.amazonaws.com/fast-ai-sample/mnist_sample.tgz'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLs.MNIST_SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.fast.ai/data.external.html  - downloads them to /home/.fastai folder\n",
    "path = untar_data(URLs.MNIST_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('.')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path  # tells me where it is; path is . because I used this special base_path attribute to path, to tell it where's my starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pathlib.PosixPath"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(path)  # it's a pathlib path object; pathlib is part of the python standard library; p s l doesn't have ls though, so we addded ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path.ls?  # from where it comes from: a library called fastcore, which is a lot of the foundational stuff in fastai that is not dependent on pytorch, pandas or any of these big libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path.ls??  # if you want to see exactly what it does put ?? to get the source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr/>\n",
       "<h3>Path.ls</h3>\n",
       "<blockquote><pre><code>Path.ls(n_max=None, file_type=None, file_exts=None)</code></pre></blockquote><p>Contents of path as a list</p>\n",
       "<p><a href=\"https://fastcore.fast.ai/xtras.html#path.ls\" target=\"_blank\" rel=\"noreferrer noopener\">Show in docs</a></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc(path.ls)  # documentation, to see examples, pictures, tutorials, tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see **what's in this directory by using `ls`**, a method added by fastai.  \n",
    "This method returns an object of a special fastai class called `L`, which has all the same functionality of Python's built-in `list`, plus a lot more.  \n",
    "One of its handy features is that, when printed, it displays **the count of items**, before **listing the items** themselves (if there are more than 10 items, it just shows the first few):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('train'),Path('valid'),Path('labels.csv')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()  # it makes it easier not to have to see the whole set of parent path folders; use ls to see what's in the dataset \n",
    "           # there's a train folder and valid folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset follows a common layout for machine learning datasets: **separate folders for the training set and the validation set (and/or test set)**.  \n",
    "Let's see what's **inside the training set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('train/7'),Path('train/3')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()  # this looks like our bear classifier data set; we downloaded each set of images into a folder based on what its label was (3 or 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a folder of 3s, and a folder of 7s. In machine learning parlance, we say that **\"3\" and \"7\" are the *labels* (or targets)** in this dataset.  \n",
    "Let's take a look in one of these folders (using **`sorted`** to ensure we all get the same order of files):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threes = (path/'train'/'3').ls().sorted()  # they's just numbered \n",
    "sevens = (path/'train'/'7').ls().sorted()\n",
    "threes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we might expect, it's full of image files.  \n",
    "Let’s take a look at one now.  \n",
    "Here’s an image of a **handwritten number 3**, taken from the famous MNIST dataset of handwritten numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9ElEQVR4nM3Or0sDcRjH8c/pgrfBVBjCgibThiKIyTWbWF1bORhGwxARxH/AbtW0JoIGwzXRYhJhtuFY2q1ocLgbe3sGReTuuWbwkx6+r+/zQ/pncX6q+YOldSe6nG3dn8U/rTQ70L8FCGJUewvxl7NTmezNb8xIkvKugr1HSeMP6SrWOVkoTEuSyh0Gm2n3hQyObMnXnxkempRrvgD+gokzwxFAr7U7YXHZ8x4A/Dl7rbu6D2yl3etcw/F3nZgfRVI7rXM7hMUUqzzBec427x26rkmlkzEEa4nnRqnSOH2F0UUx0ePzlbuqMXAHgN6GY9if5xP8dmtHFfwjuQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im3_path = threes[1]        \n",
    "im3 = Image.open(im3_path)\n",
    "im3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.PngImagePlugin.PngImageFile"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(im3)  # PIL is python imaging library (the most popular lib by far for working with images and its a PNG); PIL will come with something that displays the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are using the **`Image` class from the *Python Imaging Library* (PIL)**, which is the most widely used Python package for opening, manipulating, and viewing images.  \n",
    "Jupyter knows about PIL images, so it displays the image for us automatically.\n",
    "\n",
    "In a computer, everything is represented as a number.  \n",
    "**To view the numbers that make up this image, we have to convert it to a *NumPy array* or a *PyTorch tensor***.  \n",
    "For instance, here's what a section of the image looks like, converted to a **NumPy array**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,  29],\n",
       "       [  0,   0,   0,  48, 166, 224],\n",
       "       [  0,  93, 244, 249, 253, 187],\n",
       "       [  0, 107, 253, 253, 230,  48],\n",
       "       [  0,   3,  20,  20,  15,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "one easy way to treat things as numbers is to turn them into an array\n",
    "the array is part of numpy, the most popular array programming library for python\n",
    "so if we pass our PIL image object to array it just converts the image into a bunch of numbers\n",
    "the truth is it was a bunch of numbers the whole time\n",
    "it was stored as bunch of numbers on disk\n",
    "it just that there's this magic thing in jupyter that knows how to display those numbers on the screen\n",
    "an image is just a bunch of numbers on a pc, so we can compute with it\n",
    "'''\n",
    "array(im3)[4:10,4:10]  # this converts the image im3 to numbers in form of an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `4:10` indicates we requested the rows from index 4 (included) to 10 (not included) and the same for the columns**.  \n",
    "NumPy indexes from top to bottom and left to right, so this section is located in the top-left corner of the image.  \n",
    "Here's the same thing as a **PyTorch tensor**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  29],\n",
       "        [  0,   0,   0,  48, 166, 224],\n",
       "        [  0,  93, 244, 249, 253, 187],\n",
       "        [  0, 107, 253, 253, 230,  48],\n",
       "        [  0,   3,  20,  20,  15,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a tensor is the pytorch version of a numpy array\n",
    "a pytorch tensor and numpy array behave nearly identically, much if not most of the time\n",
    "the key thing is a pytorch tensor can also be computed on a gpu, not just cpu\n",
    "so we use tensors because they have all the benefits of numpy arrays plus all the benefits of gpu computation\n",
    "and they have a whole lot of extra functionality as well\n",
    "'''\n",
    "tensor(im3)[4:10,4:10]  # this converts the image im3 to numbers in form of an tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **slice the array** to pick just the part with the top of the digit in it,   \n",
    "and then use a Pandas DataFrame to **color-code the values using a gradient**, which shows us clearly how the image is created from the pixel values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e6e9c_row0_col0, #T_e6e9c_row0_col1, #T_e6e9c_row0_col2, #T_e6e9c_row0_col3, #T_e6e9c_row0_col4, #T_e6e9c_row0_col5, #T_e6e9c_row0_col6, #T_e6e9c_row0_col7, #T_e6e9c_row0_col8, #T_e6e9c_row0_col9, #T_e6e9c_row0_col10, #T_e6e9c_row0_col11, #T_e6e9c_row0_col12, #T_e6e9c_row0_col13, #T_e6e9c_row0_col14, #T_e6e9c_row0_col15, #T_e6e9c_row0_col16, #T_e6e9c_row0_col17, #T_e6e9c_row0_col18, #T_e6e9c_row0_col19, #T_e6e9c_row0_col20, #T_e6e9c_row0_col21, #T_e6e9c_row0_col22, #T_e6e9c_row0_col23, #T_e6e9c_row0_col24, #T_e6e9c_row0_col25, #T_e6e9c_row0_col26, #T_e6e9c_row0_col27, #T_e6e9c_row1_col0, #T_e6e9c_row1_col1, #T_e6e9c_row1_col2, #T_e6e9c_row1_col3, #T_e6e9c_row1_col4, #T_e6e9c_row1_col5, #T_e6e9c_row1_col6, #T_e6e9c_row1_col7, #T_e6e9c_row1_col8, #T_e6e9c_row1_col9, #T_e6e9c_row1_col10, #T_e6e9c_row1_col11, #T_e6e9c_row1_col12, #T_e6e9c_row1_col13, #T_e6e9c_row1_col14, #T_e6e9c_row1_col15, #T_e6e9c_row1_col16, #T_e6e9c_row1_col17, #T_e6e9c_row1_col18, #T_e6e9c_row1_col19, #T_e6e9c_row1_col20, #T_e6e9c_row1_col21, #T_e6e9c_row1_col22, #T_e6e9c_row1_col23, #T_e6e9c_row1_col24, #T_e6e9c_row1_col25, #T_e6e9c_row1_col26, #T_e6e9c_row1_col27, #T_e6e9c_row2_col0, #T_e6e9c_row2_col1, #T_e6e9c_row2_col2, #T_e6e9c_row2_col3, #T_e6e9c_row2_col4, #T_e6e9c_row2_col5, #T_e6e9c_row2_col6, #T_e6e9c_row2_col7, #T_e6e9c_row2_col8, #T_e6e9c_row2_col9, #T_e6e9c_row2_col10, #T_e6e9c_row2_col11, #T_e6e9c_row2_col12, #T_e6e9c_row2_col13, #T_e6e9c_row2_col14, #T_e6e9c_row2_col15, #T_e6e9c_row2_col16, #T_e6e9c_row2_col17, #T_e6e9c_row2_col18, #T_e6e9c_row2_col19, #T_e6e9c_row2_col20, #T_e6e9c_row2_col21, #T_e6e9c_row2_col22, #T_e6e9c_row2_col23, #T_e6e9c_row2_col24, #T_e6e9c_row2_col25, #T_e6e9c_row2_col26, #T_e6e9c_row2_col27, #T_e6e9c_row3_col0, #T_e6e9c_row3_col1, #T_e6e9c_row3_col2, #T_e6e9c_row3_col3, #T_e6e9c_row3_col4, #T_e6e9c_row3_col5, #T_e6e9c_row3_col6, #T_e6e9c_row3_col7, #T_e6e9c_row3_col8, #T_e6e9c_row3_col9, #T_e6e9c_row3_col10, #T_e6e9c_row3_col11, #T_e6e9c_row3_col12, #T_e6e9c_row3_col13, #T_e6e9c_row3_col14, #T_e6e9c_row3_col15, #T_e6e9c_row3_col16, #T_e6e9c_row3_col17, #T_e6e9c_row3_col18, #T_e6e9c_row3_col19, #T_e6e9c_row3_col20, #T_e6e9c_row3_col21, #T_e6e9c_row3_col22, #T_e6e9c_row3_col23, #T_e6e9c_row3_col24, #T_e6e9c_row3_col25, #T_e6e9c_row3_col26, #T_e6e9c_row3_col27, #T_e6e9c_row4_col0, #T_e6e9c_row4_col1, #T_e6e9c_row4_col2, #T_e6e9c_row4_col3, #T_e6e9c_row4_col4, #T_e6e9c_row4_col5, #T_e6e9c_row4_col6, #T_e6e9c_row4_col7, #T_e6e9c_row4_col8, #T_e6e9c_row4_col9, #T_e6e9c_row4_col10, #T_e6e9c_row4_col11, #T_e6e9c_row4_col12, #T_e6e9c_row4_col13, #T_e6e9c_row4_col14, #T_e6e9c_row4_col15, #T_e6e9c_row4_col16, #T_e6e9c_row4_col17, #T_e6e9c_row4_col18, #T_e6e9c_row4_col19, #T_e6e9c_row4_col20, #T_e6e9c_row4_col21, #T_e6e9c_row4_col22, #T_e6e9c_row4_col23, #T_e6e9c_row4_col24, #T_e6e9c_row4_col25, #T_e6e9c_row4_col26, #T_e6e9c_row4_col27, #T_e6e9c_row5_col0, #T_e6e9c_row5_col1, #T_e6e9c_row5_col2, #T_e6e9c_row5_col3, #T_e6e9c_row5_col4, #T_e6e9c_row5_col5, #T_e6e9c_row5_col6, #T_e6e9c_row5_col7, #T_e6e9c_row5_col8, #T_e6e9c_row5_col19, #T_e6e9c_row5_col20, #T_e6e9c_row5_col21, #T_e6e9c_row5_col22, #T_e6e9c_row5_col23, #T_e6e9c_row5_col24, #T_e6e9c_row5_col25, #T_e6e9c_row5_col26, #T_e6e9c_row5_col27, #T_e6e9c_row6_col0, #T_e6e9c_row6_col1, #T_e6e9c_row6_col2, #T_e6e9c_row6_col3, #T_e6e9c_row6_col4, #T_e6e9c_row6_col5, #T_e6e9c_row6_col6, #T_e6e9c_row6_col19, #T_e6e9c_row6_col20, #T_e6e9c_row6_col21, #T_e6e9c_row6_col22, #T_e6e9c_row6_col23, #T_e6e9c_row6_col24, #T_e6e9c_row6_col25, #T_e6e9c_row6_col26, #T_e6e9c_row6_col27, #T_e6e9c_row7_col0, #T_e6e9c_row7_col1, #T_e6e9c_row7_col2, #T_e6e9c_row7_col3, #T_e6e9c_row7_col4, #T_e6e9c_row7_col19, #T_e6e9c_row7_col20, #T_e6e9c_row7_col21, #T_e6e9c_row7_col22, #T_e6e9c_row7_col23, #T_e6e9c_row7_col24, #T_e6e9c_row7_col25, #T_e6e9c_row7_col26, #T_e6e9c_row7_col27, #T_e6e9c_row8_col0, #T_e6e9c_row8_col1, #T_e6e9c_row8_col2, #T_e6e9c_row8_col3, #T_e6e9c_row8_col4, #T_e6e9c_row8_col10, #T_e6e9c_row8_col11, #T_e6e9c_row8_col12, #T_e6e9c_row8_col13, #T_e6e9c_row8_col14, #T_e6e9c_row8_col19, #T_e6e9c_row8_col20, #T_e6e9c_row8_col21, #T_e6e9c_row8_col22, #T_e6e9c_row8_col23, #T_e6e9c_row8_col24, #T_e6e9c_row8_col25, #T_e6e9c_row8_col26, #T_e6e9c_row8_col27, #T_e6e9c_row9_col0, #T_e6e9c_row9_col1, #T_e6e9c_row9_col2, #T_e6e9c_row9_col3, #T_e6e9c_row9_col4, #T_e6e9c_row9_col9, #T_e6e9c_row9_col10, #T_e6e9c_row9_col11, #T_e6e9c_row9_col12, #T_e6e9c_row9_col13, #T_e6e9c_row9_col19, #T_e6e9c_row9_col20, #T_e6e9c_row9_col21, #T_e6e9c_row9_col22, #T_e6e9c_row9_col23, #T_e6e9c_row9_col24, #T_e6e9c_row9_col25, #T_e6e9c_row9_col26, #T_e6e9c_row9_col27, #T_e6e9c_row10_col0, #T_e6e9c_row10_col1, #T_e6e9c_row10_col2, #T_e6e9c_row10_col3, #T_e6e9c_row10_col4, #T_e6e9c_row10_col5, #T_e6e9c_row10_col6, #T_e6e9c_row10_col7, #T_e6e9c_row10_col8, #T_e6e9c_row10_col9, #T_e6e9c_row10_col10, #T_e6e9c_row10_col11, #T_e6e9c_row10_col12, #T_e6e9c_row10_col13, #T_e6e9c_row10_col18, #T_e6e9c_row10_col19, #T_e6e9c_row10_col20, #T_e6e9c_row10_col21, #T_e6e9c_row10_col22, #T_e6e9c_row10_col23, #T_e6e9c_row10_col24, #T_e6e9c_row10_col25, #T_e6e9c_row10_col26, #T_e6e9c_row10_col27, #T_e6e9c_row11_col0, #T_e6e9c_row11_col1, #T_e6e9c_row11_col2, #T_e6e9c_row11_col3, #T_e6e9c_row11_col4, #T_e6e9c_row11_col5, #T_e6e9c_row11_col6, #T_e6e9c_row11_col7, #T_e6e9c_row11_col8, #T_e6e9c_row11_col9, #T_e6e9c_row11_col10, #T_e6e9c_row11_col17, #T_e6e9c_row11_col18, #T_e6e9c_row11_col19, #T_e6e9c_row11_col20, #T_e6e9c_row11_col21, #T_e6e9c_row11_col22, #T_e6e9c_row11_col23, #T_e6e9c_row11_col24, #T_e6e9c_row11_col25, #T_e6e9c_row11_col26, #T_e6e9c_row11_col27, #T_e6e9c_row12_col0, #T_e6e9c_row12_col1, #T_e6e9c_row12_col2, #T_e6e9c_row12_col3, #T_e6e9c_row12_col4, #T_e6e9c_row12_col5, #T_e6e9c_row12_col6, #T_e6e9c_row12_col7, #T_e6e9c_row12_col8, #T_e6e9c_row12_col17, #T_e6e9c_row12_col18, #T_e6e9c_row12_col19, #T_e6e9c_row12_col20, #T_e6e9c_row12_col21, #T_e6e9c_row12_col22, #T_e6e9c_row12_col23, #T_e6e9c_row12_col24, #T_e6e9c_row12_col25, #T_e6e9c_row12_col26, #T_e6e9c_row12_col27, #T_e6e9c_row13_col0, #T_e6e9c_row13_col1, #T_e6e9c_row13_col2, #T_e6e9c_row13_col3, #T_e6e9c_row13_col4, #T_e6e9c_row13_col5, #T_e6e9c_row13_col6, #T_e6e9c_row13_col7, #T_e6e9c_row13_col8, #T_e6e9c_row13_col20, #T_e6e9c_row13_col21, #T_e6e9c_row13_col22, #T_e6e9c_row13_col23, #T_e6e9c_row13_col24, #T_e6e9c_row13_col25, #T_e6e9c_row13_col26, #T_e6e9c_row13_col27, #T_e6e9c_row14_col0, #T_e6e9c_row14_col1, #T_e6e9c_row14_col2, #T_e6e9c_row14_col3, #T_e6e9c_row14_col4, #T_e6e9c_row14_col5, #T_e6e9c_row14_col6, #T_e6e9c_row14_col7, #T_e6e9c_row14_col8, #T_e6e9c_row14_col9, #T_e6e9c_row14_col10, #T_e6e9c_row14_col21, #T_e6e9c_row14_col22, #T_e6e9c_row14_col23, #T_e6e9c_row14_col24, #T_e6e9c_row14_col25, #T_e6e9c_row14_col26, #T_e6e9c_row14_col27, #T_e6e9c_row15_col0, #T_e6e9c_row15_col1, #T_e6e9c_row15_col2, #T_e6e9c_row15_col3, #T_e6e9c_row15_col4, #T_e6e9c_row15_col5, #T_e6e9c_row15_col6, #T_e6e9c_row15_col7, #T_e6e9c_row15_col8, #T_e6e9c_row15_col9, #T_e6e9c_row15_col10, #T_e6e9c_row15_col11, #T_e6e9c_row15_col12, #T_e6e9c_row15_col13, #T_e6e9c_row15_col21, #T_e6e9c_row15_col22, #T_e6e9c_row15_col23, #T_e6e9c_row15_col24, #T_e6e9c_row15_col25, #T_e6e9c_row15_col26, #T_e6e9c_row15_col27, #T_e6e9c_row16_col0, #T_e6e9c_row16_col1, #T_e6e9c_row16_col2, #T_e6e9c_row16_col3, #T_e6e9c_row16_col4, #T_e6e9c_row16_col5, #T_e6e9c_row16_col6, #T_e6e9c_row16_col7, #T_e6e9c_row16_col8, #T_e6e9c_row16_col9, #T_e6e9c_row16_col10, #T_e6e9c_row16_col11, #T_e6e9c_row16_col12, #T_e6e9c_row16_col13, #T_e6e9c_row16_col14, #T_e6e9c_row16_col15, #T_e6e9c_row16_col16, #T_e6e9c_row16_col21, #T_e6e9c_row16_col22, #T_e6e9c_row16_col23, #T_e6e9c_row16_col24, #T_e6e9c_row16_col25, #T_e6e9c_row16_col26, #T_e6e9c_row16_col27, #T_e6e9c_row17_col0, #T_e6e9c_row17_col1, #T_e6e9c_row17_col2, #T_e6e9c_row17_col3, #T_e6e9c_row17_col4, #T_e6e9c_row17_col5, #T_e6e9c_row17_col6, #T_e6e9c_row17_col7, #T_e6e9c_row17_col8, #T_e6e9c_row17_col9, #T_e6e9c_row17_col10, #T_e6e9c_row17_col11, #T_e6e9c_row17_col12, #T_e6e9c_row17_col13, #T_e6e9c_row17_col14, #T_e6e9c_row17_col15, #T_e6e9c_row17_col16, #T_e6e9c_row17_col21, #T_e6e9c_row17_col22, #T_e6e9c_row17_col23, #T_e6e9c_row17_col24, #T_e6e9c_row17_col25, #T_e6e9c_row17_col26, #T_e6e9c_row17_col27, #T_e6e9c_row18_col0, #T_e6e9c_row18_col1, #T_e6e9c_row18_col2, #T_e6e9c_row18_col3, #T_e6e9c_row18_col4, #T_e6e9c_row18_col5, #T_e6e9c_row18_col6, #T_e6e9c_row18_col7, #T_e6e9c_row18_col8, #T_e6e9c_row18_col9, #T_e6e9c_row18_col10, #T_e6e9c_row18_col11, #T_e6e9c_row18_col12, #T_e6e9c_row18_col13, #T_e6e9c_row18_col14, #T_e6e9c_row18_col15, #T_e6e9c_row18_col21, #T_e6e9c_row18_col22, #T_e6e9c_row18_col23, #T_e6e9c_row18_col24, #T_e6e9c_row18_col25, #T_e6e9c_row18_col26, #T_e6e9c_row18_col27, #T_e6e9c_row19_col0, #T_e6e9c_row19_col1, #T_e6e9c_row19_col2, #T_e6e9c_row19_col3, #T_e6e9c_row19_col4, #T_e6e9c_row19_col5, #T_e6e9c_row19_col6, #T_e6e9c_row19_col7, #T_e6e9c_row19_col8, #T_e6e9c_row19_col9, #T_e6e9c_row19_col10, #T_e6e9c_row19_col11, #T_e6e9c_row19_col12, #T_e6e9c_row19_col13, #T_e6e9c_row19_col14, #T_e6e9c_row19_col21, #T_e6e9c_row19_col22, #T_e6e9c_row19_col23, #T_e6e9c_row19_col24, #T_e6e9c_row19_col25, #T_e6e9c_row19_col26, #T_e6e9c_row19_col27, #T_e6e9c_row20_col0, #T_e6e9c_row20_col1, #T_e6e9c_row20_col2, #T_e6e9c_row20_col3, #T_e6e9c_row20_col4, #T_e6e9c_row20_col5, #T_e6e9c_row20_col6, #T_e6e9c_row20_col7, #T_e6e9c_row20_col8, #T_e6e9c_row20_col9, #T_e6e9c_row20_col10, #T_e6e9c_row20_col11, #T_e6e9c_row20_col12, #T_e6e9c_row20_col13, #T_e6e9c_row20_col20, #T_e6e9c_row20_col21, #T_e6e9c_row20_col22, #T_e6e9c_row20_col23, #T_e6e9c_row20_col24, #T_e6e9c_row20_col25, #T_e6e9c_row20_col26, #T_e6e9c_row20_col27, #T_e6e9c_row21_col0, #T_e6e9c_row21_col1, #T_e6e9c_row21_col2, #T_e6e9c_row21_col3, #T_e6e9c_row21_col4, #T_e6e9c_row21_col5, #T_e6e9c_row21_col6, #T_e6e9c_row21_col7, #T_e6e9c_row21_col8, #T_e6e9c_row21_col9, #T_e6e9c_row21_col10, #T_e6e9c_row21_col11, #T_e6e9c_row21_col19, #T_e6e9c_row21_col20, #T_e6e9c_row21_col21, #T_e6e9c_row21_col22, #T_e6e9c_row21_col23, #T_e6e9c_row21_col24, #T_e6e9c_row21_col25, #T_e6e9c_row21_col26, #T_e6e9c_row21_col27, #T_e6e9c_row22_col0, #T_e6e9c_row22_col1, #T_e6e9c_row22_col2, #T_e6e9c_row22_col3, #T_e6e9c_row22_col4, #T_e6e9c_row22_col5, #T_e6e9c_row22_col18, #T_e6e9c_row22_col19, #T_e6e9c_row22_col20, #T_e6e9c_row22_col21, #T_e6e9c_row22_col22, #T_e6e9c_row22_col23, #T_e6e9c_row22_col24, #T_e6e9c_row22_col25, #T_e6e9c_row22_col26, #T_e6e9c_row22_col27, #T_e6e9c_row23_col0, #T_e6e9c_row23_col1, #T_e6e9c_row23_col2, #T_e6e9c_row23_col3, #T_e6e9c_row23_col4, #T_e6e9c_row23_col5, #T_e6e9c_row23_col16, #T_e6e9c_row23_col17, #T_e6e9c_row23_col18, #T_e6e9c_row23_col19, #T_e6e9c_row23_col20, #T_e6e9c_row23_col21, #T_e6e9c_row23_col22, #T_e6e9c_row23_col23, #T_e6e9c_row23_col24, #T_e6e9c_row23_col25, #T_e6e9c_row23_col26, #T_e6e9c_row23_col27, #T_e6e9c_row24_col0, #T_e6e9c_row24_col1, #T_e6e9c_row24_col2, #T_e6e9c_row24_col3, #T_e6e9c_row24_col4, #T_e6e9c_row24_col5, #T_e6e9c_row24_col14, #T_e6e9c_row24_col15, #T_e6e9c_row24_col16, #T_e6e9c_row24_col17, #T_e6e9c_row24_col18, #T_e6e9c_row24_col19, #T_e6e9c_row24_col20, #T_e6e9c_row24_col21, #T_e6e9c_row24_col22, #T_e6e9c_row24_col23, #T_e6e9c_row24_col24, #T_e6e9c_row24_col25, #T_e6e9c_row24_col26, #T_e6e9c_row24_col27, #T_e6e9c_row25_col0, #T_e6e9c_row25_col1, #T_e6e9c_row25_col2, #T_e6e9c_row25_col3, #T_e6e9c_row25_col4, #T_e6e9c_row25_col5, #T_e6e9c_row25_col6, #T_e6e9c_row25_col7, #T_e6e9c_row25_col8, #T_e6e9c_row25_col9, #T_e6e9c_row25_col10, #T_e6e9c_row25_col11, #T_e6e9c_row25_col12, #T_e6e9c_row25_col13, #T_e6e9c_row25_col14, #T_e6e9c_row25_col15, #T_e6e9c_row25_col16, #T_e6e9c_row25_col17, #T_e6e9c_row25_col18, #T_e6e9c_row25_col19, #T_e6e9c_row25_col20, #T_e6e9c_row25_col21, #T_e6e9c_row25_col22, #T_e6e9c_row25_col23, #T_e6e9c_row25_col24, #T_e6e9c_row25_col25, #T_e6e9c_row25_col26, #T_e6e9c_row25_col27, #T_e6e9c_row26_col0, #T_e6e9c_row26_col1, #T_e6e9c_row26_col2, #T_e6e9c_row26_col3, #T_e6e9c_row26_col4, #T_e6e9c_row26_col5, #T_e6e9c_row26_col6, #T_e6e9c_row26_col7, #T_e6e9c_row26_col8, #T_e6e9c_row26_col9, #T_e6e9c_row26_col10, #T_e6e9c_row26_col11, #T_e6e9c_row26_col12, #T_e6e9c_row26_col13, #T_e6e9c_row26_col14, #T_e6e9c_row26_col15, #T_e6e9c_row26_col16, #T_e6e9c_row26_col17, #T_e6e9c_row26_col18, #T_e6e9c_row26_col19, #T_e6e9c_row26_col20, #T_e6e9c_row26_col21, #T_e6e9c_row26_col22, #T_e6e9c_row26_col23, #T_e6e9c_row26_col24, #T_e6e9c_row26_col25, #T_e6e9c_row26_col26, #T_e6e9c_row26_col27, #T_e6e9c_row27_col0, #T_e6e9c_row27_col1, #T_e6e9c_row27_col2, #T_e6e9c_row27_col3, #T_e6e9c_row27_col4, #T_e6e9c_row27_col5, #T_e6e9c_row27_col6, #T_e6e9c_row27_col7, #T_e6e9c_row27_col8, #T_e6e9c_row27_col9, #T_e6e9c_row27_col10, #T_e6e9c_row27_col11, #T_e6e9c_row27_col12, #T_e6e9c_row27_col13, #T_e6e9c_row27_col14, #T_e6e9c_row27_col15, #T_e6e9c_row27_col16, #T_e6e9c_row27_col17, #T_e6e9c_row27_col18, #T_e6e9c_row27_col19, #T_e6e9c_row27_col20, #T_e6e9c_row27_col21, #T_e6e9c_row27_col22, #T_e6e9c_row27_col23, #T_e6e9c_row27_col24, #T_e6e9c_row27_col25, #T_e6e9c_row27_col26, #T_e6e9c_row27_col27 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #ffffff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row5_col9, #T_e6e9c_row12_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f1f1f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row5_col10, #T_e6e9c_row5_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #7c7c7c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row5_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4a4a4a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row5_col12, #T_e6e9c_row5_col13, #T_e6e9c_row5_col14, #T_e6e9c_row6_col10, #T_e6e9c_row6_col11, #T_e6e9c_row6_col15, #T_e6e9c_row6_col16, #T_e6e9c_row6_col17, #T_e6e9c_row7_col8, #T_e6e9c_row7_col16, #T_e6e9c_row7_col17, #T_e6e9c_row8_col5, #T_e6e9c_row8_col6, #T_e6e9c_row8_col7, #T_e6e9c_row8_col16, #T_e6e9c_row8_col17, #T_e6e9c_row9_col16, #T_e6e9c_row10_col15, #T_e6e9c_row13_col15, #T_e6e9c_row14_col15, #T_e6e9c_row14_col16, #T_e6e9c_row14_col17, #T_e6e9c_row14_col18, #T_e6e9c_row15_col17, #T_e6e9c_row15_col18, #T_e6e9c_row15_col19, #T_e6e9c_row16_col18, #T_e6e9c_row16_col19, #T_e6e9c_row16_col20, #T_e6e9c_row17_col18, #T_e6e9c_row17_col19, #T_e6e9c_row18_col18, #T_e6e9c_row18_col19, #T_e6e9c_row19_col17, #T_e6e9c_row19_col18, #T_e6e9c_row20_col16, #T_e6e9c_row20_col17, #T_e6e9c_row21_col15, #T_e6e9c_row21_col16, #T_e6e9c_row23_col7, #T_e6e9c_row23_col8, #T_e6e9c_row23_col9, #T_e6e9c_row23_col10, #T_e6e9c_row23_col11, #T_e6e9c_row24_col7, #T_e6e9c_row24_col8, #T_e6e9c_row24_col9, #T_e6e9c_row24_col10, #T_e6e9c_row24_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #000000;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row5_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #606060;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row5_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4d4d4d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row5_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #bbbbbb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row6_col7, #T_e6e9c_row8_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e4e4e4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row6_col8, #T_e6e9c_row12_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #6b6b6b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row6_col9, #T_e6e9c_row9_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #222222;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row6_col12, #T_e6e9c_row6_col18, #T_e6e9c_row7_col18, #T_e6e9c_row21_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #171717;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row6_col13, #T_e6e9c_row7_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4b4b4b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row6_col14, #T_e6e9c_row11_col14, #T_e6e9c_row12_col12, #T_e6e9c_row12_col14, #T_e6e9c_row13_col12, #T_e6e9c_row13_col14, #T_e6e9c_row22_col14, #T_e6e9c_row23_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #010101;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row7_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #272727;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row7_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #0a0a0a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row7_col7, #T_e6e9c_row18_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #050505;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row7_col9, #T_e6e9c_row12_col15, #T_e6e9c_row14_col19, #T_e6e9c_row23_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #545454;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row7_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e6e6e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row7_col11, #T_e6e9c_row7_col14, #T_e6e9c_row12_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fafafa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row7_col12, #T_e6e9c_row15_col14, #T_e6e9c_row20_col19 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fbfbfb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row7_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fdfdfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row8_col8 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #1b1b1b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row8_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #4e4e4e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row8_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #767676;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row9_col5 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fcfcfc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row9_col6, #T_e6e9c_row9_col7, #T_e6e9c_row19_col20 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f6f6f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row9_col8, #T_e6e9c_row11_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f8f8f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row9_col14, #T_e6e9c_row14_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e8e8e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row9_col17, #T_e6e9c_row10_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #090909;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row9_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d0d0d0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row10_col14, #T_e6e9c_row11_col15, #T_e6e9c_row13_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #060606;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row10_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #979797;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row11_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #b6b6b6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row11_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #252525;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row11_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #999999;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row12_col11, #T_e6e9c_row22_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #101010;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row12_col13, #T_e6e9c_row13_col13, #T_e6e9c_row21_col14, #T_e6e9c_row22_col13, #T_e6e9c_row23_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #020202;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row13_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f7f7f7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row13_col11, #T_e6e9c_row22_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #030303;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row13_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #181818;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row13_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #303030;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row13_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #a9a9a9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row13_col19 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #fefefe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row14_col12, #T_e6e9c_row14_col13, #T_e6e9c_row20_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #bababa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row14_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #393939;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row14_col20 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #eaeaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row15_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e2e2e2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row15_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #9f9f9f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row15_col20 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #898989;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row16_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #585858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row17_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #5a5a5a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row17_col20 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #525252;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row18_col16, #T_e6e9c_row23_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #c5c5c5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row18_col20 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d7d7d7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row19_col15, #T_e6e9c_row22_col9 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #dcdcdc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row19_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #2f2f2f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row19_col19 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #636363;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row20_col15 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #070707;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row20_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #1f1f1f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row21_col12, #T_e6e9c_row22_col6, #T_e6e9c_row22_col8 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e9e9e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row21_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #7d7d7d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row21_col18 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #e1e1e1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row22_col7, #T_e6e9c_row22_col10 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #a4a4a4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row22_col11 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #727272;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row22_col16 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #616161;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row22_col17 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #f3f3f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row23_col14 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #484848;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row24_col6 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #b3b3b3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_e6e9c_row24_col12 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #1a1a1a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_e6e9c_row24_col13 {\n",
       "  font-size: 6pt;\n",
       "  background-color: #d6d6d6;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e6e9c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e6e9c_level0_col0\" class=\"col_heading level0 col0\" >0</th>\n",
       "      <th id=\"T_e6e9c_level0_col1\" class=\"col_heading level0 col1\" >1</th>\n",
       "      <th id=\"T_e6e9c_level0_col2\" class=\"col_heading level0 col2\" >2</th>\n",
       "      <th id=\"T_e6e9c_level0_col3\" class=\"col_heading level0 col3\" >3</th>\n",
       "      <th id=\"T_e6e9c_level0_col4\" class=\"col_heading level0 col4\" >4</th>\n",
       "      <th id=\"T_e6e9c_level0_col5\" class=\"col_heading level0 col5\" >5</th>\n",
       "      <th id=\"T_e6e9c_level0_col6\" class=\"col_heading level0 col6\" >6</th>\n",
       "      <th id=\"T_e6e9c_level0_col7\" class=\"col_heading level0 col7\" >7</th>\n",
       "      <th id=\"T_e6e9c_level0_col8\" class=\"col_heading level0 col8\" >8</th>\n",
       "      <th id=\"T_e6e9c_level0_col9\" class=\"col_heading level0 col9\" >9</th>\n",
       "      <th id=\"T_e6e9c_level0_col10\" class=\"col_heading level0 col10\" >10</th>\n",
       "      <th id=\"T_e6e9c_level0_col11\" class=\"col_heading level0 col11\" >11</th>\n",
       "      <th id=\"T_e6e9c_level0_col12\" class=\"col_heading level0 col12\" >12</th>\n",
       "      <th id=\"T_e6e9c_level0_col13\" class=\"col_heading level0 col13\" >13</th>\n",
       "      <th id=\"T_e6e9c_level0_col14\" class=\"col_heading level0 col14\" >14</th>\n",
       "      <th id=\"T_e6e9c_level0_col15\" class=\"col_heading level0 col15\" >15</th>\n",
       "      <th id=\"T_e6e9c_level0_col16\" class=\"col_heading level0 col16\" >16</th>\n",
       "      <th id=\"T_e6e9c_level0_col17\" class=\"col_heading level0 col17\" >17</th>\n",
       "      <th id=\"T_e6e9c_level0_col18\" class=\"col_heading level0 col18\" >18</th>\n",
       "      <th id=\"T_e6e9c_level0_col19\" class=\"col_heading level0 col19\" >19</th>\n",
       "      <th id=\"T_e6e9c_level0_col20\" class=\"col_heading level0 col20\" >20</th>\n",
       "      <th id=\"T_e6e9c_level0_col21\" class=\"col_heading level0 col21\" >21</th>\n",
       "      <th id=\"T_e6e9c_level0_col22\" class=\"col_heading level0 col22\" >22</th>\n",
       "      <th id=\"T_e6e9c_level0_col23\" class=\"col_heading level0 col23\" >23</th>\n",
       "      <th id=\"T_e6e9c_level0_col24\" class=\"col_heading level0 col24\" >24</th>\n",
       "      <th id=\"T_e6e9c_level0_col25\" class=\"col_heading level0 col25\" >25</th>\n",
       "      <th id=\"T_e6e9c_level0_col26\" class=\"col_heading level0 col26\" >26</th>\n",
       "      <th id=\"T_e6e9c_level0_col27\" class=\"col_heading level0 col27\" >27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e6e9c_row0_col0\" class=\"data row0 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col3\" class=\"data row0 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col5\" class=\"data row0 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col6\" class=\"data row0 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col7\" class=\"data row0 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col8\" class=\"data row0 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col9\" class=\"data row0 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col10\" class=\"data row0 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col11\" class=\"data row0 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col12\" class=\"data row0 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col13\" class=\"data row0 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col14\" class=\"data row0 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col15\" class=\"data row0 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col16\" class=\"data row0 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col17\" class=\"data row0 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col18\" class=\"data row0 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col19\" class=\"data row0 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col20\" class=\"data row0 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col21\" class=\"data row0 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col22\" class=\"data row0 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col23\" class=\"data row0 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col24\" class=\"data row0 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col25\" class=\"data row0 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col26\" class=\"data row0 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row0_col27\" class=\"data row0 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e6e9c_row1_col0\" class=\"data row1 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col1\" class=\"data row1 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col3\" class=\"data row1 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col5\" class=\"data row1 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col6\" class=\"data row1 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col7\" class=\"data row1 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col8\" class=\"data row1 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col9\" class=\"data row1 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col10\" class=\"data row1 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col11\" class=\"data row1 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col12\" class=\"data row1 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col13\" class=\"data row1 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col14\" class=\"data row1 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col15\" class=\"data row1 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col16\" class=\"data row1 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col17\" class=\"data row1 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col18\" class=\"data row1 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col19\" class=\"data row1 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col20\" class=\"data row1 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col21\" class=\"data row1 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col22\" class=\"data row1 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col23\" class=\"data row1 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col24\" class=\"data row1 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col25\" class=\"data row1 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col26\" class=\"data row1 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row1_col27\" class=\"data row1 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e6e9c_row2_col0\" class=\"data row2 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col1\" class=\"data row2 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col2\" class=\"data row2 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col3\" class=\"data row2 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col4\" class=\"data row2 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col5\" class=\"data row2 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col6\" class=\"data row2 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col7\" class=\"data row2 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col8\" class=\"data row2 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col9\" class=\"data row2 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col10\" class=\"data row2 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col11\" class=\"data row2 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col12\" class=\"data row2 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col13\" class=\"data row2 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col14\" class=\"data row2 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col15\" class=\"data row2 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col16\" class=\"data row2 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col17\" class=\"data row2 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col18\" class=\"data row2 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col19\" class=\"data row2 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col20\" class=\"data row2 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col21\" class=\"data row2 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col22\" class=\"data row2 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col23\" class=\"data row2 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col24\" class=\"data row2 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col25\" class=\"data row2 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col26\" class=\"data row2 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row2_col27\" class=\"data row2 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e6e9c_row3_col0\" class=\"data row3 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col1\" class=\"data row3 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col2\" class=\"data row3 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col3\" class=\"data row3 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col4\" class=\"data row3 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col5\" class=\"data row3 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col6\" class=\"data row3 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col7\" class=\"data row3 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col8\" class=\"data row3 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col9\" class=\"data row3 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col10\" class=\"data row3 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col11\" class=\"data row3 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col12\" class=\"data row3 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col13\" class=\"data row3 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col14\" class=\"data row3 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col15\" class=\"data row3 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col16\" class=\"data row3 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col17\" class=\"data row3 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col18\" class=\"data row3 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col19\" class=\"data row3 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col20\" class=\"data row3 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col21\" class=\"data row3 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col22\" class=\"data row3 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col23\" class=\"data row3 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col24\" class=\"data row3 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col25\" class=\"data row3 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col26\" class=\"data row3 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row3_col27\" class=\"data row3 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e6e9c_row4_col0\" class=\"data row4 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col1\" class=\"data row4 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col2\" class=\"data row4 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col3\" class=\"data row4 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col5\" class=\"data row4 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col6\" class=\"data row4 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col7\" class=\"data row4 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col8\" class=\"data row4 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col9\" class=\"data row4 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col10\" class=\"data row4 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col11\" class=\"data row4 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col12\" class=\"data row4 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col13\" class=\"data row4 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col14\" class=\"data row4 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col15\" class=\"data row4 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col16\" class=\"data row4 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col17\" class=\"data row4 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col18\" class=\"data row4 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col19\" class=\"data row4 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col20\" class=\"data row4 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col21\" class=\"data row4 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col22\" class=\"data row4 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col23\" class=\"data row4 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col24\" class=\"data row4 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col25\" class=\"data row4 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col26\" class=\"data row4 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row4_col27\" class=\"data row4 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e6e9c_row5_col0\" class=\"data row5 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col1\" class=\"data row5 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col2\" class=\"data row5 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col3\" class=\"data row5 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col4\" class=\"data row5 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col5\" class=\"data row5 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col6\" class=\"data row5 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col7\" class=\"data row5 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col8\" class=\"data row5 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col9\" class=\"data row5 col9\" >29</td>\n",
       "      <td id=\"T_e6e9c_row5_col10\" class=\"data row5 col10\" >150</td>\n",
       "      <td id=\"T_e6e9c_row5_col11\" class=\"data row5 col11\" >195</td>\n",
       "      <td id=\"T_e6e9c_row5_col12\" class=\"data row5 col12\" >254</td>\n",
       "      <td id=\"T_e6e9c_row5_col13\" class=\"data row5 col13\" >255</td>\n",
       "      <td id=\"T_e6e9c_row5_col14\" class=\"data row5 col14\" >254</td>\n",
       "      <td id=\"T_e6e9c_row5_col15\" class=\"data row5 col15\" >176</td>\n",
       "      <td id=\"T_e6e9c_row5_col16\" class=\"data row5 col16\" >193</td>\n",
       "      <td id=\"T_e6e9c_row5_col17\" class=\"data row5 col17\" >150</td>\n",
       "      <td id=\"T_e6e9c_row5_col18\" class=\"data row5 col18\" >96</td>\n",
       "      <td id=\"T_e6e9c_row5_col19\" class=\"data row5 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col20\" class=\"data row5 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col21\" class=\"data row5 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col22\" class=\"data row5 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col23\" class=\"data row5 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col24\" class=\"data row5 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col25\" class=\"data row5 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col26\" class=\"data row5 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row5_col27\" class=\"data row5 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e6e9c_row6_col0\" class=\"data row6 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col1\" class=\"data row6 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col2\" class=\"data row6 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col3\" class=\"data row6 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col4\" class=\"data row6 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col5\" class=\"data row6 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col6\" class=\"data row6 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col7\" class=\"data row6 col7\" >48</td>\n",
       "      <td id=\"T_e6e9c_row6_col8\" class=\"data row6 col8\" >166</td>\n",
       "      <td id=\"T_e6e9c_row6_col9\" class=\"data row6 col9\" >224</td>\n",
       "      <td id=\"T_e6e9c_row6_col10\" class=\"data row6 col10\" >253</td>\n",
       "      <td id=\"T_e6e9c_row6_col11\" class=\"data row6 col11\" >253</td>\n",
       "      <td id=\"T_e6e9c_row6_col12\" class=\"data row6 col12\" >234</td>\n",
       "      <td id=\"T_e6e9c_row6_col13\" class=\"data row6 col13\" >196</td>\n",
       "      <td id=\"T_e6e9c_row6_col14\" class=\"data row6 col14\" >253</td>\n",
       "      <td id=\"T_e6e9c_row6_col15\" class=\"data row6 col15\" >253</td>\n",
       "      <td id=\"T_e6e9c_row6_col16\" class=\"data row6 col16\" >253</td>\n",
       "      <td id=\"T_e6e9c_row6_col17\" class=\"data row6 col17\" >253</td>\n",
       "      <td id=\"T_e6e9c_row6_col18\" class=\"data row6 col18\" >233</td>\n",
       "      <td id=\"T_e6e9c_row6_col19\" class=\"data row6 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col20\" class=\"data row6 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col21\" class=\"data row6 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col22\" class=\"data row6 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col23\" class=\"data row6 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col24\" class=\"data row6 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col25\" class=\"data row6 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col26\" class=\"data row6 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row6_col27\" class=\"data row6 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e6e9c_row7_col0\" class=\"data row7 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col1\" class=\"data row7 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col2\" class=\"data row7 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col3\" class=\"data row7 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col4\" class=\"data row7 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col5\" class=\"data row7 col5\" >93</td>\n",
       "      <td id=\"T_e6e9c_row7_col6\" class=\"data row7 col6\" >244</td>\n",
       "      <td id=\"T_e6e9c_row7_col7\" class=\"data row7 col7\" >249</td>\n",
       "      <td id=\"T_e6e9c_row7_col8\" class=\"data row7 col8\" >253</td>\n",
       "      <td id=\"T_e6e9c_row7_col9\" class=\"data row7 col9\" >187</td>\n",
       "      <td id=\"T_e6e9c_row7_col10\" class=\"data row7 col10\" >46</td>\n",
       "      <td id=\"T_e6e9c_row7_col11\" class=\"data row7 col11\" >10</td>\n",
       "      <td id=\"T_e6e9c_row7_col12\" class=\"data row7 col12\" >8</td>\n",
       "      <td id=\"T_e6e9c_row7_col13\" class=\"data row7 col13\" >4</td>\n",
       "      <td id=\"T_e6e9c_row7_col14\" class=\"data row7 col14\" >10</td>\n",
       "      <td id=\"T_e6e9c_row7_col15\" class=\"data row7 col15\" >194</td>\n",
       "      <td id=\"T_e6e9c_row7_col16\" class=\"data row7 col16\" >253</td>\n",
       "      <td id=\"T_e6e9c_row7_col17\" class=\"data row7 col17\" >253</td>\n",
       "      <td id=\"T_e6e9c_row7_col18\" class=\"data row7 col18\" >233</td>\n",
       "      <td id=\"T_e6e9c_row7_col19\" class=\"data row7 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col20\" class=\"data row7 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col21\" class=\"data row7 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col22\" class=\"data row7 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col23\" class=\"data row7 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col24\" class=\"data row7 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col25\" class=\"data row7 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col26\" class=\"data row7 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row7_col27\" class=\"data row7 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e6e9c_row8_col0\" class=\"data row8 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col1\" class=\"data row8 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col2\" class=\"data row8 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col3\" class=\"data row8 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col4\" class=\"data row8 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col5\" class=\"data row8 col5\" >107</td>\n",
       "      <td id=\"T_e6e9c_row8_col6\" class=\"data row8 col6\" >253</td>\n",
       "      <td id=\"T_e6e9c_row8_col7\" class=\"data row8 col7\" >253</td>\n",
       "      <td id=\"T_e6e9c_row8_col8\" class=\"data row8 col8\" >230</td>\n",
       "      <td id=\"T_e6e9c_row8_col9\" class=\"data row8 col9\" >48</td>\n",
       "      <td id=\"T_e6e9c_row8_col10\" class=\"data row8 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col11\" class=\"data row8 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col12\" class=\"data row8 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col13\" class=\"data row8 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col14\" class=\"data row8 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col15\" class=\"data row8 col15\" >192</td>\n",
       "      <td id=\"T_e6e9c_row8_col16\" class=\"data row8 col16\" >253</td>\n",
       "      <td id=\"T_e6e9c_row8_col17\" class=\"data row8 col17\" >253</td>\n",
       "      <td id=\"T_e6e9c_row8_col18\" class=\"data row8 col18\" >156</td>\n",
       "      <td id=\"T_e6e9c_row8_col19\" class=\"data row8 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col20\" class=\"data row8 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col21\" class=\"data row8 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col22\" class=\"data row8 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col23\" class=\"data row8 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col24\" class=\"data row8 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col25\" class=\"data row8 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col26\" class=\"data row8 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row8_col27\" class=\"data row8 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e6e9c_row9_col0\" class=\"data row9 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col1\" class=\"data row9 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col2\" class=\"data row9 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col3\" class=\"data row9 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col4\" class=\"data row9 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col5\" class=\"data row9 col5\" >3</td>\n",
       "      <td id=\"T_e6e9c_row9_col6\" class=\"data row9 col6\" >20</td>\n",
       "      <td id=\"T_e6e9c_row9_col7\" class=\"data row9 col7\" >20</td>\n",
       "      <td id=\"T_e6e9c_row9_col8\" class=\"data row9 col8\" >15</td>\n",
       "      <td id=\"T_e6e9c_row9_col9\" class=\"data row9 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col10\" class=\"data row9 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col11\" class=\"data row9 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col12\" class=\"data row9 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col13\" class=\"data row9 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col14\" class=\"data row9 col14\" >43</td>\n",
       "      <td id=\"T_e6e9c_row9_col15\" class=\"data row9 col15\" >224</td>\n",
       "      <td id=\"T_e6e9c_row9_col16\" class=\"data row9 col16\" >253</td>\n",
       "      <td id=\"T_e6e9c_row9_col17\" class=\"data row9 col17\" >245</td>\n",
       "      <td id=\"T_e6e9c_row9_col18\" class=\"data row9 col18\" >74</td>\n",
       "      <td id=\"T_e6e9c_row9_col19\" class=\"data row9 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col20\" class=\"data row9 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col21\" class=\"data row9 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col22\" class=\"data row9 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col23\" class=\"data row9 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col24\" class=\"data row9 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col25\" class=\"data row9 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col26\" class=\"data row9 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row9_col27\" class=\"data row9 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_e6e9c_row10_col0\" class=\"data row10 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col1\" class=\"data row10 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col2\" class=\"data row10 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col3\" class=\"data row10 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col4\" class=\"data row10 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col5\" class=\"data row10 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col6\" class=\"data row10 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col7\" class=\"data row10 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col8\" class=\"data row10 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col9\" class=\"data row10 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col10\" class=\"data row10 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col11\" class=\"data row10 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col12\" class=\"data row10 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col13\" class=\"data row10 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col14\" class=\"data row10 col14\" >249</td>\n",
       "      <td id=\"T_e6e9c_row10_col15\" class=\"data row10 col15\" >253</td>\n",
       "      <td id=\"T_e6e9c_row10_col16\" class=\"data row10 col16\" >245</td>\n",
       "      <td id=\"T_e6e9c_row10_col17\" class=\"data row10 col17\" >126</td>\n",
       "      <td id=\"T_e6e9c_row10_col18\" class=\"data row10 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col19\" class=\"data row10 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col20\" class=\"data row10 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col21\" class=\"data row10 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col22\" class=\"data row10 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col23\" class=\"data row10 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col24\" class=\"data row10 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col25\" class=\"data row10 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col26\" class=\"data row10 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row10_col27\" class=\"data row10 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_e6e9c_row11_col0\" class=\"data row11 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col1\" class=\"data row11 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col2\" class=\"data row11 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col3\" class=\"data row11 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col4\" class=\"data row11 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col5\" class=\"data row11 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col6\" class=\"data row11 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col7\" class=\"data row11 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col8\" class=\"data row11 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col9\" class=\"data row11 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col10\" class=\"data row11 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col11\" class=\"data row11 col11\" >14</td>\n",
       "      <td id=\"T_e6e9c_row11_col12\" class=\"data row11 col12\" >101</td>\n",
       "      <td id=\"T_e6e9c_row11_col13\" class=\"data row11 col13\" >223</td>\n",
       "      <td id=\"T_e6e9c_row11_col14\" class=\"data row11 col14\" >253</td>\n",
       "      <td id=\"T_e6e9c_row11_col15\" class=\"data row11 col15\" >248</td>\n",
       "      <td id=\"T_e6e9c_row11_col16\" class=\"data row11 col16\" >124</td>\n",
       "      <td id=\"T_e6e9c_row11_col17\" class=\"data row11 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col18\" class=\"data row11 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col19\" class=\"data row11 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col20\" class=\"data row11 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col21\" class=\"data row11 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col22\" class=\"data row11 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col23\" class=\"data row11 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col24\" class=\"data row11 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col25\" class=\"data row11 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col26\" class=\"data row11 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row11_col27\" class=\"data row11 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_e6e9c_row12_col0\" class=\"data row12 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col1\" class=\"data row12 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col2\" class=\"data row12 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col3\" class=\"data row12 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col4\" class=\"data row12 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col5\" class=\"data row12 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col6\" class=\"data row12 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col7\" class=\"data row12 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col8\" class=\"data row12 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col9\" class=\"data row12 col9\" >11</td>\n",
       "      <td id=\"T_e6e9c_row12_col10\" class=\"data row12 col10\" >166</td>\n",
       "      <td id=\"T_e6e9c_row12_col11\" class=\"data row12 col11\" >239</td>\n",
       "      <td id=\"T_e6e9c_row12_col12\" class=\"data row12 col12\" >253</td>\n",
       "      <td id=\"T_e6e9c_row12_col13\" class=\"data row12 col13\" >253</td>\n",
       "      <td id=\"T_e6e9c_row12_col14\" class=\"data row12 col14\" >253</td>\n",
       "      <td id=\"T_e6e9c_row12_col15\" class=\"data row12 col15\" >187</td>\n",
       "      <td id=\"T_e6e9c_row12_col16\" class=\"data row12 col16\" >30</td>\n",
       "      <td id=\"T_e6e9c_row12_col17\" class=\"data row12 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col18\" class=\"data row12 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col19\" class=\"data row12 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col20\" class=\"data row12 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col21\" class=\"data row12 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col22\" class=\"data row12 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col23\" class=\"data row12 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col24\" class=\"data row12 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col25\" class=\"data row12 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col26\" class=\"data row12 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row12_col27\" class=\"data row12 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_e6e9c_row13_col0\" class=\"data row13 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col1\" class=\"data row13 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col2\" class=\"data row13 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col3\" class=\"data row13 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col4\" class=\"data row13 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col5\" class=\"data row13 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col6\" class=\"data row13 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col7\" class=\"data row13 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col8\" class=\"data row13 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col9\" class=\"data row13 col9\" >16</td>\n",
       "      <td id=\"T_e6e9c_row13_col10\" class=\"data row13 col10\" >248</td>\n",
       "      <td id=\"T_e6e9c_row13_col11\" class=\"data row13 col11\" >250</td>\n",
       "      <td id=\"T_e6e9c_row13_col12\" class=\"data row13 col12\" >253</td>\n",
       "      <td id=\"T_e6e9c_row13_col13\" class=\"data row13 col13\" >253</td>\n",
       "      <td id=\"T_e6e9c_row13_col14\" class=\"data row13 col14\" >253</td>\n",
       "      <td id=\"T_e6e9c_row13_col15\" class=\"data row13 col15\" >253</td>\n",
       "      <td id=\"T_e6e9c_row13_col16\" class=\"data row13 col16\" >232</td>\n",
       "      <td id=\"T_e6e9c_row13_col17\" class=\"data row13 col17\" >213</td>\n",
       "      <td id=\"T_e6e9c_row13_col18\" class=\"data row13 col18\" >111</td>\n",
       "      <td id=\"T_e6e9c_row13_col19\" class=\"data row13 col19\" >2</td>\n",
       "      <td id=\"T_e6e9c_row13_col20\" class=\"data row13 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col21\" class=\"data row13 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col22\" class=\"data row13 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col23\" class=\"data row13 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col24\" class=\"data row13 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col25\" class=\"data row13 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col26\" class=\"data row13 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row13_col27\" class=\"data row13 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_e6e9c_row14_col0\" class=\"data row14 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col1\" class=\"data row14 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col2\" class=\"data row14 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col3\" class=\"data row14 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col4\" class=\"data row14 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col5\" class=\"data row14 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col6\" class=\"data row14 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col7\" class=\"data row14 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col8\" class=\"data row14 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col9\" class=\"data row14 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col10\" class=\"data row14 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col11\" class=\"data row14 col11\" >43</td>\n",
       "      <td id=\"T_e6e9c_row14_col12\" class=\"data row14 col12\" >98</td>\n",
       "      <td id=\"T_e6e9c_row14_col13\" class=\"data row14 col13\" >98</td>\n",
       "      <td id=\"T_e6e9c_row14_col14\" class=\"data row14 col14\" >208</td>\n",
       "      <td id=\"T_e6e9c_row14_col15\" class=\"data row14 col15\" >253</td>\n",
       "      <td id=\"T_e6e9c_row14_col16\" class=\"data row14 col16\" >253</td>\n",
       "      <td id=\"T_e6e9c_row14_col17\" class=\"data row14 col17\" >253</td>\n",
       "      <td id=\"T_e6e9c_row14_col18\" class=\"data row14 col18\" >253</td>\n",
       "      <td id=\"T_e6e9c_row14_col19\" class=\"data row14 col19\" >187</td>\n",
       "      <td id=\"T_e6e9c_row14_col20\" class=\"data row14 col20\" >22</td>\n",
       "      <td id=\"T_e6e9c_row14_col21\" class=\"data row14 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col22\" class=\"data row14 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col23\" class=\"data row14 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col24\" class=\"data row14 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col25\" class=\"data row14 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col26\" class=\"data row14 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row14_col27\" class=\"data row14 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_e6e9c_row15_col0\" class=\"data row15 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col1\" class=\"data row15 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col2\" class=\"data row15 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col3\" class=\"data row15 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col4\" class=\"data row15 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col5\" class=\"data row15 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col6\" class=\"data row15 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col7\" class=\"data row15 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col8\" class=\"data row15 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col9\" class=\"data row15 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col10\" class=\"data row15 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col11\" class=\"data row15 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col12\" class=\"data row15 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col13\" class=\"data row15 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col14\" class=\"data row15 col14\" >9</td>\n",
       "      <td id=\"T_e6e9c_row15_col15\" class=\"data row15 col15\" >51</td>\n",
       "      <td id=\"T_e6e9c_row15_col16\" class=\"data row15 col16\" >119</td>\n",
       "      <td id=\"T_e6e9c_row15_col17\" class=\"data row15 col17\" >253</td>\n",
       "      <td id=\"T_e6e9c_row15_col18\" class=\"data row15 col18\" >253</td>\n",
       "      <td id=\"T_e6e9c_row15_col19\" class=\"data row15 col19\" >253</td>\n",
       "      <td id=\"T_e6e9c_row15_col20\" class=\"data row15 col20\" >76</td>\n",
       "      <td id=\"T_e6e9c_row15_col21\" class=\"data row15 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col22\" class=\"data row15 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col23\" class=\"data row15 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col24\" class=\"data row15 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col25\" class=\"data row15 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col26\" class=\"data row15 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row15_col27\" class=\"data row15 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_e6e9c_row16_col0\" class=\"data row16 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col1\" class=\"data row16 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col2\" class=\"data row16 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col3\" class=\"data row16 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col4\" class=\"data row16 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col5\" class=\"data row16 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col6\" class=\"data row16 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col7\" class=\"data row16 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col8\" class=\"data row16 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col9\" class=\"data row16 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col10\" class=\"data row16 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col11\" class=\"data row16 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col12\" class=\"data row16 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col13\" class=\"data row16 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col14\" class=\"data row16 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col15\" class=\"data row16 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col16\" class=\"data row16 col16\" >1</td>\n",
       "      <td id=\"T_e6e9c_row16_col17\" class=\"data row16 col17\" >183</td>\n",
       "      <td id=\"T_e6e9c_row16_col18\" class=\"data row16 col18\" >253</td>\n",
       "      <td id=\"T_e6e9c_row16_col19\" class=\"data row16 col19\" >253</td>\n",
       "      <td id=\"T_e6e9c_row16_col20\" class=\"data row16 col20\" >139</td>\n",
       "      <td id=\"T_e6e9c_row16_col21\" class=\"data row16 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col22\" class=\"data row16 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col23\" class=\"data row16 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col24\" class=\"data row16 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col25\" class=\"data row16 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col26\" class=\"data row16 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row16_col27\" class=\"data row16 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_e6e9c_row17_col0\" class=\"data row17 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col1\" class=\"data row17 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col2\" class=\"data row17 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col3\" class=\"data row17 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col4\" class=\"data row17 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col5\" class=\"data row17 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col6\" class=\"data row17 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col7\" class=\"data row17 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col8\" class=\"data row17 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col9\" class=\"data row17 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col10\" class=\"data row17 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col11\" class=\"data row17 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col12\" class=\"data row17 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col13\" class=\"data row17 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col14\" class=\"data row17 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col15\" class=\"data row17 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col16\" class=\"data row17 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col17\" class=\"data row17 col17\" >182</td>\n",
       "      <td id=\"T_e6e9c_row17_col18\" class=\"data row17 col18\" >253</td>\n",
       "      <td id=\"T_e6e9c_row17_col19\" class=\"data row17 col19\" >253</td>\n",
       "      <td id=\"T_e6e9c_row17_col20\" class=\"data row17 col20\" >104</td>\n",
       "      <td id=\"T_e6e9c_row17_col21\" class=\"data row17 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col22\" class=\"data row17 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col23\" class=\"data row17 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col24\" class=\"data row17 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col25\" class=\"data row17 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col26\" class=\"data row17 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row17_col27\" class=\"data row17 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_e6e9c_row18_col0\" class=\"data row18 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col1\" class=\"data row18 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col2\" class=\"data row18 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col3\" class=\"data row18 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col4\" class=\"data row18 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col5\" class=\"data row18 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col6\" class=\"data row18 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col7\" class=\"data row18 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col8\" class=\"data row18 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col9\" class=\"data row18 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col10\" class=\"data row18 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col11\" class=\"data row18 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col12\" class=\"data row18 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col13\" class=\"data row18 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col14\" class=\"data row18 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col15\" class=\"data row18 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col16\" class=\"data row18 col16\" >85</td>\n",
       "      <td id=\"T_e6e9c_row18_col17\" class=\"data row18 col17\" >249</td>\n",
       "      <td id=\"T_e6e9c_row18_col18\" class=\"data row18 col18\" >253</td>\n",
       "      <td id=\"T_e6e9c_row18_col19\" class=\"data row18 col19\" >253</td>\n",
       "      <td id=\"T_e6e9c_row18_col20\" class=\"data row18 col20\" >36</td>\n",
       "      <td id=\"T_e6e9c_row18_col21\" class=\"data row18 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col22\" class=\"data row18 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col23\" class=\"data row18 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col24\" class=\"data row18 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col25\" class=\"data row18 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col26\" class=\"data row18 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row18_col27\" class=\"data row18 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_e6e9c_row19_col0\" class=\"data row19 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col1\" class=\"data row19 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col2\" class=\"data row19 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col3\" class=\"data row19 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col4\" class=\"data row19 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col5\" class=\"data row19 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col6\" class=\"data row19 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col7\" class=\"data row19 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col8\" class=\"data row19 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col9\" class=\"data row19 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col10\" class=\"data row19 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col11\" class=\"data row19 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col12\" class=\"data row19 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col13\" class=\"data row19 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col14\" class=\"data row19 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col15\" class=\"data row19 col15\" >60</td>\n",
       "      <td id=\"T_e6e9c_row19_col16\" class=\"data row19 col16\" >214</td>\n",
       "      <td id=\"T_e6e9c_row19_col17\" class=\"data row19 col17\" >253</td>\n",
       "      <td id=\"T_e6e9c_row19_col18\" class=\"data row19 col18\" >253</td>\n",
       "      <td id=\"T_e6e9c_row19_col19\" class=\"data row19 col19\" >173</td>\n",
       "      <td id=\"T_e6e9c_row19_col20\" class=\"data row19 col20\" >11</td>\n",
       "      <td id=\"T_e6e9c_row19_col21\" class=\"data row19 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col22\" class=\"data row19 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col23\" class=\"data row19 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col24\" class=\"data row19 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col25\" class=\"data row19 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col26\" class=\"data row19 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row19_col27\" class=\"data row19 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_e6e9c_row20_col0\" class=\"data row20 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col1\" class=\"data row20 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col2\" class=\"data row20 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col3\" class=\"data row20 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col4\" class=\"data row20 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col5\" class=\"data row20 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col6\" class=\"data row20 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col7\" class=\"data row20 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col8\" class=\"data row20 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col9\" class=\"data row20 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col10\" class=\"data row20 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col11\" class=\"data row20 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col12\" class=\"data row20 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col13\" class=\"data row20 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col14\" class=\"data row20 col14\" >98</td>\n",
       "      <td id=\"T_e6e9c_row20_col15\" class=\"data row20 col15\" >247</td>\n",
       "      <td id=\"T_e6e9c_row20_col16\" class=\"data row20 col16\" >253</td>\n",
       "      <td id=\"T_e6e9c_row20_col17\" class=\"data row20 col17\" >253</td>\n",
       "      <td id=\"T_e6e9c_row20_col18\" class=\"data row20 col18\" >226</td>\n",
       "      <td id=\"T_e6e9c_row20_col19\" class=\"data row20 col19\" >9</td>\n",
       "      <td id=\"T_e6e9c_row20_col20\" class=\"data row20 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col21\" class=\"data row20 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col22\" class=\"data row20 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col23\" class=\"data row20 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col24\" class=\"data row20 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col25\" class=\"data row20 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col26\" class=\"data row20 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row20_col27\" class=\"data row20 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_e6e9c_row21_col0\" class=\"data row21 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col1\" class=\"data row21 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col2\" class=\"data row21 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col3\" class=\"data row21 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col4\" class=\"data row21 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col5\" class=\"data row21 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col6\" class=\"data row21 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col7\" class=\"data row21 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col8\" class=\"data row21 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col9\" class=\"data row21 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col10\" class=\"data row21 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col11\" class=\"data row21 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col12\" class=\"data row21 col12\" >42</td>\n",
       "      <td id=\"T_e6e9c_row21_col13\" class=\"data row21 col13\" >150</td>\n",
       "      <td id=\"T_e6e9c_row21_col14\" class=\"data row21 col14\" >252</td>\n",
       "      <td id=\"T_e6e9c_row21_col15\" class=\"data row21 col15\" >253</td>\n",
       "      <td id=\"T_e6e9c_row21_col16\" class=\"data row21 col16\" >253</td>\n",
       "      <td id=\"T_e6e9c_row21_col17\" class=\"data row21 col17\" >233</td>\n",
       "      <td id=\"T_e6e9c_row21_col18\" class=\"data row21 col18\" >53</td>\n",
       "      <td id=\"T_e6e9c_row21_col19\" class=\"data row21 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col20\" class=\"data row21 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col21\" class=\"data row21 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col22\" class=\"data row21 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col23\" class=\"data row21 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col24\" class=\"data row21 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col25\" class=\"data row21 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col26\" class=\"data row21 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row21_col27\" class=\"data row21 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_e6e9c_row22_col0\" class=\"data row22 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col1\" class=\"data row22 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col2\" class=\"data row22 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col3\" class=\"data row22 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col4\" class=\"data row22 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col5\" class=\"data row22 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col6\" class=\"data row22 col6\" >42</td>\n",
       "      <td id=\"T_e6e9c_row22_col7\" class=\"data row22 col7\" >115</td>\n",
       "      <td id=\"T_e6e9c_row22_col8\" class=\"data row22 col8\" >42</td>\n",
       "      <td id=\"T_e6e9c_row22_col9\" class=\"data row22 col9\" >60</td>\n",
       "      <td id=\"T_e6e9c_row22_col10\" class=\"data row22 col10\" >115</td>\n",
       "      <td id=\"T_e6e9c_row22_col11\" class=\"data row22 col11\" >159</td>\n",
       "      <td id=\"T_e6e9c_row22_col12\" class=\"data row22 col12\" >240</td>\n",
       "      <td id=\"T_e6e9c_row22_col13\" class=\"data row22 col13\" >253</td>\n",
       "      <td id=\"T_e6e9c_row22_col14\" class=\"data row22 col14\" >253</td>\n",
       "      <td id=\"T_e6e9c_row22_col15\" class=\"data row22 col15\" >250</td>\n",
       "      <td id=\"T_e6e9c_row22_col16\" class=\"data row22 col16\" >175</td>\n",
       "      <td id=\"T_e6e9c_row22_col17\" class=\"data row22 col17\" >25</td>\n",
       "      <td id=\"T_e6e9c_row22_col18\" class=\"data row22 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col19\" class=\"data row22 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col20\" class=\"data row22 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col21\" class=\"data row22 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col22\" class=\"data row22 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col23\" class=\"data row22 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col24\" class=\"data row22 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col25\" class=\"data row22 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col26\" class=\"data row22 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row22_col27\" class=\"data row22 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_e6e9c_row23_col0\" class=\"data row23 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col1\" class=\"data row23 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col2\" class=\"data row23 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col3\" class=\"data row23 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col4\" class=\"data row23 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col5\" class=\"data row23 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col6\" class=\"data row23 col6\" >187</td>\n",
       "      <td id=\"T_e6e9c_row23_col7\" class=\"data row23 col7\" >253</td>\n",
       "      <td id=\"T_e6e9c_row23_col8\" class=\"data row23 col8\" >253</td>\n",
       "      <td id=\"T_e6e9c_row23_col9\" class=\"data row23 col9\" >253</td>\n",
       "      <td id=\"T_e6e9c_row23_col10\" class=\"data row23 col10\" >253</td>\n",
       "      <td id=\"T_e6e9c_row23_col11\" class=\"data row23 col11\" >253</td>\n",
       "      <td id=\"T_e6e9c_row23_col12\" class=\"data row23 col12\" >253</td>\n",
       "      <td id=\"T_e6e9c_row23_col13\" class=\"data row23 col13\" >253</td>\n",
       "      <td id=\"T_e6e9c_row23_col14\" class=\"data row23 col14\" >197</td>\n",
       "      <td id=\"T_e6e9c_row23_col15\" class=\"data row23 col15\" >86</td>\n",
       "      <td id=\"T_e6e9c_row23_col16\" class=\"data row23 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col17\" class=\"data row23 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col18\" class=\"data row23 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col19\" class=\"data row23 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col20\" class=\"data row23 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col21\" class=\"data row23 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col22\" class=\"data row23 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col23\" class=\"data row23 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col24\" class=\"data row23 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col25\" class=\"data row23 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col26\" class=\"data row23 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row23_col27\" class=\"data row23 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_e6e9c_row24_col0\" class=\"data row24 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col1\" class=\"data row24 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col2\" class=\"data row24 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col3\" class=\"data row24 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col4\" class=\"data row24 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col5\" class=\"data row24 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col6\" class=\"data row24 col6\" >103</td>\n",
       "      <td id=\"T_e6e9c_row24_col7\" class=\"data row24 col7\" >253</td>\n",
       "      <td id=\"T_e6e9c_row24_col8\" class=\"data row24 col8\" >253</td>\n",
       "      <td id=\"T_e6e9c_row24_col9\" class=\"data row24 col9\" >253</td>\n",
       "      <td id=\"T_e6e9c_row24_col10\" class=\"data row24 col10\" >253</td>\n",
       "      <td id=\"T_e6e9c_row24_col11\" class=\"data row24 col11\" >253</td>\n",
       "      <td id=\"T_e6e9c_row24_col12\" class=\"data row24 col12\" >232</td>\n",
       "      <td id=\"T_e6e9c_row24_col13\" class=\"data row24 col13\" >67</td>\n",
       "      <td id=\"T_e6e9c_row24_col14\" class=\"data row24 col14\" >1</td>\n",
       "      <td id=\"T_e6e9c_row24_col15\" class=\"data row24 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col16\" class=\"data row24 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col17\" class=\"data row24 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col18\" class=\"data row24 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col19\" class=\"data row24 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col20\" class=\"data row24 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col21\" class=\"data row24 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col22\" class=\"data row24 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col23\" class=\"data row24 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col24\" class=\"data row24 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col25\" class=\"data row24 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col26\" class=\"data row24 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row24_col27\" class=\"data row24 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_e6e9c_row25_col0\" class=\"data row25 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col1\" class=\"data row25 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col2\" class=\"data row25 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col3\" class=\"data row25 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col4\" class=\"data row25 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col5\" class=\"data row25 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col6\" class=\"data row25 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col7\" class=\"data row25 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col8\" class=\"data row25 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col9\" class=\"data row25 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col10\" class=\"data row25 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col11\" class=\"data row25 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col12\" class=\"data row25 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col13\" class=\"data row25 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col14\" class=\"data row25 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col15\" class=\"data row25 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col16\" class=\"data row25 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col17\" class=\"data row25 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col18\" class=\"data row25 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col19\" class=\"data row25 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col20\" class=\"data row25 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col21\" class=\"data row25 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col22\" class=\"data row25 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col23\" class=\"data row25 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col24\" class=\"data row25 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col25\" class=\"data row25 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col26\" class=\"data row25 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row25_col27\" class=\"data row25 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_e6e9c_row26_col0\" class=\"data row26 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col1\" class=\"data row26 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col2\" class=\"data row26 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col3\" class=\"data row26 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col4\" class=\"data row26 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col5\" class=\"data row26 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col6\" class=\"data row26 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col7\" class=\"data row26 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col8\" class=\"data row26 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col9\" class=\"data row26 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col10\" class=\"data row26 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col11\" class=\"data row26 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col12\" class=\"data row26 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col13\" class=\"data row26 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col14\" class=\"data row26 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col15\" class=\"data row26 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col16\" class=\"data row26 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col17\" class=\"data row26 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col18\" class=\"data row26 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col19\" class=\"data row26 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col20\" class=\"data row26 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col21\" class=\"data row26 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col22\" class=\"data row26 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col23\" class=\"data row26 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col24\" class=\"data row26 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col25\" class=\"data row26 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col26\" class=\"data row26 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row26_col27\" class=\"data row26 col27\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e6e9c_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_e6e9c_row27_col0\" class=\"data row27 col0\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col1\" class=\"data row27 col1\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col2\" class=\"data row27 col2\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col3\" class=\"data row27 col3\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col4\" class=\"data row27 col4\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col5\" class=\"data row27 col5\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col6\" class=\"data row27 col6\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col7\" class=\"data row27 col7\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col8\" class=\"data row27 col8\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col9\" class=\"data row27 col9\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col10\" class=\"data row27 col10\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col11\" class=\"data row27 col11\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col12\" class=\"data row27 col12\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col13\" class=\"data row27 col13\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col14\" class=\"data row27 col14\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col15\" class=\"data row27 col15\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col16\" class=\"data row27 col16\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col17\" class=\"data row27 col17\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col18\" class=\"data row27 col18\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col19\" class=\"data row27 col19\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col20\" class=\"data row27 col20\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col21\" class=\"data row27 col21\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col22\" class=\"data row27 col22\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col23\" class=\"data row27 col23\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col24\" class=\"data row27 col24\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col25\" class=\"data row27 col25\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col26\" class=\"data row27 col26\" >0</td>\n",
       "      <td id=\"T_e6e9c_row27_col27\" class=\"data row27 col27\" >0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7eff78158210>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "grab a bit of the image with slicing and turn it into a panda's data frame, \n",
    "because pandas has a very convenient thing called backgroud_gradient()\n",
    "that turns a background into a gradient, as you can see\n",
    "\n",
    "this is super tiny, my phone has millions of pixels\n",
    "'''\n",
    "#hide_output\n",
    "im3_t = tensor(im3)  # image converted into a tensor\n",
    "#df = pd.DataFrame(im3_t[4:15,4:22])  # panda data frame\n",
    "df = pd.DataFrame(im3_t[0:28,0:28])  # panda data frame\n",
    "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"453\" id=\"output_pd_pixels\" src=\"images/att_00058.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the background **white** pixels are stored as the number **0**, **black** is the number **255**, and **shades of gray** are between the two.  \n",
    "**The entire image contains 28 pixels across and 28 pixels down, for a total of 784 pixels.**  \n",
    "(This is much smaller than an image that you would get from a phone camera, which has millions of pixels, but is a convenient size for our initial learning and experiments.  \n",
    "We will build up to bigger, full-color images soon.)\n",
    "\n",
    "So, now you've seen what an image looks like to a computer,  \n",
    "**let's recall our GOAL: create a model (some kind of a computer program) that can recognize 3s and 7s**.  \n",
    "How might you go about getting a computer to do that?\n",
    "\n",
    "> Warning: Stop and Think!:\n",
    "> Before you read on, take a moment to think about how a computer might be able to recognize these two different digits.  \n",
    "> What kinds of features might it be able to look at?  \n",
    "> How might it be able to identify these features?  \n",
    "> How could it combine them together?  \n",
    "> Learning works best when you try to solve problems yourself, rather than just reading somebody else's answers;  \n",
    "> so step away from this book for a few minutes, grab a piece of paper and pen, and jot some ideas down…  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Try: Pixel Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here is a first idea: how about we **find the average pixel value for every pixel of the 3s, then do the same for the 7s**.  \n",
    "This will give us two group averages, defining what we might call the **\"ideal\" 3 and 7**.  \n",
    "**Then, to classify an image as one digit or the other, we see which of these two ideal digits the image is most similar to**.  \n",
    "This certainly seems like it should be better than nothing, so it will make **a good baseline**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Baseline: A simple model which you are confident should perform reasonably well.  \n",
    "> It should be very simple to implement, and very easy to test, so that you can then test each of your improved ideas, and make sure they are always better than your baseline.  \n",
    "> Without starting with a sensible baseline, it is very difficult to know whether your super-fancy models are actually any good.  \n",
    "> One good approach to creating a baseline is doing what we have done here: think of a simple, easy-to-implement model.  \n",
    "> Another good approach is to search around to find other people that have solved similar problems to yours, and download and run their code on your dataset.  \n",
    "> Ideally, try both of these!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step one for our simple model is to get **the average of pixel values for each of our two groups**.  \n",
    "In the process of doing this, we will learn a lot of neat Python numeric programming tricks!\n",
    "\n",
    "Let's create **a tensor containing all of our 3s stacked together**.  \n",
    "We already know how to create a tensor containing a single image.  \n",
    "To create **a tensor containing all the images in a directory**, we will first use a **Python list comprehension** to create **a plain list of the single image tensors**.\n",
    "\n",
    "We will use Jupyter to do some little checks of our work along the way—in this case, making sure that the number of returned items seems reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6131, 6265)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for each of those file names in the sevens,\n",
    "lets Image.open() that file just like we did before, to get a PIL object, \n",
    "and convert that into a tensor\n",
    "and all will be colated into a list with all of the sevens\n",
    "\n",
    "all this thing is a list comprehension (one of the most powerful and useful tools in python)\n",
    "similar with link in c# but not as powerful\n",
    "'''\n",
    "# tensor(img) creates a tensor then you have a tensor of tensors with [tensor(img1), tensor(img2), etc]\n",
    "seven_tensors = [tensor(Image.open(o)) for o in sevens]  # sevens is a list (vector like) that keeps the path to pictures \n",
    "three_tensors = [tensor(Image.open(o)) for o in threes]  # and seven_tensors is a tensor that keeps the all the pictures, as pictures\n",
    "len(three_tensors),len(seven_tensors)                    # see below how an image looks like "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note: **List Comprehensions:** List and dictionary comprehensions are a wonderful feature of Python.  \n",
    "Many Python programmers use them every day, including the authors of this book—they are part of \"idiomatic Python.\"  \n",
    "But programmers coming from other languages may have never seen them before.  \n",
    "There are a lot of great tutorials just a web search away, so we won't spend a long time discussing them now.  \n",
    "Here is a quick explanation and example to get you started. A list comprehension looks like this:  \n",
    "**`new_list = [f(o) for o in a_list if o>0]`.  \n",
    "This will return every element of `a_list` that is greater than 0, after passing it to the function `f`**.  \n",
    "There are three parts here:  \n",
    "the collection you are iterating over (`a_list`),  \n",
    "an optional filter (`if o>0`),  \n",
    "and something to do to each element (`f(o)`).  \n",
    "It's not only shorter to write but way faster than the alternative ways of creating the same list with a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also check that one of the images looks okay.  \n",
    "Since we **now have tensors** (which Jupyter by default will print as values),  \n",
    "**rather than PIL images** (which Jupyter by default will display as images),  \n",
    "we need to use **fastai's `show_image` function** to display it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(three_tensors[1]);  # a tensors needs fastai's function to display the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  29, 150, 195, 254, 255, 254, 176, 193, 150,  96,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,  48, 166, 224, 253, 253, 234, 196, 253, 253, 253, 253, 233,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,  93, 244, 249, 253, 187,  46,  10,   8,   4,  10, 194, 253, 253, 233,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0, 107, 253, 253, 230,  48,   0,   0,   0,   0,   0, 192, 253, 253, 156,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   3,  20,  20,  15,   0,   0,   0,   0,   0,  43, 224, 253, 245,  74,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249, 253, 245, 126,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  14, 101, 223, 253, 248, 124,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 166, 239, 253, 253, 253, 187,  30,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  16, 248, 250, 253, 253, 253, 253, 232, 213, 111,   2,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  43,  98,  98, 208, 253, 253, 253, 253, 187,  22,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   9,  51, 119, 253, 253, 253,  76,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1, 183, 253, 253, 139,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 182, 253, 253, 104,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  85, 249, 253, 253,  36,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  60, 214, 253, 253, 173,  11,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  98, 247, 253, 253, 226,   9,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  42, 150, 252, 253, 253, 233,  53,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,  42, 115,  42,  60, 115, 159, 240, 253, 253, 250, 175,  25,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0, 187, 253, 253, 253, 253, 253, 253, 253, 197,  86,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0, 103, 253, 253, 253, 253, 253, 232,  67,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_tensors[1]  # added by me - jupyter by default prints tensors as values not images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_tensors[1].shape  # the size is a 28 by 28; the rows by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "but three_tensors itself is a list; I can't easily do mathematical computations on that\n",
    "we could stack these 28x28 images on top of eachother, to create like a 3d cube of images\n",
    "it's still called a tensor, and to stack them up you use stack()\n",
    "'''\n",
    "type(three_tensors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For every pixel position**, we want to compute **the average** over all the images of the intensity of that pixel.  \n",
    "**To do this we first combine all the images in this list into a single three-dimensional tensor**.  \n",
    "    - three_tensors is a list and we transform it into a 3-dim tensor  \n",
    "    - why 3 dims?! bc each image is 28x28 and there are 6131 images - so 3 dimensions  \n",
    "    - but why the need for that? a tensor of tensors like three_tensors isn't the same? no, a tensor of tensors holds multiple tensors, and a rank-3 tensor is only one tensor!  \n",
    "The most common way to describe such a tensor is to call it a **rank-3 tensor**.   \n",
    "**We often need to stack up individual tensors in a collection into a single tensor.**  \n",
    "Unsurprisingly, **PyTorch** comes with a function called **`stack`** that we can use for this purpose.\n",
    "\n",
    "Some operations in PyTorch, such as **taking a mean, require us to *cast* our integer types to float types**.  \n",
    "Since we'll be needing this later, we'll also cast our stacked tensor to `float` now.  \n",
    "Casting in PyTorch is as simple as typing the name of the type you wish to cast to, and treating it as a method.\n",
    "\n",
    "**Generally when images are floats, the pixel values are expected to be between 0 and 1**, so we will also divide by 255 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6131, 28, 28])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "we need the averate of all of those 3s\n",
    "so the 1st thing to do is to change a list into a tensor\n",
    "the shape of it is now 6131x28x28\n",
    "it's like a cube of height 6131 by 28 by 28 \n",
    "if we want to take the mean, we're going to turn into floating point values, because we don't want to have integers rounding off\n",
    "and it's kind of a standard in computer vision that when you're working with floats, you expect them to be between 0 and 1\n",
    "so we divide by 255 because they were between 0 and 255 before (the pixels)\n",
    "'''\n",
    "stacked_sevens = torch.stack(seven_tensors).float()/255  # stacked tensors is a single tensor with 3 dims, seven_tensors is a list of tensors rank 2 (a list of matrices; like a vector of vectors)\n",
    "stacked_threes = torch.stack(three_tensors).float()/255  # mean operation needs float, and when images are floats we need values between 0 and 1\n",
    "stacked_threes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(stacked_sevens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(stacked_threes[1]);  # added by me\n",
    "#stacked_threes[1]  # shows numbers between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most important attribute of a tensor is its **shape**.  \n",
    "**This tells you the length of each axis.**   \n",
    "In this case, we can see that we have **6,131 images, each of size 28×28 pixels**.  -> that's why 3 dims    \n",
    "There is nothing specifically about this tensor that says that:  \n",
    "    the first axis is the number of images,  \n",
    "    the second is the height,  \n",
    "    and the third is the width  \n",
    "— the semantics of a tensor are entirely up to us, and how we construct it.  \n",
    "As far as PyTorch is concerned, it is just a bunch of numbers in memory.\n",
    "\n",
    "**The length of a tensor's shape is its rank**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "this is rank 3 tensor, with 3 axis\n",
    "there_tensors[1] was a rank 2 tensor, with 2 axis\n",
    "numpy calls it axis, pytorch calls it dimensions \n",
    "so the rank is also the number of dimensions: ndim\n",
    "'''\n",
    "len(stacked_threes.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is really important for you to commit to memory and practice these bits of tensor jargon:  \n",
    "**_rank_ is the number of axes or dimensions in a tensor;**  \n",
    "**_shape_ is the size of each axis of a tensor.**  (a list containing the size of each axis in a tensor)  \n",
    "\n",
    "> A: Watch out because the term \"dimension\" is sometimes used in two ways.  \n",
    "Consider that we live in \"three-dimensional space\" where a physical position can be described by a 3-vector `v`.  \n",
    "But according to PyTorch, the attribute `v.ndim` (which sure looks like the \"number of dimensions\" of `v`) equals one, not three!  \n",
    "Why? Because `v` is a vector, which is a tensor of rank one, meaning that it has only one _axis_ (even if that axis has a length of three).  \n",
    "In other words, sometimes dimension is used for the size of an axis (\"space is three-dimensional\");  \n",
    "other times, it is used for the rank, or the number of axes (\"a matrix has two dimensions\").  \n",
    "When confused, I find it helpful to translate all statements into terms of rank, axis, and length, which are unambiguous terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a **tensor's rank** directly with **`ndim`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_threes.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we can compute what the ideal 3 looks like.**  \n",
    "We calculate **the mean of all the image tensors by taking the mean along dimension 0** of our stacked, rank-3 tensor.  \n",
    "This is the dimension that indexes over all the images.  \n",
    "\n",
    "In other words, **for every pixel position, this will compute the average of that pixel over all images.**  \n",
    "**The result will be one value for every pixel position, or a single image.**  \n",
    "Here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1415)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "this returns a single number, that's the average pixel across the whole cube, the whole rank three tensor  \n",
    "'''\n",
    "stacked_threes.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "but if we say mean(0) that means take the mean over axis 0; \n",
    "the mean across the images\n",
    "'''\n",
    "mean3 = stacked_threes.mean(0)  # calculate mean along dimension 0\n",
    "show_image(mean3);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "that's now 28x28 again, because we kind of like reduced over this 6131\n",
    "we took the mean across that axis and so we can show that image\n",
    "'''\n",
    "mean3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this dataset, this is **the ideal number 3**! (You may not like it, but this is what peak number 3 performance looks like.)  \n",
    "You can see how it's very dark where all the images agree it should be dark, but it becomes wispy and blurry where the images disagree. \n",
    "\n",
    "Let's do **the same thing for the 7s**, but put all the steps together at once to save some time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean7 = stacked_sevens.mean(0)\n",
    "show_image(mean7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now pick an **arbitrary 3 and measure its distance from our \"ideal digits\"**.\n",
    "\n",
    "> Stop and Think!: How would you calculate how similar a particular image is to each of our ideal digits?  \n",
    "Remember to step away from this book and jot down some ideas before you move on!  \n",
    "Research shows that recall and understanding improves dramatically when you are engaged with the learning process by solving problems, experimenting, and trying new ideas yourself\n",
    "\n",
    "**Here's a sample 3:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "now let's grab any 3\n",
    "and now is this 3 more similar to the ideal 3 or to the ideal 7?\n",
    "and whichever one it's more similar to, I'm going to assume that's the answer \n",
    "'''\n",
    "a_3 = stacked_threes[1]    \n",
    "show_image(a_3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How can we determine its distance from our ideal 3?**  \n",
    "**We can't just add up the differences between the pixels of this image and the ideal digit.  \n",
    "Some differences will be positive while others will be negative, and these differences will cancel out,**  \n",
    "resulting in a situation where an image that is too dark in some places and too light in others might be shown as having zero total differences from the ideal.  \n",
    "That would be misleading!  \n",
    "(we need them all to be positive numbers  \n",
    "there are 2 ways to make them all positive numbers)\n",
    "\n",
    "To avoid this, there are **two main ways data scientists measure distance in this context**:\n",
    "\n",
    "- Take the **mean of the *absolute value* of differences** (absolute value is the function that replaces negative values with positive values). This is called the **mean absolute difference** or **L1 norm**\n",
    "- Take the **mean of the *square* of differences** (which makes everything positive) **and then take the *square root*** (which undoes the squaring). This is called the **root mean squared error** (RMSE) or **L2 norm**.\n",
    "\n",
    "> important: It's Okay to Have Forgotten Your Math: In this book we generally assume that you have completed high school math, and remember at least some of it... But everybody forgets some things! It all depends on what you happen to have had reason to practice in the meantime. Perhaps you have forgotten what a _square root_ is, or exactly how they work. No problem! Any time you come across a maths concept that is not explained fully in this book, don't just keep moving on; instead, stop and look it up. Make sure you understand the basic idea, how it works, and why we might be using it. One of the best places to refresh your understanding is Khan Academy. For instance, Khan Academy has a great [introduction to square roots](https://www.khanacademy.org/math/algebra/x2f8bb11595b61c86:rational-exponents-radicals/x2f8bb11595b61c86:radicals/v/understanding-square-roots)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try both of these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1114), tensor(0.2021))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_3_abs = (a_3 - mean3).abs().mean()  # a_3 is a tensor from position 1 and mean3 and the average of all 3s\n",
    "dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n",
    "dist_3_abs,dist_3_sqr   # this will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_7_abs = (a_3 - mean7).abs().mean()  # this time we'll compare it to the mean of the sevens\n",
    "dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\n",
    "dist_7_abs,dist_7_sqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In both cases, the distance between our 3 and the \"ideal\" 3 is less than the distance to the ideal 7.**  \n",
    "(0.1114 < 0.1586 and 0.2021 < 0.3021)  \n",
    "So our simple model will give the right prediction in this case. (me: it's a 3)  \n",
    "\n",
    "so it's closer to the mean of the 3s than to the mean of the 7s  \n",
    "so we guess therefore that this is a 3  \n",
    "so this is a good baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch already provides both of these as **loss functions**.  \n",
    "You'll find these inside `torch.nn.functional`, which the PyTorch team recommends importing as `F` (and is available by default under that name in fastai):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1586), tensor(0.3021))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()  # original code but float() has no sense there bc a_3 is a stacked_three[1] which is already transformed to float, and the same for mean7\n",
    "F.l1_loss(a_3,mean7), F.mse_loss(a_3,mean7).sqrt()  # mse_loss doesn't do the sqrt by default so we need to pop that in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `mse` stands for *mean squared error*, and `l1` refers to the standard mathematical jargon for *mean absolute value* (in math it's called the *L1 norm*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> S: Intuitively, the difference between L1 norm and mean squared error (MSE) is that the **latter (MSE) will penalize bigger mistakes more heavily than the former(L1 norm) (and be more lenient with small mistakes).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: When I first came across this \"L1\" thingie, I looked it up to see what on earth it meant. I found on Google that it is a _vector norm_ using _absolute value_, so looked up _vector norm_ and started reading: _Given a vector space V over a field F of the real or complex numbers, a norm on V is a nonnegative-valued any function p: V → \\[0,+∞) with the following properties: For all a ∈ F and all u, v ∈ V, p(u + v) ≤ p(u) + p(v)..._ Then I stopped reading. \"Ugh, I'll never understand math!\" I thought, for the thousandth time. Since then I've learned that every time these complex mathy bits of jargon come up in practice, it turns out I can replace them with a tiny bit of code!  \n",
    "Like, the **_L1 loss_ is just equal to `(a-b).abs().mean()`**, here `a` and `b` are tensors.  \n",
    "I guess mathy folks just think differently than me... I'll make sure in this book that every time some mathy jargon comes up, I'll give you the little bit of code it's equal to as well, and explain in common-sense terms what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just completed various mathematical operations on PyTorch tensors.  \n",
    "If you've done some numeric programming in NumPy before, you may recognize these as being similar to NumPy arrays.  \n",
    "**Let's have a look at those two very important data structures.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Arrays and PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[NumPy](https://numpy.org/)** is the most widely used library for scientific and numeric programming in Python.  \n",
    "It provides very similar functionality and a very similar API to that provided by PyTorch;  \n",
    "however, **it does not support using the GPU or calculating gradients**, which are both critical for deep learning.  \n",
    "Therefore, in this book **we will generally use PyTorch tensors instead of NumPy arrays**, where possible.  \n",
    "\n",
    "(Note that fastai adds some features to NumPy and PyTorch to make them a bit more similar to each other.  \n",
    "If any code in this book doesn't work on your computer, it's possible that you forgot to include a line like this at the start of your notebook: `from fastai.vision.all import *`.)\n",
    "\n",
    "But what are arrays and tensors, and why should you care?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is slow compared to many languages. Anything fast in Python, NumPy, or PyTorch is likely to be a wrapper for a compiled object written (and optimized) in another language—specifically C.  \n",
    "In fact, **NumPy arrays and PyTorch tensors can finish computations many thousands of times faster than using pure Python.**\n",
    "\n",
    "**A NumPy array is a multidimensional table of data, with all items of the same type.**  \n",
    "Since that can be any type at all, they can even be arrays of arrays, with **the innermost arrays potentially being different sizes—this is called a \"jagged array.\"** (ca un fierastrau)  \n",
    "By \"multidimensional table\" we mean, for instance, a list (dimension of one), a table or matrix (dimension of two), a \"table of tables\" or \"cube\" (dimension of three), and so forth.  \n",
    "**If the items are all of some simple type such as integer or float, then NumPy will store them as a compact C data structure in memory.**  \n",
    "This is where NumPy shines. NumPy has a wide variety of operators and methods that can run computations on these compact structures at the same speed as optimized C, because they are written in optimized C.\n",
    "\n",
    "**A PyTorch tensor** is nearly the same thing as a NumPy array, but with **an additional restriction that unlocks some additional capabilities.**  \n",
    "It's the same in that it, too, is a multidimensional table of data, with all items of the same type.  \n",
    "**However, the restriction is that a tensor cannot use just any old type—it has to use a single basic numeric type for all components.**     \n",
    "For example, **a PyTorch tensor cannot be jagged. It is always a regularly shaped multidimensional rectangular structure.**  \n",
    "\n",
    "The vast majority of methods and operators supported by NumPy on these structures are also supported by PyTorch, but **PyTorch tensors have additional capabilities**.  \n",
    "One major capability is that **these structures can live on the GPU, in which case their computation will be optimized for the GPU and can run much faster** (given lots of values to work on).  \n",
    "In addition, PyTorch can automatically **calculate derivatives** of these operations, including combinations of operations.  \n",
    "As you'll see, it would be impossible to do deep learning in practice without this capability.  \n",
    "\n",
    "> S: If you don't know what C is, don't worry as you won't need it at all. In a nutshell, it's a low-level  (low-level means more similar to the language that computers use internally) language that is very fast compared to Python.  \n",
    "**To take advantage of its speed while programming in Python, try to avoid as much as possible writing loops, and replace them by commands that work directly on arrays or tensors.**\n",
    "\n",
    "Perhaps the most important new coding skill for a Python programmer to learn is how to **effectively use the array/tensor APIs**.  \n",
    "We will be showing lots more tricks later in this book, but **here's a summary** of the key things you need to know for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create an array or tensor,  \n",
    "pass a list (or list of lists, or list of lists of lists, etc.)  \n",
    "to `array()` or `tensor()`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,2,3],[4,5,6]]\n",
    "arr = array (data)\n",
    "tns = tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr  # numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns  # pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the operations that follow are shown on tensors, but the syntax and results for NumPy arrays is identical.\n",
    "\n",
    "**You can select a row** (note that, like lists in Python, tensors are 0-indexed so 1 refers to the second row/column):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[1]  # row 1; the 2nd row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or **a column**, by using **`:` to indicate *all of the first axis*** (we sometimes refer to the dimensions of tensors/arrays as *axes*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[:,1]  # 1st position shows the row, 2nd position shows the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine these with Python **slice syntax (`[start:end]` with `end` being excluded)** to select part of a row or column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 6])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns[1,1:3]   # row 1, columns 1,2 (not 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can use the standard operators such as `+`, `-`, `*`, `/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4],\n",
       "        [5, 6, 7]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors have a **type**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "this is different from python type, which is a method\n",
    "this tells you it's a tensor\n",
    "if you want to know what kind of a tensor, you have to use type as a method: tns.type()\n",
    "'''\n",
    "type(tns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And will **automatically change type as needed**, for example from `int` to `float`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5000, 3.0000, 4.5000],\n",
       "        [6.0000, 7.5000, 9.0000]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns*1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So, is our baseline model any good?  \n",
    "Our model that compares something to the mean  \n",
    "To quantify this, we must define a metric**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Metrics Using Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that **a metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is**.  \n",
    "For instance, we could use either of the functions we saw in the previous section, **mean squared error, or mean absolute error, and take the average of them over the whole dataset.**   \n",
    "However, neither of these are numbers that are very understandable to most people;  \n",
    "in practice, we normally **use accuracy as the metric for classification models**.  \n",
    "\n",
    "As we've discussed, we want to **calculate our metric over a validation set**.  \n",
    "This is so that we don't inadvertently **overfit—that is, train a model to work well only on our training data**.  \n",
    "This is not really a risk with the pixel similarity model we're using here as a first try, since it has no trained components,  \n",
    "but we'll use a validation set anyway to follow normal practices and to be ready for our second try later.\n",
    "\n",
    "To get a validation set we need to remove some of the data from training entirely, so it is not seen by the model at all.  \n",
    "As it turns out, the creators of the MNIST dataset have already done this for us.  \n",
    "Do you remember how there was a whole separate **directory called *valid***? That's what this directory is for!\n",
    "\n",
    "So to start with, **let's create tensors for our 3s and 7s from that directory (valid)**.  \n",
    "**These are the tensors we will use to calculate a metric measuring the quality of our first-try model, which measures distance from an ideal image**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'3').ls()])  # combine all the steps for valids\n",
    "valid_3_tens = valid_3_tens.float()/255\n",
    "#\n",
    "valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/'valid'/'7').ls()])\n",
    "valid_7_tens = valid_7_tens.float()/255\n",
    "#\n",
    "valid_3_tens.shape, valid_7_tens.shape   # they are just 1k because it's the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to get in **the habit of checking shapes** as you go.  \n",
    "Here we see two tensors, one representing the 3s validation set of 1,010 images of size 28×28, and one representing the 7s validation set of 1,028 images of size 28×28.\n",
    "\n",
    "==\n",
    "\n",
    "We ultimately want to write **a function, `is_3`, that will decide if an arbitrary image is a 3 or a 7.  \n",
    "(will return true if something is a 3)**    \n",
    "**It will do this by deciding which of our two \"ideal digits\" this arbitrary image is closer to.**  \n",
    "(to do that we have to decide whether our digit that we're testing on is closer to the ideal 3 or to ideal 7)  \n",
    "For that we need to define **a notion of distance  \n",
    "— that is, a function that calculates the distance between two images**.  \n",
    "\n",
    "We can write a simple **function that calculates the mean absolute error** using an expression very similar to the one we wrote in the last section:  \n",
    "(takes distance between 2 tensors, takes the absolute value and then takes the mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1114)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "the mean over the last and the second last dimensions (0 is the 1st, -1 is the last, -2 is the second last)\n",
    "so this is going to take the mean acros the x and y axis\n",
    "'''\n",
    "\n",
    "def mnist_distance(a,b):               \n",
    "    return (a-b).abs().mean((-1,-2))   # why -1,-2? https://forums.fast.ai/t/understanding-mean-1-2-in-mnist-distance/84430\n",
    "mnist_distance(a_3, mean3)             # same as dist_a3_abs, cell 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **the same value** we previously calculated for the **distance between these two images,  \n",
    "the ideal 3, `mean3` and the arbitrary sample 3 `a_3`,**    \n",
    "which are both single-image tensors with a shape of `[28,28]`.\n",
    "\n",
    "But in order to calculate **A METRIC FOR OVERALL ACCURACY**,  \n",
    "**we will need to calculate the distance to the ideal 3 for _every_ image in the validation set**.  \n",
    "How do we do that calculation?  \n",
    "We could write a loop over all of the single-image tensors that are stacked within our validation set tensor,  \n",
    "`valid_3_tens`, which has a shape of `[1010,28,28]` representing 1,010 images.  \n",
    "But there is a better way.  \n",
    "\n",
    "Something very interesting happens when **we take this exact same distance function,  \n",
    "designed for comparing two single images,  \n",
    "but pass in as an argument `valid_3_tens`,  \n",
    "the tensor that represents the 3s validation set** (a rank-3 tensors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1086, 0.1092, 0.1124,  ..., 0.1368, 0.1382, 0.1187]),\n",
       " torch.Size([1010]),\n",
       " 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "mnist_distance:\n",
    "    takes the difference between different tensors with broadcasting\n",
    "    then we say the absolute value on something of size 1010x28x28 (difference.shape) \n",
    "    it just calls abs value on each underlying thing\n",
    "    and finally, we call mean: -1 is the last element in python, -2 is the second last\n",
    "    so this is taking the mean over the last 2 axis\n",
    "    and it's going to return just the 1st axis - meaning 1010 elements, each element is the mean of a 784 (28x28) pixels\n",
    "    which is what we want: to know how far away is each of our validation items from the ideal 3\n",
    "'''\n",
    "valid_3_dist = mnist_distance(valid_3_tens, mean3)\n",
    "valid_3_dist, valid_3_dist.shape, valid_3_dist.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1010, 28, 28])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_3_tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "because they don't match it acts like as if there's a 1010 versions of this 28x28 of this mean3\n",
    "so it's going to substract mean3 from every single valid_3_tens \n",
    "'''\n",
    "mean3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of complaining about shapes not matching,  \n",
    "it returned the **distance for every single image as a vector (i.e., a rank-1 tensor) of length 1,010** (the number of 3s in our validation set).  \n",
    "How did that happen?\n",
    "\n",
    "Take another look at our function `mnist_distance`, and you'll see we have there the subtraction `(a-b)`.  \n",
    "The magic trick is that PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use **broadcasting**.  \n",
    "That is, **it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank**.  \n",
    "Broadcasting is an important capability that makes tensor code much easier to write.\n",
    "\n",
    "After broadcasting so the two argument tensors have the same rank,  \n",
    "PyTorch applies its usual logic for two tensors of the same rank:  \n",
    "it performs the operation on each corresponding element of the two tensors, and returns the tensor result.  \n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "element wise operations - adds each element\n",
    "'''\n",
    "tensor([1,2,3]) + tensor([1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "when I have different shapes, it copies 1 for 3 times\n",
    "'''\n",
    "tensor([1,2,3]) + tensor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in this case, PyTorch **treats `mean3`, a rank-2 tensor representing a single image, as if it were 1,010 copies of the same image**,  \n",
    "and then subtracts each of those copies from each 3 in our validation set.  \n",
    "What shape would you expect this tensor to have?  \n",
    "Try to figure it out yourself before you look at the answer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1010, 28, 28])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "valid_3_tens minus 1010 copies of mean3\n",
    "'''\n",
    "(valid_3_tens-mean3).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are calculating **the difference between our \"ideal 3\" and each of the 1,010 3s in the validation set, for each of 28×28 images, resulting in the shape `[1010,28,28]`**.\n",
    "\n",
    "There are a couple of **important points about how broadcasting is implemented**, which make it valuable not just for expressivity but also for **performance**:\n",
    "\n",
    "- **PyTorch doesn't *actually* copy `mean3` 1,010 times. It *pretends* it were a tensor of that shape, but doesn't actually allocate any additional memory**  \n",
    "- **It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU)**, tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!).\n",
    "\n",
    "**This is true of all broadcasting and elementwise operations and functions done in PyTorch.  \n",
    "*It's the most important technique for you to know to create efficient PyTorch code.***\n",
    "\n",
    "Next in `mnist_distance` we see **`abs`**. You might be able to guess now what this does when applied to a tensor.  \n",
    "**It applies the method to each individual element in the tensor, and returns a tensor of the results (that is, it applies the method \"elementwise\").**    \n",
    "So in this case, we'll get back **1,010 matrices of absolute values**. (1010x(28x28))\n",
    "\n",
    "**Finally, our function calls `mean((-1,-2))`**.  \n",
    "The tuple `(-1,-2)` represents **a range of axes**.  \n",
    "In Python, `-1` refers to the *last element*, and `-2` refers to the *second-to-last*.  \n",
    "So in this case, **this tells PyTorch that we want to take the mean ranging over the values indexed by the last two axes of the tensor.  \n",
    "The last two axes are the horizontal and vertical dimensions of an image. (me: the 28x28 axis of each image)  \n",
    "After taking the mean over the last two axes, we are left with just the first tensor axis,  \n",
    "which indexes over our images, which is why our final size was `(1010 means)`.**  \n",
    "**In other words, for every image, we averaged the intensity of all the pixels in that image.**  (so not the average of every pixel from the axis 0, but the average of every image 28x28)\n",
    "\n",
    "We'll be learning lots more about broadcasting throughout this book, especially in <<chapter_foundations>>, and will be practicing it regularly too.\n",
    "\n",
    "We can use `mnist_distance` to figure out whether **an image is a 3 or not** by using the following logic:  \n",
    "**if the distance between the digit in question and the ideal 3 is less than the distance to the ideal 7, then it's a 3.**  \n",
    "This function will automatically do broadcasting and be applied elementwise, just like all PyTorch functions and operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "is the distance between the number in question and the perfect 3 less than \n",
    "the distance between the number in question and the perfect 7?\n",
    "if it is, it's a 3\n",
    "'''\n",
    "\n",
    "def is_3(x): return mnist_distance(x,mean3) < mnist_distance(x,mean7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's test it on our example case:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), tensor(1.))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_3(a_3), is_3(a_3).float()  # we can turn that into a float and yes becomes 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we convert the Boolean response to a float, we get `1.0` for `True` and `0.0` for `False`.  \n",
    "**Thanks to broadcasting, we can also test it on the full validation set of 3s:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "thanks to broadcasting, we can do it for the entire validation set of 3s\n",
    "we basically get rid of loops\n",
    "in this kind of programming, you should have very few loops\n",
    "loops make things much harder to read, and hundreds of thousands of times slower\n",
    "on gpu potentially tens of millions of times slower\n",
    "'''\n",
    "is_3(valid_3_tens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_3(valid_3_tens).float()  # added by me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate **the accuracy for each of the 3s and 7s by taking the average of that function for all 3s and its inverse for all 7s:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9168), tensor(0.9854), tensor(0.9511))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so the accuracy across 3s is about 91%, on 7 is 98%\n",
    "and the average of those 2 is about 95%\n",
    "so here we have a model that's 95% accurate in recognising 3s from 7s\n",
    "it can suprise you that we can do that nothing but arithmetic\n",
    "so that's what I mean by getting a good baseline\n",
    "\n",
    "but it doesn't match Arthur Samuel description of machine learning\n",
    "this is not something where there's a function which has some parameters\n",
    "which we're testing against some kind of measure of fitness\n",
    "and then using that to like improve the parameters iteratively\n",
    "\n",
    "here we just did one step and that's that\n",
    "'''\n",
    "accuracy_3s =      is_3(valid_3_tens).float() .mean()\n",
    "accuracy_7s = (1 - is_3(valid_7_tens).float()).mean()\n",
    "\n",
    "accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2  # me: it's 0.90 because some of the is_3 are not 1 but 0 or 0.5 etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9854)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-is_3(valid_7_tens).float()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a pretty good start! We're getting **over 90% accuracy on both 3s and 7s,** (me: because some of the is_3 are not 1 but 0 or 0.5 etc)  \n",
    "and we've seen how to define a metric conveniently using broadcasting.\n",
    "\n",
    "But let's be honest: 3s and 7s are very different-looking digits.  \n",
    "And we're only classifying 2 out of the 10 possible digits so far.  \n",
    "So we're going to need to do better!\n",
    "\n",
    "To do better, perhaps it is time to **try a system that does some real learning\n",
    "— that is, that can automatically modify itself to improve its performance.**  \n",
    "In other words, it's time to talk about **the training process, and SGD.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember the way that Arthur Samuel **described machine learning**, which we quoted in <<chapter_intro>>?\n",
    "\n",
    "> **Suppose we arrange for some automatic means  \n",
    "of testing the effectiveness of any current weight (parameter) assignment in terms of actual performance  \n",
    "and provide a mechanism for altering the weight assignment so as to maximize the performance.  \n",
    "We need not go into the details of such a procedure to see that it could be made entirely automatic  \n",
    "and to see that a machine so programmed would \"learn\" from its experience.**\n",
    "\n",
    "As we discussed, this is the key to allowing us to have **a model that can get better and better — that can learn.**  \n",
    "**But our pixel similarity approach does not really do this.**  \n",
    "We do not have any kind of weight assignment, or any way of improving based on testing the effectiveness of a weight assignment.  \n",
    "In other words, **we can't really improve our pixel similarity approach by modifying a set of parameters.**    \n",
    "**In order to take advantage of the power of deep learning,  \n",
    "we will first have to represent our task in the way that Arthur Samuel described it.**\n",
    "\n",
    "Instead of trying to find the similarity between an image and an \"ideal image,\"  \n",
    "we could instead **look at each individual pixel and come up with a set of weights for each one,  \n",
    "such that the highest weights are associated with those pixels most likely to be black for a particular category.**  \n",
    "For instance, **pixels toward the bottom right** are not very likely to be activated for a 7, so they should have **a low weight for a 7,**  \n",
    "but they are likely to be activated for an 8, so they should have **a high weight for an 8.**  \n",
    "This can be represented as **a function and set of weight values for each possible category  \n",
    "— for instance the probability of being the number 8:**\n",
    "\n",
    "```\n",
    "def pr_eight(x,w): return (x*w).sum()\n",
    "```\n",
    "> why this formula? a nn multiplies each input by a number of values and adds them (how a nn works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninstead of finding an ideal image and seeing how far away something is from the ideal image\\nwe could come up with a set of weights for each pixel\\nso we're trying to find out if something is the number 3\\nand so we know that like in the places that you would expect to find 3 pixels you could give those high weights\\nif there's a dot in those places, we give it like a high score \\nand if there's dots in other places we give it a low score (why?)\\n\\nso we can actually come up with a function where the probability of something being an 8\\nis equal to pixels in the image (x) multiplied by some sort of weights and them we sum them up\\nso then anywhere where the image we're looking at has pixels (where) there are high weights\\nit's going to end up with high probability \\n\\nhere X is the image, represented as vector, have all the rows stacked up, end to end into a single long line\\n\\nso we're going to use an approach were we're going to start with a vector W (rank 1 tensor)\\n1. that's going to contain random weights (random parameters)\\n2. we'll then predict whether a number appears to be a 3 or a 7 by using this tiny little function pr_eight\\n3. and then we'll figure out how good/accurate our model is (it's loss)\\n4. and then the key step is to calculate the gradient\\nthe gradient is something that measures for each weight \\nif I made it a little bit bigger would the loss get better or worse\\nif I made it a little bit smaller would the loss get better or worse?\\nso if we do that for every weight we can decide for every weight whether we should make that weight a bit bigger or a bit smaller\\nthat's called the gradient\\n5. once we have the gradient we then step (change) all the weights \\nup a little bit for the ones where the gradient said we should make them a bit higher\\nand down a little bit for all the once where the gradient said they should be a little bit lower\\nso now it should be a tiny bit better \\n6. and then we go back to step 2 and calculate a new set of predictions, using this formula\\ncalculate the gradient again, step the weights, keep doing that\\nthis is the flow chart \\n7. and at somw point when we're sick of waiting or when the loss gets good enough we'll stop\\n\\nthese 7 steps are key to training all DL models\\nthis technique is called stochastic gradient descent \\nwell, it's gradient descent, we'll the see the stochastic very soon\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "instead of finding an ideal image and seeing how far away something is from the ideal image\n",
    "we could come up with a set of weights for each pixel\n",
    "so we're trying to find out if something is the number 3\n",
    "and so we know that like in the places that you would expect to find 3 pixels you could give those high weights\n",
    "if there's a dot in those places, we give it like a high score \n",
    "and if there's dots in other places we give it a low score (why?)\n",
    "\n",
    "so we can actually come up with a function where the probability of something being an 8\n",
    "is equal to pixels in the image (x) multiplied by some sort of weights and them we sum them up\n",
    "so then anywhere where the image we're looking at has pixels (where) there are high weights\n",
    "it's going to end up with high probability \n",
    "\n",
    "here X is the image, represented as vector, have all the rows stacked up, end to end into a single long line\n",
    "\n",
    "so we're going to use an approach were we're going to start with a vector W (rank 1 tensor)\n",
    "1. that's going to contain random weights (random parameters)\n",
    "2. we'll then predict whether a number appears to be a 3 or a 7 by using this tiny little function pr_eight\n",
    "3. and then we'll figure out how good/accurate our model is (it's loss)\n",
    "4. and then the key step is to calculate the gradient\n",
    "the gradient is something that measures for each weight \n",
    "if I made it a little bit bigger would the loss get better or worse\n",
    "if I made it a little bit smaller would the loss get better or worse?\n",
    "so if we do that for every weight we can decide for every weight whether we should make that weight a bit bigger or a bit smaller\n",
    "that's called the gradient\n",
    "5. once we have the gradient we then step (change) all the weights \n",
    "up a little bit for the ones where the gradient said we should make them a bit higher\n",
    "and down a little bit for all the once where the gradient said they should be a little bit lower\n",
    "so now it should be a tiny bit better \n",
    "6. and then we go back to step 2 and calculate a new set of predictions, using this formula\n",
    "calculate the gradient again, step the weights, keep doing that\n",
    "this is the flow chart \n",
    "7. and at somw point when we're sick of waiting or when the loss gets good enough we'll stop\n",
    "\n",
    "these 7 steps are key to training all DL models\n",
    "this technique is called stochastic gradient descent \n",
    "well, it's gradient descent, we'll the see the stochastic very soon\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are assuming that **`x` is the image, represented as a vector**  \n",
    "— in other words, with <u>all of the rows stacked up end to end into a single long line</u>.    \n",
    "And we are assuming that **the weights are a vector `w`**.  \n",
    "If we have this function, then we just need some **way to update the weights to make them a little bit better**.  \n",
    "With such an approach, we can repeat that step a number of times,  \n",
    "making the weights better and better, until they are as good as we can make them.\n",
    "\n",
    "**We want to find the specific values for the vector `w`  \n",
    "that causes the result of our function to be high for those images that are actually 8s,  \n",
    "and low for those images that are not.**  \n",
    "**Searching for the best vector `w` is a way to search for the best function for recognising 8s.**  \n",
    "(<u>Because we are not yet using a deep neural network, we are limited by what our function can actually do</u>.  \n",
    "— we are going to fix that constraint later in this chapter.) \n",
    "\n",
    "To be more specific, here are **the steps** that we are going to require,  \n",
    "**to turn this function into a machine learning classifier:**\n",
    "\n",
    "1. ***Initialize*** the weights.\n",
    "1. For each image, **use these weights to *predict*** whether it appears to be a 3 or a 7.\n",
    "1. Based on these predictions, **calculate how good the model is (its *loss*).**\n",
    "1. **Calculate the *gradient***, <u>which measures for each weight, how changing that weight would change the loss</u>\n",
    "1. ***Step* (that is, change) all the weights** based on that calculation.\n",
    "1. Go back to the **step 2, and *repeat*** the process.\n",
    "1. **Iterate until you decide to *stop*** the training process (for instance, because the model is good enough or you don't want to wait any longer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These seven steps, illustrated in <<gradient_descent>>, are the key to the training of all deep learning models.**    \n",
    "That deep learning turns out to rely entirely on these steps is extremely surprising and **counterintuitive**.  \n",
    "It's amazing that this process can solve such complex problems.  \n",
    "But, as you'll see, it really does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.1.0 (20230122.1345)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"661pt\" height=\"78pt\"\n",
       " viewBox=\"0.00 0.00 660.87 78.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-74 656.87,-74 656.87,4 -4,4\"/>\n",
       "<!-- init -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>init</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">init</text>\n",
       "</g>\n",
       "<!-- predict -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>predict</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135.2\" cy=\"-18\" rx=\"44.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.2\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">predict</text>\n",
       "</g>\n",
       "<!-- init&#45;&gt;predict -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>init&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.25,-18C61.85,-18 70.45,-18 79.12,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.96,-21.5 88.96,-18 78.96,-14.5 78.96,-21.5\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"244.99\" cy=\"-52\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"244.99\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predict&#45;&gt;loss -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>predict&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.19,-29.04C182.93,-32.74 196.04,-36.88 207.87,-40.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.69,-43.9 217.28,-43.58 208.8,-37.23 206.69,-43.9\"/>\n",
       "</g>\n",
       "<!-- gradient -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>gradient</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"406.63\" cy=\"-52\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"406.63\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">gradient</text>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;gradient -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>loss&#45;&gt;gradient</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274.16,-52C293.71,-52 320.51,-52 344.81,-52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"344.61,-55.5 354.61,-52 344.61,-48.5 344.61,-55.5\"/>\n",
       "</g>\n",
       "<!-- step -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>step</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"524.23\" cy=\"-18\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"524.23\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">step</text>\n",
       "</g>\n",
       "<!-- gradient&#45;&gt;step -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>gradient&#45;&gt;step</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M446.12,-40.68C458.69,-36.98 472.64,-32.88 485.18,-29.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"486.16,-32.55 494.77,-26.37 484.18,-25.84 486.16,-32.55\"/>\n",
       "</g>\n",
       "<!-- step&#45;&gt;predict -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>step&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M493.24,-18C428.38,-18 274.25,-18 191.3,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.35,-14.5 181.35,-18 191.35,-21.5 191.35,-14.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"315.09\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\">repeat</text>\n",
       "</g>\n",
       "<!-- stop -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"622.32\" cy=\"-18\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"622.32\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">stop</text>\n",
       "</g>\n",
       "<!-- step&#45;&gt;stop -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>step&#45;&gt;stop</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M555.1,-18C563.06,-18 571.79,-18 580.23,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"579.95,-21.5 589.95,-18 579.95,-14.5 579.95,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7eff77bc8bd0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#id gradient_descent\n",
    "#caption The gradient descent process\n",
    "#alt Graph showing the steps for Gradient Descent\n",
    "gv('''\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor each of these 7 steps there are lots of choices around exactly how to do it\\nwe've just kind of hand waved a lot\\n    what kind of random initialization\\n    how do you calculate the gradient\\n    and what step do you take based on the gradient\\n    and how do you decide when to stop\\n\\nin this course we're going to be learning about these steps, that's kind of the part 1\\nthe other part is what is the actual function, neural network\\nso how do we train the thing that we train?\\n\\nso we initialize the parameters with random values\\nwe need some function that's going to be the loss function that will return a number that's small if the performance of the model is good\\nsome way to figure out if the weight should be increased or decreased a bit \\nand then we need to decide when to stop, which is to say let's do a certain number of epochs\\n\\n1.45\\n\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for each of these 7 steps there are lots of choices around exactly how to do it\n",
    "we've just kind of hand waved a lot\n",
    "    what kind of random initialization\n",
    "    how do you calculate the gradient\n",
    "    and what step do you take based on the gradient\n",
    "    and how do you decide when to stop\n",
    "\n",
    "in this course we're going to be learning about these steps, that's kind of the part 1\n",
    "the other part is what is the actual function, neural network\n",
    "so how do we train the thing that we train?\n",
    "\n",
    "so we initialize the parameters with random values\n",
    "we need some function that's going to be the loss function that will return a number that's small if the performance of the model is good\n",
    "some way to figure out if the weight should be increased or decreased a bit \n",
    "and then we need to decide when to stop, which is to say let's do a certain number of epochs\n",
    "\n",
    "1.45\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are many different ways to do each of these seven steps**, and we will be learning about them throughout the rest of this book.  \n",
    "These are the details that make a big difference for deep learning practitioners,  \n",
    "but it turns out that **the general approach to each one generally follows some basic principles**.  \n",
    "**Here are a few guidelines:**\n",
    "\n",
    "- **Initialize**:: **We initialize the parameters to random values**.  \n",
    "This may sound surprising.  \n",
    "There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category  \n",
    "— but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.   \n",
    "- **Loss**:: This is what Samuel referred to when he spoke of ***testing the effectiveness of any current weight assignment in terms of actual performance***.  \n",
    "**We need some function that will return a number that is small if the performance of the model is good**  \n",
    "(the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).  \n",
    "- **Step**:: A simple way to **figure out whether a weight should be increased a bit, or decreased a bit**,  \n",
    "would be just to try it: **increase the weight by a small amount, and see if the loss goes up or down.**    \n",
    "Once you find the correct direction, you could then change that amount by a bit more, and a bit less,  \n",
    "until you find an amount that works well.  \n",
    "However, this is slow! As we will see, the magic of calculus allows us  \n",
    "**to directly figure out in which direction, and by roughly how much, to change each weight,**  \n",
    "without having to try all these small changes.  \n",
    "The way to do this is by calculating ***gradients***.  \n",
    "This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.  \n",
    "- **Stop**:: Once we've decided **how many epochs to train the model for** (a few suggestions for this were given in the earlier list),  \n",
    "we apply that decision. This is where that decision is applied.  \n",
    "For our digit classifier, we would **keep training until the accuracy of the model started getting worse, or we ran out of time.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying these steps to our image classification problem, **let's illustrate what they look like in a simpler case.**    \n",
    "First we will define a very simple function,  \n",
    "**the quadratic — let's pretend that this is our loss function, and `x` is a weight parameter of the function:**  \n",
    ">me: why x is the weight parameter?  \n",
    "we need to minimize the loss function  \n",
    "and the loss is a function of the weight (in this case a quadratic function, but it can be some other function)  \n",
    "why loss is a function of the weight? because we need to find the weights where the loss is minimum!!  \n",
    "more logical would be f(a)=a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so let's go even simpler, we're not going to do mnist\n",
    "we're going to start with this x**2\n",
    "and in fastai we've created a tiny thing called plot function\n",
    "that plots a function \n",
    "'''\n",
    "def f(x): return x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a **graph** of that function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(f, 'x', 'x**2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence of steps we described earlier \n",
    "**starts by picking some random value for a parameter, \n",
    "and calculating the value of the loss:**  \n",
    "> me: step 1 initialize the weights or parameters;  \n",
    "> so if it says pick a random value for a parameter then x is the parameter, -1.5  \n",
    "x=-1.5 and the loss is f(-1.5) because quadratic f is the loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "so there's our loss function f, \n",
    "and we're going to do is we're going to try to find the bottom point\n",
    "we're trying to figure out what is the x value, which is at the bottom\n",
    "so our seven stop procedure requires us to start out by initializing the weights\n",
    "so we need to pick some value\n",
    "so the value we pick is just randomly -1.5\n",
    "\n",
    "so now we need to know if I increased x a bit, does my loss gets a bit better\n",
    "better is smaller, going down, or a bit worse?\n",
    "we can try a slightly higher x, a lower x and see what happens\n",
    "\n",
    "and you can see it's just the slope\n",
    "the slope at this point (X=-1.5) tells you that if I increased x by a bit then \n",
    "my loss will decrease because that is the slope at this point\n",
    "so if we change our weight, our parameter, just a little bit, in the direction of the slope\n",
    "\n",
    "here is the direction of the slope; the big red, horizontal line\n",
    "here is the new value at that point, intersect of the verticall dotted line with the function\n",
    "and then do it again\n",
    "and then do it again\n",
    "eventually we'll get to the bottom of this curve\n",
    "'''\n",
    "plot_function(f, 'x', 'x**2')\n",
    "plt.scatter(-1.5, f(-1.5), color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look to see **what would happen if we increased or decreased our parameter by a little bit — the *adjustment***.  \n",
    "This is simply **the slope** at a particular point:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"A graph showing the squared function with the slope at one point\" width=\"400\" src=\"images/grad_illustration.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can \n",
    "**change our weight by a little in the direction of the slope, (me: increase the parameter/weight)    \n",
    "calculate our loss and adjustment again,  \n",
    "and repeat this a few times**.   \n",
    "Eventually, we will get to **the lowest point on our curve**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is the direction of the slope; the big red, horizontal line  \n",
    "here is the new value at that point, intersect of the verticall dotted line with the function  \n",
    "and then do it again  \n",
    "and then do it again  \n",
    "eventually we'll get to the bottom of this curve  \n",
    "\n",
    "<img alt=\"An illustration of gradient descent\" width=\"400\" src=\"images/chapter2_perfect.svg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic idea goes all the way back to **Isaac Newton**, who pointed out that we can optimize arbitrary functions in this way.  \n",
    "Regardless of how complicated our functions become, **this basic approach of gradient descent will not significantly change**.  \n",
    "The only minor changes we will see later in this book are **some handy ways we can make it faster, by finding better steps**.\n",
    "\n",
    "video:\n",
    "the idea is called newton's method  \n",
    "so a key thing we need to be able to do is to calculate this slope  \n",
    "the bad news is to do that we need calculus  \n",
    "we have to calculate the derivative  \n",
    "1.48.55  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The one magic step is the bit where we calculate the gradients**.    \n",
    "As we mentioned, we use calculus as a performance optimization;  \n",
    "it allows us to more quickly **calculate whether our loss will go up or down when we adjust our parameters up or down**.  \n",
    "In other words, **the gradients will tell us how much we have to change each weight to make our model better**.\n",
    "\n",
    "You may remember from your high school calculus class that  \n",
    "**the *derivative* of a function tells you how much a change in its parameters will change its result**.   \n",
    "If not, don't worry, lots of us forget calculus once high school is behind us!  \n",
    "But you will have to have some intuitive understanding of what a derivative is before you continue,  \n",
    "so if this is all very fuzzy in your head, head over to Khan Academy and complete the **[lessons on basic derivatives](https://www.khanacademy.org/math/differential-calculus/dc-diff-intro)**.  \n",
    "You won't have to know how to calculate them yourselves, **you just have to know what a derivative is.**\n",
    "\n",
    "**The key point about a derivative is this:** <u>for any function, such as the quadratic function we saw in the previous section, we can calculate its derivative</u>.  \n",
    "The derivative is another function.  \n",
    "**It calculates the change, rather than the value.**  \n",
    "For instance, **the derivative of the quadratic function at the value 3 tells us how rapidly the function changes at the value 3.  \n",
    "= THE SPEED**   \n",
    "More specifically, you may recall that **gradient** is defined as ***rise/run***,  \n",
    "that is, **<u>the change in the value of the function, divided by the change in the value of the parameter</u>**.  \n",
    "**When we know how our function will change, then we know what we need to do to make it smaller**.  \n",
    ">(me: because slope=rise/run or delta-function/delta-parameter and is negative  \n",
    "and delta-func is negative because the func value is lower  \n",
    "then we need to increase the param so that delta-param is positive so that the slope is negative  \n",
    "if the param would decrease then delta-param would be negative and the slope positive, but that's not the case)         \n",
    "\n",
    "**This is the key to machine learning: having a way to change the parameters of a function to make it smaller**.  \n",
    "Calculus provides us with a computational shortcut, **the derivative, which lets us directly calculate the gradients of our functions**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important thing to be aware of is that **our function has lots of weights** that we need to adjust,  \n",
    "so when we **calculate the derivative** we won't get back one number, but lots of them — **a gradient for every weight**.  \n",
    "But there is nothing mathematically tricky here;  \n",
    "you can calculate **the derivative with respect to one weight, and treat all the other ones as constant,  \n",
    "then repeat that for each other weight**.   \n",
    "This is how all of the gradients are calculated, for every weight.\n",
    "\n",
    "We mentioned just now that you won't have to calculate any gradients yourself. How can that be?  \n",
    "Amazingly enough, **PyTorch is able to automatically compute the derivative of nearly any function!**  \n",
    "What's more, it does it very fast.  \n",
    "Most of the time, it will be at least as fast as any derivative function that you can create by hand.  \n",
    "Let's see **an example.**\n",
    "\n",
    "First, **let's pick a tensor value which we want gradients at:** (me: the parameter is 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we start with a tensor, and we'll modify this tensor with this special method called requires_grad_\n",
    "and this tells pytorch that any time I do a calculation with this xt\n",
    "it should remember what calculation it does so that I can take the derivative later \n",
    "_ at the end of a method in pytorch means this is called an in-place operation: it actually modifies tensor(3.)\n",
    "so requires_grad_ modifies this tensor to tell pytorch that we want to be calculation gradients on it\n",
    "so that means it's just going to keep track of all of the computations we do so that it can calculate the derivative later\n",
    "'''\n",
    "xt = tensor(3.).requires_grad_()  # xt = xtensor?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the special method `requires_grad_`?  \n",
    "That's the magical incantation we use to tell PyTorch that we want to **calculate gradients with respect to that variable at that value.**    \n",
    "**It is essentially tagging the variable,** so PyTorch will remember to   \n",
    "**keep track of how to compute gradients of the other, direct calculations on it that you will ask for**.  \n",
    "\n",
    "> a: This API might throw you off if you're coming from math or physics.  \n",
    "In those contexts the \"gradient\" of a function is just another function (i.e., its derivative),  \n",
    "so you might expect gradient-related APIs to give you a new function.  \n",
    "**But in deep learning, \"gradients\" usually means the _value_ of a function's derivative at a particular argument value.**  \n",
    "The PyTorch API also puts the focus on the argument, not the function you're actually computing the gradients of.  \n",
    "It may feel backwards at first, but it's just a different perspective.  \n",
    "\n",
    "**Now we calculate our function with that value.**  \n",
    "Notice how PyTorch prints not just the value calculated,  \n",
    "but also a note that it has a gradient function it'll be using to calculate our gradients when needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ok, so we've got the number 3 and call f on it\n",
    "f(3)=9 but the value is not just 9\n",
    "it's 9 accompanied with a grad function which it knows that a power operation has been taken\n",
    "'''\n",
    "yt = f(xt)  # xt is 3 so yt is the loss at 3\n",
    "yt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we tell PyTorch to calculate the gradients for us:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so we can now call a special method, backward\n",
    "which refers to backpropagation, and it means take the derivative\n",
    "'''\n",
    "yt.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The \"backward\" here refers to *backpropagation***,  \n",
    "which is the name given to **the process of calculating the derivative of each layer**.   \n",
    "We'll see how this is done exactly in chapter <<chapter_foundations>>,  \n",
    "when we calculate the gradients of a deep neural net from scratch.  \n",
    "This is called **the \"backward pass\" of the network, as opposed to the \"forward pass,\"**    \n",
    "which is where the activations are calculated.  \n",
    "Life would probably be easier if `backward` was just called `calculate_grad`,  \n",
    "but deep learning folks really do like to add jargon everywhere they can!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now view the gradients by checking the `grad` attribute of our tensor:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "once it does that, we can now look inside xt, which we said it requires grad\n",
    "and find out its gradient\n",
    "and remember, the derivative of x**2 is 2*x\n",
    "in this case 2*3=6 \n",
    "so we didn't have to figure out the derivative, \n",
    "we just called backward and then get the grad attribute to get the derivative\n",
    "so that's how easy it is to do calculus in pytorch\n",
    "\n",
    "so what you need to know about calculus is not how to take a derivative, but what it means\n",
    "and what it means it's a slope at some point\n",
    "\n",
    "'''\n",
    "xt.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you remember your high school calculus rules, the derivative of `x**2` is `2*x`,  \n",
    "and we have `x=3`, so the gradients should be `2*3=6`, which is what PyTorch calculated for us!\n",
    "\n",
    "Now we'll repeat the preceding steps, **but with a vector argument for our function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.,  4., 10.], requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "something interesting, let's not just take 3, let's take a rank1 tensor, also known as a vector\n",
    "'''\n",
    "\n",
    "xt = tensor([3.,4.,10.]).requires_grad_()  # weights are 3,4,10 and are inputs for the loss function f(x)\n",
    "xt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll add **`sum`** to our function so it can **take a vector (i.e., a rank-1 tensor), and return a scalar (i.e., a rank-0 tensor):**  \n",
    "> why it needs to return a scalar? because you need to see if the loss function increases or decreases if you modify the weight/parameter x  \n",
    "> true, because it says \"pretend the quadratic is our loss function\"  \n",
    "> and the derivative of the quadratic shows if the loss function increases or decreases  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(125., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "and let's add sum to our f function\n",
    "f of this vector is 125 \n",
    "'''\n",
    "def f(x): return (x**2).sum()\n",
    "\n",
    "yt = f(xt)\n",
    "yt  # loss is 125=9+16+100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our gradients are `2*xt`, as we'd expect!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.,  8., 20.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "and we say backward and grad\n",
    "and it's 2x,2x,2x\n",
    "so this is vector calculus\n",
    "we're getting the gradient for every element of a vector with the same 2 lines of code\n",
    "so that's all you need to know about calculus\n",
    "\n",
    "so now that we know how to calculate the gradient, that is the slope of the function\n",
    "that tells us if we change our input a little bit, how will our output change correspondingly\n",
    "that's what a slope is\n",
    "\n",
    "and so that tells us that for every one of our parameters, if we know their gradients\n",
    "then we know if we change that parameter up a bit or down a bit, how it will change our loss\n",
    "so therefore, we then know how to change our parameters\n",
    "\n",
    "'''\n",
    "yt.backward()  # yt.calculate_grad() for ytensor which is the loss function; \n",
    "xt.grad        # calculate_gradients() means to derivate the loss, and it shows in which direction is the minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The gradients only tell us the slope of our function, they don't actually tell us exactly how far to adjust the parameters.**  \n",
    "> me: but why the grad is attached to xt? because the gradients show how much each weight needs to change \n",
    "\n",
    "**But it gives us some idea of how far;**  \n",
    "if the slope is **very large, then that may suggest that we have more adjustments to do,**  \n",
    "**hereas if the slope is very small, that may suggest that we are close to the optimal value.**  \n",
    ">me: very large means very far from minimum of the loss functions, where the gradient/slope is 0 (an horizontal line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping With a Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding **how to change our parameters based on the values of the gradients** is an important part of the deep learning process.  \n",
    "Nearly all approaches start with the basic idea of \n",
    "**multiplying the gradient by some small number, called the *learning rate* (LR).**    \n",
    "The learning rate is often a number between **0.001 and 0.1**, although it could be anything.  \n",
    "Often, people select a learning rate just by trying a few, and finding which results in the best model after training  \n",
    "(we'll show you a better approach later in this book, called the *learning rate finder*).  \n",
    "Once you've picked a learning rate, you can **adjust your parameters** using this simple function:\n",
    "\n",
    "```\n",
    "w -= gradient(w) * lr\n",
    "```\n",
    "\n",
    "This is known as ***stepping*** your parameters, using an ***optimizer step***.  \n",
    "Notice how we **_subtract_ the `gradient * lr` from the parameter to update it.**  \n",
    "This allows us to **adjust the parameter in the direction of the slope**  \n",
    "<u>by increasing the parameter when the slope is negative and decreasing the parameter when the slope is positive</u>.  \n",
    "**We want to adjust our parameters in the direction of the slope because our goal in deep learning is to _minimize_ the loss**.\n",
    "\n",
    "If you pick a **learning rate that's too low**, it can mean having to do a lot of steps. <<descent_small>> illustrates that.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlet\\'s say all of our weights are called \"w\",\\nwe just substract off them the gradients multiplied by some small number \\nand that small number is often a number between 0.001 and 0.1\\nand it\\'s called the learning rate\\n\\nw -= gradient(w) * lr\\n\\nand this here is the essence of gradient descent\\nso if you pick a learning rate that is very small, then you take a small step in the direction of the slope\\nand another small step, so on\\nand it\\'s going to take forever to get to the end\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "let's say all of our weights are called \"w\",\n",
    "we just substract off them the gradients multiplied by some small number \n",
    "and that small number is often a number between 0.001 and 0.1\n",
    "and it's called the learning rate\n",
    "\n",
    "w -= gradient(w) * lr\n",
    "\n",
    "and this here is the essence of gradient descent\n",
    "so if you pick a learning rate that is very small, then you take a small step in the direction of the slope\n",
    "and another small step, so on\n",
    "and it's going to take forever to get to the end\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"An illustration of gradient descent with a LR too low\" width=\"400\" caption=\"Gradient descent with low LR\" src=\"images/chapter2_small.svg\" id=\"descent_small\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But picking a **learning rate that's too high** is even worse — it can actually result in the loss getting *worse*, as we see in <<descent_div>>!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif you pick a learning rate that is too big, you jump way too far each time and again, it's goint to take forever\\nme: why like this? because w is on the x/parameter axis and if lr is big then new w is that big\\nit started at the bottom and lr is so bit that w got worse and worse, it moves away from the minimum\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if you pick a learning rate that is too big, you jump way too far each time and again, it's goint to take forever\n",
    "me: why like this? because w is on the x/parameter axis and if lr is big then new w is that big\n",
    "it started at the bottom and lr is so bit that w got worse and worse, it moves away from the minimum\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"An illustration of gradient descent with a LR too high\" width=\"400\" caption=\"Gradient descent with high LR\" src=\"images/chapter2_div.svg\" id=\"descent_div\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the **learning rate is too high**, it may also **\"bounce\" around**, rather than actually diverging;  \n",
    "<<descent_bouncy>> shows how this has the result of taking many steps to train successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthis one is not so big that it gets worse and worse\\nbut it just takes a long time to bounce in and out\\n\\nso picking a good learning rate is really important, both to making sure is even possible to solve the problem\\nand that it's possible to solve it in a reasonable amount of time\\n\\nso we'll be learning how to pick learning rates in this course\\n\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "this one is not so big that it gets worse and worse\n",
    "but it just takes a long time to bounce in and out\n",
    "\n",
    "so picking a good learning rate is really important, both to making sure is even possible to solve the problem\n",
    "and that it's possible to solve it in a reasonable amount of time\n",
    "\n",
    "so we'll be learning how to pick learning rates in this course\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"An illustation of gradient descent with a bouncy LR\" width=\"400\" caption=\"Gradient descent with bouncy LR\" src=\"images/chapter2_bouncy.svg\" id=\"descent_bouncy\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply all of this in an end-to-end example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An End-to-End SGD Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen how to use gradients to find a minimum.  \n",
    "Now it's time to look at **an SGD example and see how finding a minimum can be used to train a model to fit data better.**  \n",
    "(it's not S, just GD)\n",
    "\n",
    "Let's start with a simple, synthetic, example model.  \n",
    "**Imagine you were measuring the speed of a roller coaster as it went over the top of a hump.**  (me:hump means top)  \n",
    "**It would start fast, and then get slower as it went up the hill;**  \n",
    "**it would be slowest at the top, and it would then speed up again as it went downhill.**  \n",
    "**<u>You want to build a model of how the speed changes over time</u>.**  \n",
    "If you were measuring the speed manually **every second for 20 seconds,**  \n",
    "it might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nlet's imagine you were watching a roller coaster go over the top of a hump\\nso as it comes out of the previous hill, it's going super fast, \\nand it's going up the hill and it's going slower and slower until it gets to the top of the hump\\nand then it goes to the other side, it gets faster and faster\\n\\nand of you had a speedometer and you were measuring it just by hand at kind of equal times points\\nyou might end up with something that looks a bit like this:\\nI end up with a quadratic function which is a bit bumpy\\n\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "let's imagine you were watching a roller coaster go over the top of a hump\n",
    "so as it comes out of the previous hill, it's going super fast, \n",
    "and it's going up the hill and it's going slower and slower until it gets to the top of the hump\n",
    "and then it goes to the other side, it gets faster and faster\n",
    "\n",
    "and of you had a speedometer and you were measuring it just by hand at kind of equal times points\n",
    "you might end up with something that looks a bit like this:\n",
    "I end up with a quadratic function which is a bit bumpy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = torch.arange(0,20).float();  # a tensor with 20 time intervals from 0 to 19 when I take speed measurements\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1  # speed are some values for speed that were supposedly measured every 20s\n",
    "#speed\n",
    "plt.scatter(time,speed);                            # they are recorded data, target data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've **added a bit of random noise**, since measuring things manually isn't precise.  \n",
    "This means it's not that easy to answer the question: **what was the roller coaster's speed?**  \n",
    "**Using SGD we can try to find a function that matches our observations**.  \n",
    "We can't consider every possible function, so let's use a **guess that it will be quadratic**;  \n",
    "i.e., a function of the form **`a*(time**2)+(b*time)+c`**.    \n",
    "\n",
    "We want to distinguish clearly between  \n",
    "the **function's input** (<u>the time</u> when we are measuring the coaster's speed)  \n",
    "and **its parameters** (<u>the values that define *which* quadratic we're trying</u>).  \n",
    "So, let's **collect the parameters in one argument and thus separate the input, `t`, and the parameters, `params`**, in the function's signature: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we want to create a function that estimates at any time what is the speed of the rolercoaster\n",
    "so we start by guessing what function it might be\n",
    "so we guess that it's a quadratic function \n",
    "\n",
    "so let's create it using the albert samuel technique, machine learning technique\n",
    "it's going to take an input, which is a time\n",
    "and it's going to take some parameters, a,b,c \n",
    "so in python you can split out a list or a collection into its components (a,b,c=params)\n",
    "so we're not trying to find any function in the world, but a quadratic by finding an a,b and c\n",
    "so arthur samuel technique for doing this is to next come up with a loss function\n",
    "come up with a measurement of how good we are\n",
    "'''\n",
    "\n",
    "def f(t, params):                     # f is a function that aims to match the points above; we guess it's quadratic\n",
    "    a,b,c = params                    # input is time and parameters are a,b,c\n",
    "    return a*(t**2) + (b*t) + c       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, we've restricted the problem of finding the best imaginable function that fits the data, \n",
    "to **finding the best *quadratic* function**.  \n",
    "This greatly simplifies the problem, since every quadratic function is fully defined by the three parameters `a`, `b`, and `c`.  \n",
    "Thus, **to find the best quadratic function, we only need to find the best values for `a`, `b`, and `c`**.  \n",
    "\n",
    "If we can solve this problem for the three parameters of a quadratic function,  \n",
    "we'll be able to **apply the same approach for other, more complex functions with more parameters — such as a neural net**.  \n",
    "Let's find the parameters for `f` first, and then we'll come back and do the same thing for the MNIST dataset with a neural net.  \n",
    "\n",
    "**We need to define first what we mean by \"best\"**.  \n",
    "We define this precisely by **choosing a *loss function***,   \n",
    "**which will return a value based on a prediction and a target,**  \n",
    "where **lower values** of the function correspond to **\"better\" predictions**.   \n",
    "It is important for **loss functions to return _lower_ values when predictions are more accurate**,  \n",
    "as the SGD procedure we defined earlier will try to _minimize_ this loss.  \n",
    "**For continuous data, it's common to use *mean squared error*:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so if we've got some predictions that come out of our function and the targets which are these actual values (blue dots from the plot)\n",
    "then we could just do the mse\n",
    "now we need to go through our 7 step process\n",
    "\n",
    "\n",
    "'''\n",
    "def mse(preds, targets): return ((preds-targets)**2).mean()  # the loss function is MSE between predictions and targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's work through our 7 step process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Initialize the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, **we initialize the parameters to random values**,  \n",
    "and tell PyTorch that we want to **track their gradients**, using `requires_grad_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we want to come up with a set of parameters a,b and c to random values\n",
    "we're going to be adjusting them, so tell pytorch that we want the gradients\n",
    "'''\n",
    "params = torch.randn(3).requires_grad_()  # params are the weights for the quadratic function we want to find (a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I'm going to save these away to check them later\n",
    "'''\n",
    "#hide\n",
    "orig_params = params.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Calculate the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **calculate the predictions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "calculate the prediction using function f\n",
    "'''\n",
    "preds = f(time, params)  # f is quadratic with random parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create **a little function to see how close our predictions are to our targets**, and take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "and now create a function that plots how good are our predictions\n",
    "here's a function that prints in red our predictions and in blue our targets\n",
    "that looks pretty terible\n",
    "'''\n",
    "def show_preds(preds, ax=None):\n",
    "    if ax is None: ax=plt.subplots()[1]\n",
    "    ax.scatter(time, speed)                      # target data\n",
    "    ax.scatter(time, to_np(preds), color='red')  # prediction data\n",
    "    ax.set_ylim(-300,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_preds(preds)  # show predictions (red) with 1st choice of random parameters (params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This doesn't look very close** —  \n",
    "our random parameters suggest that the roller coaster will end up **going backwards**,  \n",
    "since we have **negative speeds!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Calculate the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **calculate the loss** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4990.9258, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so let's calculate the loss, using the mse function we wrote\n",
    "so now we want to improve this\n",
    "'''\n",
    "loss = mse(preds, speed) # preds are predictions with random parameters and speed are recorded data\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is now to **improve** this.  \n",
    "To do that, we'll need to **know the gradients.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Calculate the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to **calculate the gradients**.  \n",
    "In other words, **calculate an approximation of how the parameters need to change**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-22116.9102,  -1406.4412,   -112.9858])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "calculate the gradients using the 2 steps\n",
    "each of our parameter has a gradient that's negative\n",
    "'''\n",
    "loss.backward()  # backward() means calculate_gradients(); # calculate gradients by derivating the loss function; find each gradient by derivating each ax**2 and keeping the rest as a constant\n",
    "params.grad      # gradients are \"attached\" to params because it shows in which direction to change the params a,b,c  \n",
    "                 # -73k,-5k,-0,3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2212, -0.0141, -0.0011])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "let's pick a learning rate of 10 to the -5  \n",
    "'''\n",
    "params.grad * 1e-5  # here shows what the step will be equal to = gradients * learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can use these gradients to improve our parameters.**  \n",
    "We'll need to **pick a learning rate** (we'll discuss how to do that in practice in the next chapter; for now we'll just use 1e-5, or **0.00001**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3087,  0.8158,  0.1809], requires_grad=True)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params  # these are the parameters before being adjusted with the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Step the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to **update the parameters based on the gradients we just calculated:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "and step the weights\n",
    "step is -= learning rate * gradient\n",
    "there's a wonderful trick here, which I've called .data\n",
    "there reason I've called .data is because it's a special attribute in pytorch, which if you use it, then the gradient is not calculated\n",
    "and we don't want the gradient to be calculated of the actual step we're doing - me: here, in lr*grad we don't want to call .grad, so that's why add .data\n",
    "we only want the gradient to be calculated of our function f\n",
    "all right, so when we step the weights we have to use this special .data attribute (params.data in left side of the =)\n",
    "after we do that delete the gradients what we already had and let's see if loss improved\n",
    "\n",
    "'''\n",
    "# w -=gradient(w)*lr\n",
    "# adjusts the parameter in the direction of the slope (gradient)\n",
    "# increasing the parameter when the slope (gradient) is negative and decreasing the parameter when the slope is positive\n",
    "lr = 1e-5\n",
    "params.data -= lr * params.grad.data  # how to change our parameters based on the values of the gradients\t                            \n",
    "params.grad = None                    # multiplying the gradient by some small number, called the learning rate (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> a: Understanding this bit depends on remembering recent history.   \n",
    "To **calculate the gradients** we call `backward` on the **`loss`**.  \n",
    "But this `loss` was itself calculated by **`mse`**,  \n",
    "which in turn took **`preds`** as an input,  \n",
    "which was calculated using **`f`** taking as an input **`params`**,  \n",
    "which was the object on which we originally called **`requires_grad_`**  \n",
    "— which is the original call that now allows us to call `backward` on `loss`.  \n",
    "This **chain of function calls** represents the mathematical composition of functions,  \n",
    "which **enables PyTorch** to use calculus's chain rule under the hood to **calculate these gradients.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if **the loss has improved:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1466.9171, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "the loss before was 25823 and now is 5400\n",
    "and the plot has gone from something that goes down to -300 to something that looks much better\n",
    "'''\n",
    "preds = f(time,params)  # new predictions based on quadratic f with new parameters\n",
    "mse(preds, speed)       # new loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And take a look at **the plot:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_preds(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to repeat this a few times, so we'll create **a function to apply one step:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_step(params, prn=True):\n",
    "    preds = f(time, params)\n",
    "    loss = mse(preds, speed)\n",
    "    loss.backward()\n",
    "    params.data -= lr * params.grad.data\n",
    "    params.grad = None\n",
    "    if prn: print(loss.item())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Repeat the process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we **iterate**.  \n",
    "By looping and performing many improvements, we hope to reach a good result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1466.9171142578125\n",
      "800.063232421875\n",
      "673.8706665039062\n",
      "649.9879150390625\n",
      "645.465087890625\n",
      "644.6058959960938\n",
      "644.4398193359375\n",
      "644.4051513671875\n",
      "644.3951416015625\n",
      "644.3899536132812\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "put those steps in a function and repeat it 10 times\n",
    "and the loss is getting better and better\n",
    "'''\n",
    "for i in range(10): apply_step(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "params = orig_params.detach().requires_grad_() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The loss is going down**, just as we hoped!  \n",
    "But looking only at these loss numbers disguises the fact that each **iteration represents an entirely different quadratic function being tried**,  \n",
    "on the way to finding the best possible quadratic function.  \n",
    "We can see this process visually if, instead of printing out the loss function, **we plot the function at every step.**  \n",
    "Then we can see how **the shape is approaching the best possible quadratic function for our data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "and we can actually look at it getting better and better\n",
    "so we have a technique, this is the arthur samuel technique\n",
    "for finding a set of parameters that continuously improves \n",
    "by getting feedback from the result of measuring some loss fuction\n",
    "so that was kind of the key step  \n",
    "this is the gradient descent method\n",
    "'''\n",
    "_,axs = plt.subplots(1,4,figsize=(12,3))     \n",
    "for ax in axs: show_preds(apply_step(params, False), ax)   \n",
    "plt.tight_layout()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just decided to stop after 10 epochs arbitrarily.  \n",
    "In practice, **we would watch the training and validation losses and our metrics to decide when to stop**, as we've discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.1.0 (20230122.1345)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"661pt\" height=\"78pt\"\n",
       " viewBox=\"0.00 0.00 660.87 78.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 74)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-74 656.87,-74 656.87,4 -4,4\"/>\n",
       "<!-- init -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>init</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">init</text>\n",
       "</g>\n",
       "<!-- predict -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>predict</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"135.2\" cy=\"-18\" rx=\"44.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"135.2\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">predict</text>\n",
       "</g>\n",
       "<!-- init&#45;&gt;predict -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>init&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54.25,-18C61.85,-18 70.45,-18 79.12,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.96,-21.5 88.96,-18 78.96,-14.5 78.96,-21.5\"/>\n",
       "</g>\n",
       "<!-- loss -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>loss</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"244.99\" cy=\"-52\" rx=\"28.7\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"244.99\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">loss</text>\n",
       "</g>\n",
       "<!-- predict&#45;&gt;loss -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>predict&#45;&gt;loss</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.19,-29.04C182.93,-32.74 196.04,-36.88 207.87,-40.61\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.69,-43.9 217.28,-43.58 208.8,-37.23 206.69,-43.9\"/>\n",
       "</g>\n",
       "<!-- gradient -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>gradient</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"406.63\" cy=\"-52\" rx=\"50.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"406.63\" y=\"-48.3\" font-family=\"Times,serif\" font-size=\"14.00\">gradient</text>\n",
       "</g>\n",
       "<!-- loss&#45;&gt;gradient -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>loss&#45;&gt;gradient</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M274.16,-52C293.71,-52 320.51,-52 344.81,-52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"344.61,-55.5 354.61,-52 344.61,-48.5 344.61,-55.5\"/>\n",
       "</g>\n",
       "<!-- step -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>step</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"524.23\" cy=\"-18\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"524.23\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">step</text>\n",
       "</g>\n",
       "<!-- gradient&#45;&gt;step -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>gradient&#45;&gt;step</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M446.12,-40.68C458.69,-36.98 472.64,-32.88 485.18,-29.19\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"486.16,-32.55 494.77,-26.37 484.18,-25.84 486.16,-32.55\"/>\n",
       "</g>\n",
       "<!-- step&#45;&gt;predict -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>step&#45;&gt;predict</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M493.24,-18C428.38,-18 274.25,-18 191.3,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.35,-14.5 181.35,-18 191.35,-21.5 191.35,-14.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"315.09\" y=\"-21.8\" font-family=\"Times,serif\" font-size=\"14.00\">repeat</text>\n",
       "</g>\n",
       "<!-- stop -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>stop</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"622.32\" cy=\"-18\" rx=\"30.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"622.32\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">stop</text>\n",
       "</g>\n",
       "<!-- step&#45;&gt;stop -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>step&#45;&gt;stop</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M555.1,-18C563.06,-18 571.79,-18 580.23,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"579.95,-21.5 589.95,-18 579.95,-14.5 579.95,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x7eff74319290>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide_input\n",
    "#id gradient_descent\n",
    "#caption The gradient descent process\n",
    "#alt Graph showing the steps for Gradient Descent\n",
    "gv('''\n",
    "init->predict->loss->gradient->step->stop\n",
    "step->predict[label=repeat]\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize,  \n",
    "at the beginning, **the weights of our model can be random (training *from scratch*)**    \n",
    "**or come from a pretrained model (*transfer learning*).**    \n",
    "In the first case, the output we will get from our inputs won't have anything to do with what we want,  \n",
    "and even in the second case, it's very likely the pretrained model won't be very good at the specific task we are targeting.  \n",
    "**So the model will need to *learn* better weights.**  \n",
    "\n",
    "**We begin by comparing the outputs the model gives us with our targets**  \n",
    "(we have labeled data, so we know what result the model should give) **using a *loss function*,**  \n",
    "**which returns a number that we want to make as low as possible by improving our weights.**  \n",
    "To do this, **we take a few data items (such as images) from the training set and feed them to our model.  \n",
    "We compare the corresponding targets using our loss function,  \n",
    "and the score we get tells us how wrong our predictions were.**    \n",
    "We then **change the weights a little bit** to make it slightly better.   \n",
    "\n",
    "To find **how to change the weights to make the loss a bit better, we use calculus to calculate the *gradients*.**    \n",
    "(Actually, we let PyTorch do it for us!)   \n",
    "Let's consider an **analogy**.  \n",
    "Imagine you are lost in the mountains with your **car parked at the lowest point.**  \n",
    "To find your way back to it, you might wander in a random direction, but that probably wouldn't help much.  \n",
    "Since you know your vehicle is at the lowest point, you would be better off going downhill.  \n",
    "**By always taking a step in the direction of the steepest downward slope, you should eventually arrive at your destination.**  \n",
    "**We use the magnitude of the gradient (i.e., the steepness of the slope) to tell us how big a step to take;**  \n",
    "specifically, we multiply the gradient by a number we choose called the ***learning rate*** to decide on the step size.  \n",
    "We then ***iterate*** until we have reached the lowest point, which will be our parking lot,  \n",
    "then we can ***stop***.\n",
    "\n",
    "All of that we just saw can be transposed directly to the MNIST dataset, except for the loss function.  \n",
    "Let's now see how we can **define a good training objective.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have our **independent variables `x`—these are the images themselves.**  \n",
    "We'll **concatenate them all into a single tensor,**  \n",
    "and also change them from **a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor).**  \n",
    "We can do this using **`view`, which is a PyTorch method that changes the shape of a tensor without changing its contents.**  \n",
    "`-1` is a special parameter to `view` that means **\"make this axis as big as necessary to fit all the data\":**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nso now let's apply this to MNIST\\nso for MNIST we want to use this exact technique and there's nothing extra that we have to do\\nexcept on thing, we need a LOSS function\\nand the metric that we've been using is the error rate, or the accuracy \\n    (the accuracy = how many of the predictions are true in the validation set\\n    using the mnist_distance that calculates the overall distance between an the validation set and an ideal 3/7)\\naccuracy = it's like how often are we correct \\nand that's the thing we're actually trying to make good, our metric\\nbut we've got a very serious problem, \\nwhich is we need to calculate the gradient to figure out how we should change our parameters\\nand the gradient is the slope, or the steepness, which is rise / run\\nit's (y_new-y_old) / (x_new-x_old)  -->> on the loss graph\\nthe gradient is actually defined when x_new is very, very close to x_old, \\nmeaning their difference is very small (because the derivative it's the tangent)\\nbut think about accuracy: if I change a parameter by a tiny tiny amount, the accuracy might not change at all\\n(it's an horizontal line and it's slope in that x_new point is 0)\\nbecause there might not be any 3 that we now predict as a 7 or any 7 that we now predict as a 3\\nbecause we change the parameter by such a small amount\\nso it's certain that the gradient is 0 in many places, \\nand that means our parameters aren't going to change at all\\nbecause learning rate * gradient is still 0 when the gradient is 0 for any learning rate\\nso this is why the loss function and the metric are not always the same thing\\nwe can't use a metric as our loss if that metric has a gradient of zero\\nso we need something different\\nwe need to find something that is pretty similar to the accuracy \\nin that like as the accuracy gets better this ideal function we want gets better as well\\nbut it should not have a gradient of 0\\n\\nso let's think about that function\\nsuppose we had 3 images\\nactually we will stop\\nwe understand GD\\nwe know how to do it with a simple loss function \\n\\n\\n\\nlesson 4 2020\\ndeep into the weeds of what exactly is going on when we are training a neural network\\nand we started looking at this in the previous lesson, at SGD\\n\\nwe were looking at what Arthur Samuel said:\\nautomatic means of testing the effectivenes of any current weight/paramenter assignment in terms of actual performance\\nand provide a mechanism for altering the weight assignment so as to maximize that performance\\nso we could make that entirely automatic \\nand a machine so programmed would learn from its experience\\n\\nour initial attempt on MNIST data set was not based on that\\nwe didn't have any parameters \\n\\nlesson 4 2020\\nnow let's use this to create our MNIST 3 and 7 model\\nand so to create a model we're going to need to create something that we can pass into a function like\\nthis one: def pr_eight(x,w) = (x*w).sum()\\nso we need some pixels that are all lined up and some parameters that are all lined up\\nand then we're going to sum them up\\nso our axis/x's are going to be pixels and so in this case,\\nbecause we're just going to multiply each pixel by a parameter and add them up,\\nthe effect that the're laid out in a grid is not important so let's reshape those grids\\nand turn them into vectors\\nthe way we reshape things in pytorch is by using the view method\\nthe view method you can pass to it how large you want each dimension to be\\nin this case, we want the number of columns to be equal to the total number of pixels in each picture, which is 28*28=784\\nand then the number of rows will be however many rows there are in the data\\n(me: so each picture now occupies a row of 784 columns or 784 numbers)\\n(in total are 12396 3s and 7s so there are that many rows)\\nso if you use -1 when you call view, that means as many there are in the data\\nso this will create something of the same, with the same total number of elements that we had before\\nso we can grab all our 3, we can concatenate them with torch cat with all of our 7s\\nand then reshape that into a matrix, where each row is one image\\nwith all of the rows and columns of an image all lined up in a single vector\\nso that's our x.\\n\""
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so now let's apply this to MNIST\n",
    "so for MNIST we want to use this exact technique and there's nothing extra that we have to do\n",
    "except on thing, we need a LOSS function\n",
    "and the metric that we've been using is the error rate, or the accuracy \n",
    "    (the accuracy = how many of the predictions are true in the validation set\n",
    "    using the mnist_distance that calculates the overall distance between an the validation set and an ideal 3/7)\n",
    "accuracy = it's like how often are we correct \n",
    "and that's the thing we're actually trying to make good, our metric\n",
    "but we've got a very serious problem, \n",
    "which is we need to calculate the gradient to figure out how we should change our parameters\n",
    "and the gradient is the slope, or the steepness, which is rise / run\n",
    "it's (y_new-y_old) / (x_new-x_old)  -->> on the loss graph\n",
    "the gradient is actually defined when x_new is very, very close to x_old, \n",
    "meaning their difference is very small (because the derivative it's the tangent)\n",
    "but think about accuracy: if I change a parameter by a tiny tiny amount, the accuracy might not change at all\n",
    "(it's an horizontal line and it's slope in that x_new point is 0)\n",
    "because there might not be any 3 that we now predict as a 7 or any 7 that we now predict as a 3\n",
    "because we change the parameter by such a small amount\n",
    "so it's certain that the gradient is 0 in many places, \n",
    "and that means our parameters aren't going to change at all\n",
    "because learning rate * gradient is still 0 when the gradient is 0 for any learning rate\n",
    "so this is why the loss function and the metric are not always the same thing\n",
    "we can't use a metric as our loss if that metric has a gradient of zero\n",
    "so we need something different\n",
    "we need to find something that is pretty similar to the accuracy \n",
    "in that like as the accuracy gets better this ideal function we want gets better as well\n",
    "but it should not have a gradient of 0\n",
    "\n",
    "so let's think about that function\n",
    "suppose we had 3 images\n",
    "actually we will stop\n",
    "we understand GD\n",
    "we know how to do it with a simple loss function \n",
    "\n",
    "\n",
    "\n",
    "lesson 4 2020\n",
    "deep into the weeds of what exactly is going on when we are training a neural network\n",
    "and we started looking at this in the previous lesson, at SGD\n",
    "\n",
    "we were looking at what Arthur Samuel said:\n",
    "automatic means of testing the effectivenes of any current weight/paramenter assignment in terms of actual performance\n",
    "and provide a mechanism for altering the weight assignment so as to maximize that performance\n",
    "so we could make that entirely automatic \n",
    "and a machine so programmed would learn from its experience\n",
    "\n",
    "our initial attempt on MNIST data set was not based on that\n",
    "we didn't have any parameters \n",
    "\n",
    "lesson 4 2020\n",
    "now let's use this to create our MNIST 3 and 7 model\n",
    "and so to create a model we're going to need to create something that we can pass into a function like\n",
    "this one: def pr_eight(x,w) = (x*w).sum()\n",
    "so we need some pixels that are all lined up and some parameters that are all lined up\n",
    "and then we're going to sum them up\n",
    "so our axis/x's are going to be pixels and so in this case,\n",
    "because we're just going to multiply each pixel by a parameter and add them up,\n",
    "the effect that the're laid out in a grid is not important so let's reshape those grids\n",
    "and turn them into vectors\n",
    "the way we reshape things in pytorch is by using the view method\n",
    "the view method you can pass to it how large you want each dimension to be\n",
    "in this case, we want the number of columns to be equal to the total number of pixels in each picture, which is 28*28=784\n",
    "and then the number of rows will be however many rows there are in the data\n",
    "(me: so each picture now occupies a row of 784 columns or 784 numbers)\n",
    "(in total are 12396 3s and 7s so there are that many rows)\n",
    "so if you use -1 when you call view, that means as many there are in the data\n",
    "so this will create something of the same, with the same total number of elements that we had before\n",
    "so we can grab all our 3, we can concatenate them with torch cat with all of our 7s\n",
    "and then reshape that into a matrix, where each row is one image\n",
    "with all of the rows and columns of an image all lined up in a single vector\n",
    "so that's our x.\n",
    "'''\n",
    "\n",
    "#train_test = torch.cat([stacked_threes, stacked_sevens])\n",
    "#train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)  \n",
    "# cat puts 3 above 7 because default axis is 0, the vertical axis\n",
    "# view(-1,28*28) means fix one axis to 28*28 and one picture is now 784 columns/numbers\n",
    "# and the -1 means to adapt so it can fit all images, the sum of 3s + 7s\n",
    "# so this is a matrix now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a **label for each image.**  \n",
    "We'll use `1` for 3s and `0` for 7s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test only\n",
    "#train_test = tensor([1]*len(threes) + [0]*len(sevens))\n",
    "#train_test, train_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12396, 784]), torch.Size([12396, 1]))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "then we're going to need labels, 1 for each of the 3s and a 0 for each of the 7s\n",
    "so basically we're going to create an is_3 model (what??)\n",
    "so that's going to create a vector, we actually need it to be a matrix in pytorch\n",
    "so unsqueeze will and an additional unit dimension to whatever I've asked for\n",
    "so here in position 1, \n",
    "so in other words, this is going to turn up from which is a vector of 12396 long \n",
    "into a matrix with 12396 rows and 1 column\n",
    "'''\n",
    "# create a tensor for labels\n",
    "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)  # puts them vertical\n",
    "train_x.shape,train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y  # added by me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A `Dataset` in PyTorch is required to return a tuple of `(x,y)` when indexed.**  \n",
    "Python provides **a `zip` function which, when combined with `list`**, provides a simple way to get this functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1]))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "now we're going to turn or x and y into a dataset\n",
    "and a Dataset is a very specific concept in pytorch\n",
    "it's something we can index into, using []\n",
    "and when we do so, it's expected to return a tuple \n",
    "containing our independent variable (x) and a dependent variable (y)\n",
    "for each particular row (0 in this case)\n",
    "\n",
    "and so to do that we can use the python zip function, \n",
    "which takes one element of the train_x and combines/concatenates it with one element of train_y\n",
    "and it does that again and again and again\n",
    "and so then if we create a list of those it gives us a Dataset\n",
    "\n",
    "it gives us a list which when we index into it, it's going to contain 1 image and 1 label\n",
    "and so here you can see \"y\" there's my label\n",
    "and \"x\" my image, but I won't print the whole thing, it's a 784 long vector\n",
    "\n",
    "so that's a really important concept: Dataset, something you can index into, and get back a tuple\n",
    "this is called destructuring a tuple: which means I'm taking the 2 parts of the tuple\n",
    "and putthing the 1st part in one variable (x) and the 2nd part in the other variable\n",
    "which is something we do a lot in python, it's pretty handy\n",
    "'''\n",
    "\n",
    "dset = list(zip(train_x,train_y))  # so a pytorch dataset needs to return a tuple (a fixed list)\n",
    "x,y = dset[0]\n",
    "x.shape,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "repeat the same 3 steps for the validation set\n",
    "so we've now got a training Dataset and a validation Dataset\n",
    "'''\n",
    "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)             # create the valid dataset\n",
    "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
    "valid_dset = list(zip(valid_x,valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need **an (initially random) weight for every pixel**  \n",
    "(this is the ***initialize* step** in our seven-step process):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so now we need to initialize our parameters\n",
    "and we just do it randomly\n",
    "here's a function that given some size (shape if you like)\n",
    "we'll randomly initialize, using a normal random number distribution in pytorch\n",
    "(hit shift+tab with cursor inside the size of rand(size) to see more details)\n",
    "and it says in the details that's going to have a variance of 1\n",
    "so I probably should not call this standard deviation (std=1), but variance\n",
    "\n",
    "so multiply it by the variance to change it's variance to whatever is requested (in arguments at std=)\n",
    "which will default to 1\n",
    "\n",
    "and as we talked about when it comes to calculating our gradients\n",
    "we have to tell pytorch which things we want gradients for\n",
    "and the way we do that is requires_grad_\n",
    "the underscore _ at the end is a special magic symbol \n",
    "which tells pytorch that we want this function to actually change the thing that is referring to\n",
    "this will change this tensor, such that it requires gradience\n",
    "\n",
    "\n",
    "'''\n",
    "def init_params(size, std=1.0): \n",
    "    return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "here's some weights, so our weights are going to need to be 28*28=784 by 1 shape\n",
    "28*28 because every pixel is going to need a weight\n",
    "and then 1 because we're going to need to have that unit access to make it into a column\n",
    "(me: to be vertical 784 weights, a single column)\n",
    "so that's what pytorch expects \n",
    "so there's our weights\n",
    "'''\n",
    "\n",
    "weights = init_params((28*28,1))   # x are the pictures, which are 28*28 so the weights w should be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The function `weights*pixels` won't be flexible enough**  \n",
    "— it is always equal to 0 when the pixels are equal to 0 (i.e., its *intercept* is 0).  \n",
    "You might remember from high school math that **the formula for a line is `y=w*x+b`; we still need the `b`.**  \n",
    "**We'll initialize it to a random number too:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "now just weights by pixels actually isn't going to be enough\n",
    "because weights by pixels were always equal 0 when the pixels are equal to 0,\n",
    "it has a 0 intercept\n",
    "so we really want something where it's like w*x+b, a line\n",
    "so the b we call the bias\n",
    "so let's grab a single number for our bias =|> (1)\n",
    "\n",
    "so I told you, there's a difference between parameters and weights\n",
    "\n",
    "the weights from weights = init_params((28*28,1)) are the w in y=wx+b equation\n",
    "and the bias is b in this equation\n",
    "and the weights and bias together are the PARAMETERS  of the function\n",
    "they're all the things that we're going to change, they're all the things that have gradients that we're going to update\n",
    "'''\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, the `w` in the equation `y=w*x+b` is called the *weights*, and the `b` is called the *bias*.  \n",
    "**Together, the weights and bias make up the *parameters*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> jargon: Parameters: The _weights_ and _biases_ of a model.  \n",
    "> The weights are the `w` in the equation `w*x+b`, and the biases are the `b` in that equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now calculate a prediction for one image:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4917], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "we can now create and calculate predictions for one image \n",
    "so we can take an image such as the 1st one\n",
    "and multiply by the weights only to transpose them \n",
    "to make them line up in terms of the rows and columns \n",
    "and add it up, \n",
    "and add the bias \n",
    "and there is a prediction\n",
    "\n",
    "we want to do that for every image \n",
    "we could do that with a for loop and that would be really, really slow\n",
    "it wouldn't run on the gpu and it wouldn't run in optimized c code\n",
    "so we actually want to use always to do kind of like looping over pixels, looping over images\n",
    "you always want to make sure you're doing that without a python loop\n",
    "in this case doing this calculation for lots of rows and columns is the matrix multiplication operation\n",
    "'''\n",
    "(train_x[0]*weights.T).sum() + bias  # why? because a nn it's a linear function; and pr_eight is also multiplication + sum; here it adds a bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could use a Python `for` loop to calculate the prediction for each image, that would be very slow.  \n",
    "Because Python loops don't run on the GPU, and because Python is a slow language for loops in general,  \n",
    "**we need to represent as much of the computation in a model as possible using higher-level functions.**\n",
    "\n",
    "In this case, there's an extremely convenient mathematical operation that calculates `w*x` for every row of a matrix—it's called ***matrix multiplication***.  \n",
    "<<matmul>> shows what matrix multiplication looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Matrix multiplication\" width=\"400\" caption=\"Matrix multiplication\" src=\"images/matmul2.svg\" id=\"matmul\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows two matrices, `A` and `B`, being multiplied together.  \n",
    "Each item of the result, which we'll call `AB`, contains each item of its corresponding row of `A` multiplied by each item of its corresponding column of `B`, added together.  \n",
    "For instance, **row 1, column 2 (the yellow dot with a red border) is calculated as $a_{1,1} * b_{1,2} + a_{1,2} * b_{2,2}$.**  \n",
    "If you need a refresher on matrix multiplication, we suggest you take a look at the [Intro to Matrix Multiplication](https://youtu.be/kT4Mp9EdVqs) on *Khan Academy*,  \n",
    "since this is the most important mathematical operation in deep learning.  \n",
    "\n",
    "**In Python, matrix multiplication is represented with the `@` operator.**    \n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4917],\n",
       "        [ 8.0730],\n",
       "        [13.7015],\n",
       "        ...,\n",
       "        [-4.6291],\n",
       "        [13.3116],\n",
       "        [-3.6772]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1st prediction is 20.2336 as above with train_x[0]\n",
    "so matrix multiplication gives us an optimized way to do these simple linear function\n",
    "wehreas we want as many rows and columns as we want\n",
    "so this is one of the 2 fundamental equations of any neural network\n",
    "some rows and columns of data multiply some weights add some bias \n",
    "and the 2nd one is an activation function\n",
    "\n",
    "so these are some predictions from our randomly initialized model\n",
    "'''\n",
    "def linear1(xb): return xb@weights + bias\n",
    "preds = linear1(train_x)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element is the same as we calculated before, as we'd expect.  \n",
    "**This equation, `batch@weights + bias`, is one of the two fundamental equations of any neural network**  \n",
    "(the other one is the ***activation function***, which we'll see in a moment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check our **accuracy**.  \n",
    "**To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0.0,**  \n",
    "so our accuracy for each item can be calculated (using broadcasting, so no loops!) with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True],\n",
       "        [ True],\n",
       "        [ True],\n",
       "        ...,\n",
       "        [ True],\n",
       "        [False],\n",
       "        [ True]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so we can check how good our model is\n",
    "and so to do that we can decide that anything greater than 0 we will call a 3\n",
    "and anything less than 0 we will call a 7\n",
    "so preds greater than 0 tells us whether or not something is predicted to be a 3 or not\n",
    "then turn that into a float, so rather than true and false, make it 1 and 0, \n",
    "because it's what our training set contains (train_y)\n",
    "\n",
    "and check with our thresholded predictions are equal to our training set \n",
    "and this will return true every time a row is correctly predicted and false otherwise\n",
    "'''\n",
    "corrects = (preds>0.0).float() == train_y  # compare the predictions with train_y\n",
    "                                           # train_y is 1(3s) and 0(7s)\n",
    "                                           # preds>0 means 1; if it's equal with 1 in train_y then it's a 3\n",
    "corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5893836617469788"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so if we take all these trues and falses and turn them into floasts, that'll be 1s and 0s\n",
    "and then take their mean it's 0.49\n",
    "so not surprisingly our randomly initialized model is right about half the time at predicting 3s from 7s\n",
    "I added one more method here, which is .item() \n",
    "without .item() this will return a tensor, it's a rank 0 tensor, it has no rows, no columns\n",
    "it just it's number on its own, but I actually wanted to unwrap it to create a normal python scalar\n",
    "mainly just because I wanted to easily see the decimal places \n",
    "and the reason for that is I want to show you how we're going to calculate the derivative on the accuracy\n",
    "by changing a parameter a tiny bit\n",
    "'''\n",
    "corrects.float().mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what **the change in accuracy is for a small change in one of the weights**    \n",
    "(note that we have to ask PyTorch not to calculate gradients as we do this, which is what `with torch.no_grad()` is doing here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so let's take one parameter, which will be w[0] and multiply it with 1.0001\n",
    "and so that's going to make it a little bit bigger\n",
    "and then if I calculate how the accuracy changes based on the change in that weight\n",
    "that will be the gradient of the accuracy with respect to that parameter\n",
    "'''\n",
    "\n",
    "with torch.no_grad(): weights[0] *= 1.0001  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5893836617469788"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so I can do that by calculating my new set of predictions and then I can threshold them\n",
    "and then I can check whether they're equal to the training set and then take the mean\n",
    "and I get back exactly the same number \n",
    "so remember that gradient is equal to rise/run \n",
    "\n",
    "the change in y, so y_new-y_old (that's the accuracy, the y function = wx+b)\n",
    "which is which is 0.4912 - 0.4912, which is 0\n",
    "divided by this change in x (1.0001) \n",
    "will give us 0\n",
    "so at this point we have a problem: our derivative is 0 so we have 0 gradients:\n",
    "which means our step will be 0\n",
    "which means our prediction will be unchanged\n",
    "\n",
    "ok, so we have problem\n",
    "our problem is that our gradient is zero and with a gradient of 0\n",
    "we can't take a step and we can't get better predictions\n",
    "and so intuitively speaking, the reason that our gradient is 0\n",
    "is because when we change a single pixel by a tiny bit\n",
    "we might not ever in any way change an actual prediction to change from predicting a 3 to a 7\n",
    "or viceversa\n",
    "because we have this threshold (preds>0.0)\n",
    "\n",
    "so in other words, our accuracy loss function here is very bumpy\n",
    "it's like a flat step, flat step, flat step\n",
    "so it's got this 9 gradient all over the place\n",
    "so what we need to do it use something other than accuracy as our loss function\n",
    "\n",
    "so let's try and create a new function \n",
    "and what this new function is going to do, it's going to give us a better value\n",
    "kind of in much the same way that accuracy gives a better value\n",
    "so this is the loss, and remember, the small loss is better\n",
    "so to give us a lower loss, when the accuracy is better \n",
    "but it won't have a zero gradient\n",
    "it means that a slightly better prediction needs to have a slightly better loss (lower loss is better)\n",
    "'''\n",
    "\n",
    "preds = linear1(train_x)\n",
    "((preds>0.0).float() == train_y).float().mean().item()  # same value, which means the y is constant \n",
    "                                                        # so the gradient/derivative is 0 = useless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, **we need gradients in order to improve our model using SGD,**  \n",
    "and in order to calculate gradients we need some ***loss function* that represents how good our model is.**  \n",
    "That is because the gradients are a measure of how that loss function changes with small tweaks to the weights.  \n",
    "\n",
    "**So, we need to choose a loss function.**  \n",
    "The obvious approach would be to use **accuracy**, which is our metric, as our loss function as well.  \n",
    "In this case, we would calculate our prediction for each image, collect these values to calculate an overall accuracy,  \n",
    "and then calculate **the gradients of each weight with respect to that overall accuracy.**\n",
    "\n",
    "Unfortunately, we have a **significant technical problem here.**  \n",
    "**The gradient of a function is its *slope***, or its steepness, which can be defined as ***rise over run***       \n",
    "— that is, how much the value of the function goes up or down, divided by how much we changed the input.  \n",
    "We can write this in mathematically as: **`(y_new - y_old) / (x_new - x_old)`**.  \n",
    "This gives us **a good approximation of the gradient when `x_new` is very similar to `x_old`**, meaning that their difference is very small.  \n",
    "**But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa.**  \n",
    "The problem is that a small change in weights from `x_old` to `x_new` isn't likely to cause any prediction to change,    \n",
    "so `(y_new - y_old)` will almost always be 0.   \n",
    "In other words, **the gradient is 0 almost everywhere.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very small change in the value of a weight will often not actually change the accuracy at all.  \n",
    "This means **it is not useful to use accuracy as a loss function**   \n",
    "— if we do, most of the time our gradients will actually be 0, and the model will not be able to learn from that number.  \n",
    "\n",
    "> S: In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5),  \n",
    "> so its derivative is nil almost everywhere (and infinity at the threshold).  \n",
    "> This then gives gradients that are 0 or infinite, which are useless for updating the model.  \n",
    "\n",
    "Instead, **we need a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss.**  \n",
    "So what does a \"slightly better prediction\" look like, exactly?  \n",
    "Well, in this case, it means that **if the correct answer is a 3 the score is a little higher, or if the correct answer is a 7 the score is a little lower.**  \n",
    "\n",
    "Let's write such a function now.  \n",
    "What form does it take?  \n",
    "\n",
    "**The loss function receives not the images themselves, but the predictions from the model.**  \n",
    "Let's make **one argument, `prds`, of values between 0 and 1, where each value is the prediction that an image is a 3.**  \n",
    "It is a vector (i.e., a rank-1 tensor), indexed over the images.  \n",
    "\n",
    "**The purpose of the loss function is to measure the difference between predicted values and the true values — that is, the targets (aka labels).**  \n",
    "**Let's make another argument, `trgts`, with values of 0 or 1 which tells whether an image actually is a 3 or not.**    \n",
    "It is also a vector (i.e., another rank-1 tensor), indexed over the images.  \n",
    "\n",
    "So, for instance, **suppose we had three images which we knew were a 3, a 7, and a 3.**    \n",
    "**And suppose our model predicted:**  \n",
    "    <u>with high confidence (`0.9`) that the first was a 3,</u>  \n",
    "    <u>with slight confidence (`0.4`) that the second was a 7,</u>        \n",
    "    <u>and with fair confidence (`0.2`), but incorrectly, that the last was a 7.</u>    \n",
    "This would mean **our loss function would receive these values as its inputs:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prds are received from the model (the model doesn't send images, but predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so let's have a look at an example\n",
    "let's say our targets, our labels, of like that are 3\n",
    "it's 3 images here: 1,0,1\n",
    "and we've made some predictions from a neural net\n",
    "and those predictions gave us 0.9,0.4,0.2\n",
    "\n",
    "[after the next cell text, jumps back here:]\n",
    "so you can see here, when the prediction is correct\n",
    "in other words, it's a high number when the target is 1\n",
    "and a low number when the target is 0\n",
    "so the worst one is when we predicted 0.2\n",
    "so we really thought that was a 0, but it's actually a one\n",
    "[continuation to torch.where(trgts==1, 1-prds, prds)]\n",
    "'''\n",
    "trgts = tensor([1,0,1])            # true images:     3,          7,               3              (1 if it's a 3, and 0 if it's not a 3)\n",
    "prds  = tensor([0.9, 0.4, 0.2])    # model predicts:  0.9 is 3    0.4 that is 7    0.2 that is 7  (incorrect)                                   \n",
    "#prds  = tensor([1, 0, 1])\n",
    "#prds  = 1-tensor([1, 0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's a first try at a loss function that measures the distance between `predictions` and `targets`:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so now consider this loss function\n",
    "we're going to use torch.where, which is the same as this list comprehension\n",
    "it's basically an if statement so it's going to say:\n",
    "for where targets equals 1, we're going to return 1-predictions\n",
    "and where is not 1 it will just be predictions\n",
    "'''\n",
    "# if it should be 1 (targets==1) then what is the distance from pred to 1 (1-pred)\n",
    "# if it should be 0 (targets==0) then what is the distance from pred to 0 (pred)\n",
    "def mnist_loss(predictions, targets):  \n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()  # the loss is the mean of all these distances between predictions and targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We're using a new function, `torch.where(a,b,c)`.**  \n",
    "This is the same as running the **list comprehension `[b[i] if a[i] else c[i] for i in range(len(a))]`,**  \n",
    "except it works on tensors, at C/CUDA speed.  \n",
    "In plain English, **this function will measure how distant each prediction is from 1 if it should be 1,      \n",
    "and how distant it is from 0 if it should be 0,  \n",
    "and then it will take the mean of all those distances.**  \n",
    "\n",
    "> note: Read the Docs: It's important to learn about PyTorch functions like this,  \n",
    "> because looping over tensors in Python performs at Python speed, not C/CUDA speed!  \n",
    "> Try running **`help(torch.where)`** now to read the docs for this function, or, better still, look it up on the PyTorch documentation site.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(torch.where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's try it on our `prds` and `trgts`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.4000, 0.8000])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so we ended up with 0.8 here because it is 1-prediction (1-0.2=0.8)\n",
    "'''\n",
    "torch.where(trgts==1, 1-prds, prds)  # if targets equals 1, replace with 1-prds, otherwise replace with prds: 1-0.9, 0.4, 1-0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can see that this function returns a lower number when:    \n",
    "predictions are more accurate,  \n",
    "when accurate predictions are more confident (higher absolute values), (prds=0.9 and trgts=1 -> returns 0.1)    \n",
    "and when inaccurate predictions are less confident.**    \n",
    "In PyTorch, we always assume that a lower value of a loss function is better.  \n",
    "**Since we need a scalar for the final loss, `mnist_loss` takes the mean of the previous tensor:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4333)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so we can then take the mean of all of these to calculate a loss\n",
    "so if you think about it, this loss will be the smallest if the predictions are exactly right\n",
    "so if we did predictions that are identical to the targes ([1,0,1]) then torch.where(trgts==1, 1-prds, prds) will be [0,0,0] ~ loss is 0, the ideal\n",
    "if they were exactly wrong (prds  = 1-tensor([1, 0, 1])) then torch.where/loss is [1,1,1]\n",
    "\n",
    "so the loss will be better (smaller) when the predictions are closer to the targets\n",
    "so here we can now take the mean and when we do, we get here 0.433\n",
    "'''\n",
    "mnist_loss(prds,trgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if we change our prediction for **the one \"false\" target from `0.2` to `0.8` the loss will go down**,  \n",
    "indicating that this is **a better prediction:**  \n",
    "me: why 0.8 for an incorrect 7 is better than 0.2 for an incorrect 7  \n",
    "because 0.9 is closer to 1 than to 0 so it predicts a 1, which is a 3  \n",
    "and 0.2 is closer to 0 than to 1, so it predicts a 0, which is a 7 (which is incorrect)  \n",
    "and 0.8 is closer to 1, so it predicts 1, which is a 3 (which is correct)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2333)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "let's say we change this last bad one in accurate prediction from 0.2 to 0.8 \n",
    "and the loss gets better, from 0.43 to 0.23 but this is just this function torch.where().mean()\n",
    "this is pretty good, a loss function which pretty closely tracks accuracy\n",
    "if the accuracy is better the loss will be smaller\n",
    "but also it doesn't have these 0 gradients because every time we change the prediction the loss changes\n",
    "because the prediction is literally harder?? the loss\n",
    "but this is going to work when predictions are between 0 and 1\n",
    "otherwise this 1-prediction thing is going to look a bit funny\n",
    "we should try and find a way to ensure that the predictions are always between 0 and 1\n",
    "and it makes more intuitively sense because we want to thing at these as probabilities [0.9, 0.4, 0.8]\n",
    "or at least nicely scaled numbers \n",
    "'''\n",
    "mnist_loss(tensor([0.9, 0.4, 0.8]),trgts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One problem with `mnist_loss` as currently defined is that it assumes that predictions are always between 0 and 1.**  \n",
    "**We need to ensure, then, that this is actually the case!**  \n",
    "As it happens, there is a function that does exactly that — let's take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `sigmoid` function always outputs a number between 0 and 1.**  \n",
    "It's defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so we need some function that can take our big numbers in preds tensor (cell 99) (20, 15, 18 etc)\n",
    "and turn them all into numbers between 0 and 1\n",
    "and it happens that we have exactly the right function, \n",
    "the sigmoid: if you pass in a really small number you get a number very close to 0\n",
    "if you pass in a big number you get a number very close to 1\n",
    "it never gets past 1 and it never goes smaller than 0\n",
    "and it's kind of like this smooth curve between\n",
    "and in the middle it looks a lot like the y = x line\n",
    "this is the definition of the sigmoid function\n",
    "\n",
    "what is exponential? is e to the power of something\n",
    "'''\n",
    "def sigmoid(x): \n",
    "    return 1/(1+torch.exp(-x)) # 1/(1+e^-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3890560989306495"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "e is just a number like pi\n",
    "'''\n",
    "math.e**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.3891)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(tensor(2))  # the same numbers as the cell above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch defines an accelerated version for us, so we don’t really need our own.  \n",
    "This is an important function in deep learning, since we often want to **ensure values are between 0 and 1.**    \n",
    "This is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_function(torch.sigmoid, title='Sigmoid', min=-4, max=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it takes any input value, positive or negative, and smooshes it onto an output value between 0 and 1.  \n",
    "**It's also a smooth curve that only goes up, which makes it easier for SGD to find meaningful gradients.** \n",
    "\n",
    "**Let's update `mnist_loss` to first apply `sigmoid` to the inputs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so we can change mnist_loss to be exactly the same as it was before but first we can make everything into sigmoid first\n",
    "and then use torch.where\n",
    "\n",
    "so that is a loss function that has all the properties we want\n",
    "it will not have those nasty 0 gradients and we ensure that the input  to the where() is between 0 and 1  \n",
    "so the reason we did this is because our accuracy was kind of what we really cared about to be a good accuracy    \n",
    "we can't use it to get our gradients, just to create our steps, to improve our parameters\n",
    "so we can change our accuracy to another function that is better when the accuracy is better\n",
    "but also it does not have these 0 gradients\n",
    "and so we can see now where, why we have a metric and a loss \n",
    "the metric is the thing we actually care about\n",
    "the loss is the thing that's similar to what we care about\n",
    "but has a nicely behaved gradient\n",
    "sometimes, the thing you care about, your metric, does have a nicely defined gradient\n",
    "and you can use it directly as a loss\n",
    "for example, we often use mean squared error but for classification unfortunately not\n",
    "'''\n",
    "def mnist_loss(predictions, targets):\n",
    "    predictions = predictions.sigmoid()\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can be confident our loss function will work, even if the predictions are not between 0 and 1.  \n",
    "**All that is required is that a higher prediction corresponds to higher confidence an image is a 3.**  \n",
    "\n",
    "Having defined a loss function, now is a good moment to recapitulate why we did this.  \n",
    "After all, we already had a metric, which was **overall accuracy.**  \n",
    "**So why did we define a loss?**  \n",
    "\n",
    "The key difference is that the metric is to drive human understanding and **the loss is to drive automated learning.**    \n",
    "**To drive automated learning, the loss must be a function that has a meaningful derivative.**  \n",
    "It can't have big flat sections and large jumps, but instead **must be reasonably smooth.**   \n",
    "This is why we designed a loss function that would respond to small changes in confidence level.  \n",
    "This requirement means that sometimes it does not really reflect exactly what we are trying to achieve,  \n",
    "**but is rather a compromise between our real goal and a function that can be optimized using its gradient.**  \n",
    "**The loss** function is calculated for **each item** in our dataset,  \n",
    "and then at the end of an epoch the loss values are all averaged and **the overall mean is reported for the epoch.**\n",
    "\n",
    "**Metrics, on the other hand, are the numbers that we really care about.**  \n",
    "These are the values that are **printed at the end of each epoch that tell us how our model is really doing.**  \n",
    "It is important that we learn to **focus on these metrics, rather than the loss, when judging the performance of a model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD and Mini-Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a loss function that is suitable for driving SGD,  \n",
    "we can consider some of the details involved in **the next phase of the learning process,**  \n",
    "which is to **change or update the weights based on the gradients.**  \n",
    "**This is called an *optimization step***.\n",
    "\n",
    "In order to take **an optimization step** we need to **calculate the loss over one or more data items.**  \n",
    "How many should we use? We could calculate it for **the whole dataset, and take the average,**  \n",
    "or we could calculate it for **a single data item.**  \n",
    "But neither of these is ideal.   \n",
    "Calculating it for the whole dataset would take **a very long time.**  \n",
    "Calculating it for a single item would not use much information, so it would result in **a very imprecise and unstable gradient.**  \n",
    "That is, you'd be going to the trouble of updating the weights,  \n",
    "but taking into account only how that would improve the model's performance on that single item.  \n",
    "\n",
    "So instead we take a **compromise** between the two: we calculate **the average loss for a few data items at a time.**  \n",
    "**This is called a *mini-batch*.**  \n",
    "The number of data items in the mini-batch is called the ***batch size***.  \n",
    "**A larger batch size** means that you will get a **more accurate and stable estimate** of your dataset's gradients from the loss function,  \n",
    "but it **will take longer**, and you will process fewer mini-batches per epoch.  \n",
    "Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately.  \n",
    "We will talk about how to make this choice throughout this book.  \n",
    "\n",
    "Another good reason for using **mini-batches** rather than calculating the gradient on individual data items is that,  \n",
    "in practice, we nearly always do our training on **an accelerator such as a GPU**.  \n",
    "These accelerators only perform well if they have **lots of work to do at a time,**  \n",
    "so it's helpful if we can give them lots of data items to work on.  \n",
    "Using mini-batches is one of the best ways to do this.  \n",
    "However, if you give them too much data to work on at once, they run out of memory — making GPUs happy is also tricky!  \n",
    "\n",
    "As we saw in our discussion of data augmentation in <<chapter_production>>,  \n",
    "**we get better generalization if we can vary things during training.**  \n",
    "One simple and effective thing **we can vary is what data items we put in each mini-batch.**  \n",
    "Rather than simply enumerating our dataset in order for every epoch,  \n",
    "instead what we normally do is **randomly shuffle it on every epoch, before we create mini-batches.**    \n",
    "**PyTorch and fastai provide a class that will do the shuffling and mini-batch collation for you, called `DataLoader`.**  \n",
    "\n",
    "**A `DataLoader` can take any Python collection and turn it into an iterator over mini-batches, like so:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 3, 12,  8, 10,  2]),\n",
       " tensor([ 9,  4,  7, 14,  5]),\n",
       " tensor([ 1, 13,  0,  6, 11])]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so we need to now use this to update the parameters\n",
    "so there's a couple of ways we could do this\n",
    "one would be to loop through every image\n",
    "calculate a prediction for that image \n",
    "and then calculate a loss\n",
    "and then do a step and then step the parameters\n",
    "and then do that again for the next image, and the next image\n",
    "that's going to be really slow because we're doing a single step for a single image\n",
    "so that would mean an epoch would take quite a while\n",
    "we could go much faster by doing every single image in the data set\n",
    "so a big matrix multiplication it can all be paralelized on the GPU\n",
    "and then so we can do a step based on the gradients looking at the entire dataset\n",
    "but now that's going to be like a lot of work to just update the weights once\n",
    "and remember sometimes our datasets have millions or tens of millions of items\n",
    "so that's probably a bad idea too\n",
    "so why not compromised?\n",
    "let's grab a few data items at a time to calculate our loss and our step\n",
    "if we grab a few data items at a time those 2 data items are called a mini batch\n",
    "and a mini-batch just means a few pieces of data\n",
    "and so the size of your mini batch is called the batch size\n",
    "so the bigger the batch size, the closer you get to the full size of your dataset\n",
    "the longer it's going to take to calculate a single set of losses, a single step\n",
    "but the more accurate it's going to be, it's going to be like, \n",
    "the gradients are going to be much closer to the true data set gradients\n",
    "and then the smaller the batch size, the faster each step we'll be able to do\n",
    "but those steps will represent a smaller number of items\n",
    "so they won't be such accurate approximation of the real gradient of the whole dataset\n",
    "\n",
    "so how do we ask for a few items at a time?\n",
    "it turns out that pytorch and fastai provides something to do that for you\n",
    "you can pass in any dataset to do this class called DataLoader\n",
    "and it will grab a few items from that dataset at a time\n",
    "how many: batch_size\n",
    "so it will grab a few items at the time, until it's grabbed all of them\n",
    "\n",
    "so here, let's create a collection (coll) that just contains all the numbers from 0 to 14\n",
    "let's pass that into a dataloader with a batch size of 5\n",
    "and then, that's going to be something called an iterator, in python\n",
    "it's something that you can ask for one more thing from an iterator\n",
    "if you pass an iterator to list in python, it returns all the things from the iterator\n",
    "so here are my three mini-batches (3 rows of 5) from 0 to 14, 5 at the time, appear in random order because suffle=true\n",
    "normally in the training set we ask for things to be suffled, so it gives us a little bit of more randomization\n",
    "more randomization is good, because it makes it harder for it to learn what the dataset looks like\n",
    "so that's how our DataLoader is created\n",
    "\n",
    "now remember though, that our datasets actually return tuples, and here I've just got single ints\n",
    "\n",
    "'''\n",
    "coll = range(15)\n",
    "dl = DataLoader(coll, batch_size=5, shuffle=True)\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For training a model, we don't just want any Python collection,  \n",
    "but a collection containing independent and dependent variables (that is, the inputs and targets of the model).**  \n",
    "**A collection that contains tuples of independent and dependent variables is known in PyTorch as a `Dataset`.**  \n",
    "Here's an example of an **extremely simple `Dataset`:**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#26) [(0, 'a'),(1, 'b'),(2, 'c'),(3, 'd'),(4, 'e'),(5, 'f'),(6, 'g'),(7, 'h'),(8, 'i'),(9, 'j')...]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so let's actually create a tuple, so if we enumerate all the letters of english\n",
    "then that means that returns (0, 'a'),(1, 'b') etc\n",
    "let's make that our dataset \n",
    "'''\n",
    "\n",
    "ds = L(enumerate(string.ascii_lowercase))\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we pass a `Dataset` to a `DataLoader` we will get back mini-batches**  \n",
    "which are themselves **tuples of tensors representing batches of independent and dependent variables:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([17, 18, 10, 22,  8, 14]), ('r', 's', 'k', 'w', 'i', 'o')),\n",
       " (tensor([20, 15,  9, 13, 21, 12]), ('u', 'p', 'j', 'n', 'v', 'm')),\n",
       " (tensor([ 7, 25,  6,  5, 11, 23]), ('h', 'z', 'g', 'f', 'l', 'x')),\n",
       " (tensor([ 1,  3,  0, 24, 19, 16]), ('b', 'd', 'a', 'y', 't', 'q')),\n",
       " (tensor([2, 4]), ('c', 'e'))]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so, if we pass that to a DataLoader with a batch size of 6, as you can see, \n",
    "it returns tuples containing 6 of the first things, and the associated 6 of the second things\n",
    "so this is like our independent variable (numbers) and this is like our dependent variable (letters)\n",
    "okay, so at the end the batch size weren't necessarily exactly divided nicely into the full size of the Dataset,\n",
    "you might end up wiht a smaller batch\n",
    "so basically then, we already have a Dataset remember, and so we could pass it to a DataLoader \n",
    "\n",
    "'''\n",
    "dl = DataLoader(ds, batch_size=6, shuffle=True)  # 26 elements from 0 to 25\n",
    "list(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to write our first training loop for a model using SGD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nand then we can basically say this\\nan iterator in python is something that you can actually loop through\\nso when we say \"for\" in dataloader it\\'s going to return a tuple\\nwe can destructure it in the 1st bit, 2nd bit so that\\'s going to be our x and y\\nwe can calculate our predictions, our loss from the predictions and the targets\\ncalculate our gradients (backward)\\nand we can update our parameters just like we did in our toy SGD example for the quadratic equation\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "and then we can basically say this\n",
    "an iterator in python is something that you can actually loop through\n",
    "so when we say \"for\" in dataloader it's going to return a tuple\n",
    "we can destructure it in the 1st bit, 2nd bit so that's going to be our x and y\n",
    "we can calculate our predictions, our loss from the predictions and the targets\n",
    "calculate our gradients (backward)\n",
    "and we can update our parameters just like we did in our toy SGD example for the quadratic equation\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "It's time to **implement the process we saw in <<gradient_descent>>.**  \n",
    "In code, our process will be implemented something like this **for each epoch:**\n",
    "\n",
    "```python\n",
    "for x,y in dl:\n",
    "    pred = model(x)\n",
    "    loss = loss_func(pred, y)\n",
    "    loss.backward()\n",
    "    parameters -= parameters.grad * lr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's **re-initialize our parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so that's reinitialize our weights with the same 2 lines of code before\n",
    "'''\n",
    "weights = init_params((28*28,1))\n",
    "bias = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A `DataLoader` can be created from a `Dataset`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 784]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "create the data loader this time from our mnist dataset\n",
    "and create a nice, big batch size\n",
    "so we do plenty of work each time\n",
    "and just to take a look, grab the 1st thing rom the dataloader\n",
    "first is a fastai function which grabs the 1st thing from an iterator (dl)\n",
    "it's useful to look at, kind of an arbitrary mini batch\n",
    "so here's the shape we're going to have\n",
    "the 1st mini batch is 256 rows of 784 long, \n",
    "that's 28 by 28 \n",
    "so 256 flattened out images (xb), \n",
    "and 256 labels that are 1 column (yb), 0 or 1, depending on whether it's a 3 or a 7\n",
    " \n",
    "'''\n",
    "dl = DataLoader(dset, batch_size=256)\n",
    "xb,yb = first(dl)  # first element of dl; 256 flattened out images and 256 labels that are 1 column\n",
    "xb.shape,yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We'll do the same for the validation set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so here's out validation dataloader\n",
    "'''\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create a mini-batch of size 4 for testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 784])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so let's grab a batch here, testing\n",
    "'''\n",
    "batch = train_x[:4]\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_2594/2527206448.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linear1??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0346],\n",
       "        [ 3.7392],\n",
       "        [15.1718],\n",
       "        [ 7.5507]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "well, for our testing I'm going to just manually grab the first 4 things just so that we make sure everything lines up\n",
    "pass it into that linear function, we created earlier\n",
    "remember linear was just x batch @ weights (matrix multiply) plus bias\n",
    "and that's going to give us 4 results\n",
    "that's a prediction of each of those 4 images\n",
    "'''\n",
    "preds = linear1(batch)  # who is linear1? the model: def linear1(xb): return xb@weights + bias  // preds = linear1(train_x) - here \n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**me: calculate the loss:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0348, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "and so then we can calculate the loss using that loss function we just used\n",
    "and let's just grab the fist 4 items of the training set (4 same as the preds)\n",
    "'''\n",
    "loss = mnist_loss(preds, train_y[:4])  # the loss between predictions and targets\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can calculate the gradients:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784, 1]), tensor(-0.0045), tensor([-0.0314]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "and now we can calculate the gradients of the training set\n",
    "and so the gradients are 784 by 1, so it's a column where every weight as a gradient\n",
    "it's what's the change in loss for a small change in that parameter \n",
    "and then the bias as a gradient, it's a single number, because the bias is just a single number\n",
    "'''\n",
    "loss.backward()\n",
    "weights.grad.shape,weights.grad.mean(),bias.grad  # 784 gradients in a vertical tensor-1 ; their mean is the 2nd value ; bias gradient is the 3rd\n",
    "#weights.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's put that all in a function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so we can take those 3 steps and put it in a function\n",
    "if you pass it an x batch, a y batch and some model \n",
    "then it's going to calculate the predictions, the loss, and do the backward step\n",
    "\n",
    "'''\n",
    "def calc_grad(xb, yb, model):  # all the above\n",
    "    preds = model(xb)\n",
    "    loss = mnist_loss(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**and test it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0090), tensor([-0.0627]))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "and here we see\n",
    "just to take a look, the mean of the weights gradient and the bias gradient\n",
    "and there it is\n",
    "'''\n",
    "calc_grad(batch, train_y[:4], linear1)  # linear1 is the model\n",
    "weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But look what happens if we call it twice:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0134), tensor([-0.0941]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if I call it the 2nd time, \n",
    "notice I haven't done any step here. these are exactly the same parameters\n",
    "I get a different value, that's a concern\n",
    "you would expect to get the same gradient every time you called it with the same data\n",
    "why have the gradients changed?\n",
    "that's because loss.backward doesn't just calculate the gradients \n",
    "it calculates the gradients and adds them to the existing gradients\n",
    "the things in the .grad attribute of weights (weights.grad)\n",
    "the reasons for that will come to later\n",
    "what we need to do is to call grad.zero_()\n",
    "so .zero returns a tensor containing 0s and remember _ does it in place, \n",
    "so that updates the weights.grad attribute, which is a tensor, to contain 0s\n",
    "so now, if I do that, and call it again, I will get exactly the same number\n",
    "\n",
    "'''\n",
    "calc_grad(batch, train_y[:4], linear1)\n",
    "weights.grad.mean(),bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients have **changed!**    \n",
    "The reason for this is that **`loss.backward` actually *adds* the gradients of `loss` to any gradients that are currently stored.**    \n",
    "**So, we have to set the current gradients to 0 first:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.grad.zero_()  # all elements of tensor weights.grad set to 0\n",
    "bias.grad.zero_();  # all elements of tensor bias.grad set to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note: Inplace Operations: Methods in PyTorch whose names end in an underscore modify their objects _in place_.  \n",
    "> For instance, **`bias.zero_()` sets all elements of the tensor `bias` to 0.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our only remaining step is to update the weights and biases based on the gradient and learning rate.**  \n",
    "When we do so, we have to **tell PyTorch not to take the gradient of this step too**  \n",
    "— otherwise things will get very confusing when we try to compute the derivative at the next batch!  \n",
    "**If we assign to the `data` attribute of a tensor then PyTorch will not take the gradient of that step.**  \n",
    "Here's our basic **training loop for an epoch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "here's how you train one epoch with SGD:\n",
    "loop through the DataLoader (for), grabbing the X batch and the y batch (xb,yb)\n",
    "calculate the gradient (composed of: prediction, loss, backward)\n",
    "go through each of the parameters (passed in) - 784 weights and 1 bias\n",
    "and for each of those update the parameter to go minus equals gradient * learning rate (that's our GD step)\n",
    "and then zero it out for the next time around the loop\n",
    "I'm not saying p -= but p.data\n",
    "and the reason for that is that, remember, pytorch keeps track of all of the calculations we do\n",
    "so that it can calculate the gradient\n",
    "well, I don't want to calculate the gradient in my gradient descent step\n",
    "that's like not part of the model , right?\n",
    "so .data is a special attribute in pytorch, where if you write to it, \n",
    "it tells pytorch not to update the gradients using that calculation\n",
    "\n",
    "so this is your most basic, standard SGD loop\n",
    "so the difference between stochastic GD and GD is that GD doesn't have this for loop that loops through each mini-batch\n",
    "GD it does it on the whole dataset each time around\n",
    "so train epoch or GD would simply not have the for loop,\n",
    "but instead would calculate the gradient for the whole dataset and update the parameters based on the whole dataset\n",
    "which we never really do in practice\n",
    "we always use mini-batches of various sizes\n",
    "'''\n",
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr  # If we assign to the `data` attribute of a tensor then PyTorch will not take the gradient of that step.\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to **check how we're doing, by looking at the accuracy of the validation set.**  (how to calculate accuracy?)  \n",
    "To decide if an output represents a 3 or a 7, we can just check whether it's greater than 0. (because a 3 is represented by 1?)   \n",
    "**So our accuracy for each item can be calculated (using broadcasting, so no loops!) with:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True],\n",
       "        [True],\n",
       "        [True],\n",
       "        [True]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds>0.0).float() == train_y[:4]  # preds = linear1(batch); if prediction > 0 means it's 1 and if it's equal to targets tensor (which has 1 and 0) then it's True, otherwise False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**That gives us this function to calculate our validation accuracy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ok, so we can take the function we had before where we compare the predictions to whether that are greater than 0.5\n",
    "we used to compare predictions with 0, but now that we're doing the sigmoid, everything is between 0 and 1\n",
    "we should compare the predictions to whether they're greater than 0.5 or not\n",
    "\n",
    "if it's greater than 0.5, just to look back at our sigmoid function\n",
    "so what used to be 0, is now on sigmoid 0.5\n",
    "so we need to make this slight change to our measure of accuracy \n",
    "so to calculate the accuracy for some x batch (preditions) and some y batch (assumed, targets)\n",
    "then we take the sigmoid of the predictions\n",
    "we compare them to 0.5 to tell us whether it's a 3 or not\n",
    "we check what the actual target was, to see which ones are correct (==yb)\n",
    "and then we take the mean, after converting booleans to floats\n",
    "\n",
    "'''\n",
    "def batch_accuracy(xb, yb):\n",
    "    preds = xb.sigmoid()\n",
    "    correct = (preds>0.5) == yb  # if prediction is bigger than 0.5 then it's a 1, otherwise is a 0 ; then compare with target yb (which is 1 or 0)\n",
    "    return correct.float().mean()  # return the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can check it works:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so we can check that accuracy:\n",
    "put our batch through our simple linear model\n",
    "compare it to the 4 items of the training set\n",
    "and there's the accuracy\n",
    "'''\n",
    "batch_accuracy(linear1(batch), train_y[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**and then put the batches together:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so if we do that for every batch in the validation set \n",
    "then we can loop through with a list comprehension\n",
    "every batch in the validation set, get the accuracy based on some model\n",
    "stack those all up together, so that this is a list (acs line)\n",
    "we want to turn that list into a tensor, where the items of the tensor are the items of the list\n",
    "that's what stack all those, take the mean, \n",
    "convert it to a standard python scalar (item)\n",
    "round it to 4 decimal places, just for display\n",
    "'''\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]  # compression: calculates the accuracy for each element in the valid dataloader (is like a for loop)\n",
    "    return round(torch.stack(accs).mean().item(), 4)  # returns the mean of accuracies in the bach: 1st concatenates them into a tensor and then takes the mean, and rounds it to 4 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(torch.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7186"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so here's our validation set accuracy\n",
    "as you would expect, it's about 50% because it's random\n",
    "'''\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our starting point.  \n",
    "**Let's train for one epoch, and see if the accuracy improves:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7621"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so we can now train for 1 epoch\n",
    "so we can say, remember \"train_epoch\" needed the parameters\n",
    "so the parameters in this case are the weights tensor and the bias tensor\n",
    "so train one epoch using the linear1 model, with lr of 1, with these 2 parameters\n",
    "and then validate and the result is 0.6883, 68.8%\n",
    "so we've trained an epoch\n",
    "'''\n",
    "lr = 1.\n",
    "params = weights,bias\n",
    "train_epoch(linear1, lr, params)\n",
    "validate_epoch(linear1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Then do a few more:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8974 0.933 0.9466 0.951 0.9549 0.9583 0.9598 0.9608 0.9612 0.9627 0.9637 0.9657 0.9676 0.9686 0.9696 0.9701 0.9706 0.972 0.9715 0.9715 "
     ]
    }
   ],
   "source": [
    "'''\n",
    "so let's repeat that many times\n",
    "train and validate, and you can see the accuracy goes up and up and up to about 97%\n",
    "so that's cool! we've built and SGD optimizer\n",
    "of a simple linear function that is getting about 97% on our simplified MNIST where there's just 3s and 7s\n",
    "so a lot of steps there, let's simplify this through some refactoring\n",
    "'''\n",
    "for i in range(20):\n",
    "    train_epoch(linear1, lr, params)\n",
    "    print(validate_epoch(linear1), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good!  \n",
    "We're already about at **the same accuracy as our \"pixel similarity\" approach**, and we've created a general-purpose foundation we can build on.  \n",
    "Our next step will be to **create an object that will handle the SGD step for us.**  \n",
    "In PyTorch, it's called **an *optimizer*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is such a general foundation,  \n",
    "PyTorch provides some **useful classes to make it easier to implement.**   \n",
    "The first thing we can do is replace our `linear1` function with **PyTorch's `nn.Linear` module.**  \n",
    "A *module* is an object of a class that inherits from the PyTorch `nn.Module` class.  \n",
    "Objects of this class behave identically to standard Python functions,  \n",
    "in that you can call them using parentheses and they will return the activations of a model.\n",
    "\n",
    "**`nn.Linear` does the same thing as our `init_params` and `linear` together.**  \n",
    "**It contains both the *weights* and *biases* in a single class.**  \n",
    "Here's how we replicate our model from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /tmp/ipykernel_2594/2527206448.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "linear1??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the simple refactoring we're going to do, we're going to create something called on optimizer class\n",
    "the first thing we'll do, we'll get rid  of the linear1 function\n",
    "but remember, the linear1 function does x@w+b\n",
    "there's a class in pytorch that does that equation for us, so we may as well use it\n",
    "it's called nn.Linear and does 3 things:\n",
    "it does that function for us\n",
    "and it also initializes the parameters for us\n",
    "so we don't have to do weights and bias init_params anymore\n",
    "we're going to create an nn.Linear class and that's going to create a matrix of size (28,28,1)\n",
    "and a bias o size 1\n",
    "it will set requires_grad=True for us\n",
    "it's all going to be encapsulated in this class\n",
    "and then when I call that as a function, it's going to do my x@w+b\n",
    "'''\n",
    "linear_model = nn.Linear(28*28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every PyTorch module knows **what parameters it has that can be trained;**  \n",
    "they are available through the **`parameters` method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 784]), torch.Size([1]))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so to see the parameters in it, we could expect it to contain 784 weights and 1 bias\n",
    "we can destructure it to w, b and see! it's 784 for the weights and 1 for the bias\n",
    "could be an interesting exercise for you to create this class yourself from scratch !! DO IT\n",
    "you should be able to, at this point\n",
    "so that you can confirm that you can recreate something that behaves exactly like nn.linear\n",
    "'''\n",
    "w,b = linear_model.parameters()\n",
    "w.shape,b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to **create an optimizer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so now that we've got this object which contains our parameters in a parameters method (.parameters())\n",
    "we can now create an optimizer\n",
    "so for our optimizer, we're going to pass it the parameters to optimize (params in init) and a learning rate, \n",
    "we'll store them away (with self.etc) \n",
    "and we'll have something called step which goes through each parameter (for p in params)\n",
    "and does that thing we just saw: p.data -= etc\n",
    "and it's also going to have something called zero_grad\n",
    "which goes through each parameter and zeroes it out, or we just set it to none\n",
    "so that's the thing we call Basic Optimizer\n",
    "so those are exactly the same lines of code we've already seen wrapped up into a class\n",
    "so we can now create an optimizer, passing in the parameters of the linear model, and our learning rate \n",
    "'''\n",
    "class BasicOptim:\n",
    "    def __init__(self,params,lr):                  # self is a way to intialize the data members without declaring them separately like in c++\n",
    "        self.params,self.lr = list(params),lr\n",
    "\n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: \n",
    "            p.data -= p.grad.data * self.lr\n",
    "\n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: \n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can create our optimizer by passing in the model's parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BasicOptim(linear_model.parameters(), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our training loop can now be simplified to:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef train_epoch(model, lr, params):\\n    for xb,yb in dl:\\n        calc_grad(xb, yb, model)\\n        for p in params:\\n            p.data -= p.grad*lr  # If we assign to the `data` attribute of a tensor then PyTorch will not take the gradient of that step.\\n            p.grad.zero_()\\n'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def train_epoch(model, lr, params):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad*lr  # If we assign to the `data` attribute of a tensor then PyTorch will not take the gradient of that step.\n",
    "            p.grad.zero_()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so now our training loop is: look through each mini batch in the data loader\n",
    "calculate the gradient, opt.step, opt.zero_grad\n",
    "'''\n",
    "def train_epoch(model):\n",
    "    for xb,yb in dl:\n",
    "        calc_grad(xb, yb, model)   # calc gradients\n",
    "        opt.step()                 # modify the weights\n",
    "        opt.zero_grad()            # make gradients zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our validation function doesn't need to change at all:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4352"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "validation function doesn't have to change\n",
    "'''\n",
    "validate_epoch(linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's put our little training loop in a function, to make things simpler:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so let's put our training loop into a function\n",
    "that's going to loop through a bunch of epochs, call an epoch\n",
    "print validate_epoch and then run it\n",
    "'''\n",
    "def train_model(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model)\n",
    "        print(validate_epoch(model), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The results are the same as in the previous section:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.9339 0.7861 0.8999 0.9292 0.9438 0.9551 0.9614 0.9653 0.9673 0.9687 0.9707 0.9726 0.9751 0.9761 0.9761 0.9775 0.978 0.9785 0.9785 "
     ]
    }
   ],
   "source": [
    "'''\n",
    "it's the same\n",
    "we're getting a slightly different result here, but it's much of the same idea \n",
    "so that's cool, we've now refactoring using our own optimizer and using pytorch built-in nn.linear class \n",
    "'''\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fastai provides the `SGD` class which, by default, does the same thing as our `BasicOptim`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4932 0.772 0.853 0.918 0.9346 0.9482 0.956 0.9634 0.9658 0.9687 0.9697 0.9717 0.9736 0.9751 0.9761 0.9766 0.9775 0.978 0.9785 0.9785 "
     ]
    }
   ],
   "source": [
    "'''\n",
    "and we don't actually need to use our basicOptim\n",
    "pytorch comes with something which does exactly this and it's called SGD\n",
    "so and actually this SGD is provided by fastai\n",
    "fastai and pytorch provide some overlapping functionality\n",
    "then it works much of the same way:\n",
    "you can pass to SGD your parameters and your learning rate, just like BasicOptim\n",
    "and train it and get the same result\n",
    "\n",
    "and as you can see, these classes that are in fastai and pytorch, are not mysterious\n",
    "they're just pretty thin wrappers around functionality that we've now written ourselves\n",
    "so there's quite a few steps there\n",
    "and if you haven't done gradient descent before, then there's a lot of unpacking\n",
    "\n",
    "so this lesson is kind of the key lesson\n",
    "it's the one where, you know, take a stop and a deep breath and make sure your comfortable\n",
    "what's:\n",
    "a dataset?\n",
    "dataloader?\n",
    "nn.linear?\n",
    "SGD\n",
    "if any of these don't make sens, go back to where we defined it from scratch using python code\n",
    "the dataloader we didn't define from scratch but the functionality is not particularly interesting\n",
    "you can create your own from scratch\n",
    "if you wanted to - that would be another pretty good exercise\n",
    "\n",
    "let's refactor some more\n",
    "'''\n",
    "linear_model = nn.Linear(28*28,1)\n",
    "opt = SGD(linear_model.parameters(), lr)  # changes the weights instead of step\n",
    "train_model(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fastai also provides `Learner.fit`**, which we can use instead of `train_model`.  \n",
    "**To create a `Learner` we first need to create a `DataLoaders`, by passing in our training and validation `DataLoader`s:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "fastai has some DataLoaders class, which is as we've mentioned before, a tiny class\n",
    "that you just pass it a bunch of dataloaders and it just stores them away\n",
    "as a .train and .valid\n",
    "even though it's a tiny class, it's super handy, because with that we now have a single object that knows all the data we have\n",
    "and so it can take sure that your training dataloader is shuffled and your validation loader isn't shuffled\n",
    "you know, make sure everything works properly\n",
    "so that's what the dataloaders class is:\n",
    "you can pass in the training and valid dataloader \n",
    "'''\n",
    "dls = DataLoaders(dl, valid_dl)  # train and valid dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To create a `Learner`** without using an application (such as `vision_learner`) **we need to pass in all the elements that we've created in this chapter:**    \n",
    "the `DataLoaders`,  \n",
    "the model,  \n",
    "the optimization function (which will be passed the parameters),   \n",
    "the loss function,  \n",
    "and optionally any metrics to print:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "and then the next thing we have in fastai is the learner class\n",
    "and the learner class is something where we're going to pass in our dataloaders\n",
    "we're going to pass in our model, we're going to pass in our optimization function,\n",
    "our loss function, and our metrics\n",
    "so all the stuf we've done manually - that's all learner does!\n",
    "it's just going to do that for us\n",
    "so it's just going to call this train_model() and this train_epoch()\n",
    "it's inside Learner \n",
    "'''\n",
    "\n",
    "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,            # pass in all the elements created\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call `fit`:  \n",
    "(me: fit means to run it for a number of epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.636295</td>\n",
       "      <td>0.503405</td>\n",
       "      <td>0.495584</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.500637</td>\n",
       "      <td>0.183814</td>\n",
       "      <td>0.846418</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.184307</td>\n",
       "      <td>0.181716</td>\n",
       "      <td>0.837586</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081262</td>\n",
       "      <td>0.107085</td>\n",
       "      <td>0.911187</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.043293</td>\n",
       "      <td>0.078071</td>\n",
       "      <td>0.932287</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.028451</td>\n",
       "      <td>0.062503</td>\n",
       "      <td>0.947498</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.022335</td>\n",
       "      <td>0.052797</td>\n",
       "      <td>0.955348</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.019610</td>\n",
       "      <td>0.046329</td>\n",
       "      <td>0.963199</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018227</td>\n",
       "      <td>0.041789</td>\n",
       "      <td>0.965653</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>0.038452</td>\n",
       "      <td>0.967125</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "so now if we go learn.fit() you can see again\n",
    "it's doing the same thing, it's doing the same thing, getting the same result\n",
    "and it's got some nice functionality, it's printing it out into a pretty table for us\n",
    "and it's showing us the losses and the accuracy and how long it takes \n",
    "but it's nothing magic, right?\n",
    "you've been able to do exactly the same thing by hand using python and pytorch\n",
    "ok, so these abstractions are here to let you write less code\n",
    "and to save some time and to save some cognitive overhead\n",
    "but they're not doing anything you can't do yourself\n",
    "and that's important, right\n",
    "because if they're doing things you can't do youself, you can't customize them\n",
    "you can't debug them, you can't profile them\n",
    "so we want to make sure that the stuff we've using is stuff that we undertand what it's doing\n",
    "\n",
    "so this is just a linear function, it's not great\n",
    "we want a neural network\n",
    "'''\n",
    "learn.fit(10, lr=lr)  # no of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there's nothing magic about the PyTorch and fastai classes.  \n",
    "They are just convenient pre-packaged pieces that make your life a bit easier!  \n",
    "(They also provide a lot of extra functionality we'll be using in future chapters.)\n",
    "\n",
    "**With these classes, we can now replace our linear model with a neural network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a Nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have a general procedure for optimizing the parameters of a function,  \n",
    "and we have tried it out on a very boring function: **a simple linear classifier.**  \n",
    "**A linear classifier is very constrained in terms of what it can do.**  \n",
    "To make it a bit **more complex** (and able to handle more tasks),  \n",
    "**we need to add something nonlinear between two linear classifiers  \n",
    "— this is what gives us a neural network.**\n",
    "\n",
    "**Here is the entire definition of a basic neural network:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so, how do we turn this into a neural network?\n",
    "remember this is a linear function, x@w+b\n",
    "to turn it into a neural network, we have two linear functions exactly the same,\n",
    "but with different weights and biases\n",
    "and in between this magic line of code, which takes the result of our 1st linear function\n",
    "and then does a max between that and 0\n",
    "so a max() of res and 0 is going to take any negative numbers and turn them into 0s\n",
    "so we're going to do a linear function, we're going to replace the negatives with 0\n",
    "and then take that and put it through another linear function\n",
    "that, believe it or not, is a neural net!\n",
    "'''\n",
    "def simple_net(xb):\n",
    "    res = xb@w1 + b1\n",
    "    res = res.max(tensor(0.0))  # ReLU: rectified liniar unit = replaces every negative number with a zero\n",
    "    res = res@w2 + b2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it!  \n",
    "**All we have in `simple_net` is two linear classifiers with a `max` function between them.**\n",
    "\n",
    "**Here, `w1` and `w2` are weight tensors, and `b1` and `b2` are bias tensors;**  \n",
    "that is, **parameters that are initially randomly initialized, just like we did in the previous section:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "so w1 and w2 were weight tensors, b1 and b2 are bias tensors (just like before)\n",
    "so we can initialize them (just like before)\n",
    "and we can now call exactly the same training code that we did before to roll these \n",
    "'''\n",
    "w1 = init_params((28*28,30))  # weight tensor  \n",
    "b1 = init_params(30)          # bias tensor    \n",
    "w2 = init_params((30,1))\n",
    "b2 = init_params(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key point about this is that **`w1` has 30 output activations (which means that `w2` must have 30 input activations, so they match).**  \n",
    "That means that the first layer can construct 30 different features, each representing some different mix of pixels.  \n",
    "You can change that `30` to anything you like, to make the model more or less complex.  \n",
    "\n",
    "**That little function `res.max(tensor(0.0))` is called a *rectified linear unit*, also known as *ReLU*.**  \n",
    "We think we can all agree that *rectified linear unit* sounds pretty fancy and complicated...  \n",
    "But actually, there's nothing more to it than **`res.max(tensor(0.0))` — in other words, replace every negative number with a zero.**  \n",
    "This tiny function is also available in **PyTorch as `F.relu`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "so res.max(tensor(0)) is called a rectified linear unit\n",
    "which you will always see refered to as ReLU\n",
    "and in pytorch it already has this function - it's called F.relu()\n",
    "and if we plot it you can see it's as you'd expect, \n",
    "it's 0 for all negative numbers and then it's y=x for positive numbers\n",
    "\n",
    "jargon: rectified linear unit seems complicated but it's just this line of code: res = res.max(tensor(0.0))\n",
    "and this happens a lot in deep learning \n",
    "\n",
    "so why do we do linear layer ReLU?\n",
    "well, if we got rid of the middle relu and just went linear layer, linear layer\n",
    "then you could rewrite that as a single linear layer when you multiply things and add and then multiply things and add\n",
    "and you can just change the coefficients and make it into a single multiply and then add\n",
    "so no matter how many linear layers we stack on top of each other, we can never make anything more effective than a simple linear model\n",
    "but if you put a non-linearity between the linear layers\n",
    "then actually you have the opposite\n",
    "\n",
    "this is where something called universal approximation theorem holds \n",
    "which is that if the size of the weight and bias matrices are big enough\n",
    "this can actually approximate any arbitrary function \n",
    "including the function of how do I recognize 3s from 7s or whatever  \n",
    "\n",
    "so this is kind of amazing, this tiny thing (res = res.max(tensor(0.0))\n",
    "is actually a universal function approximator as long as you have w1 b1 and w2 b2 have the right numbers\n",
    "and we know how to make them the right numbers\n",
    "we use SGD\n",
    "could take a very long time, it could take a lot of memory\n",
    "but the basic idea is that there is some solution to any computable problem \n",
    "and this is one of the biggest challenges a lot of beginners have to deep learning\n",
    "is that there's nothing else to it\n",
    "it's like, how do I make a neural net? that is the neural net\n",
    "how do I do deep learning training with SGD\n",
    "there's things to like make a train a bit faster\n",
    "there's things to.. you need a few less parameters but everything from here is just performance tweaks\n",
    "this is the key understanding of training a neural network\n",
    "'''\n",
    "plot_function(F.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> J: There is an enormous amount of jargon in deep learning, including terms like **_rectified linear unit_.**  \n",
    "> The vast vast majority of this jargon is no more complicated than can be implemented in a short line of code, as we saw in this example.  \n",
    "> The reality is that for academics to get their papers published they need to make them sound as impressive and sophisticated as possible.  \n",
    "> One of the ways that they do that is to introduce jargon.  \n",
    "> Unfortunately, this has the result that the field ends up becoming far more intimidating and difficult to get into than it should be.  \n",
    "> You do have to learn the jargon, because otherwise papers and tutorials are not going to mean much to you.  \n",
    "> But that doesn't mean you have to find the jargon intimidating.  \n",
    "> Just remember, when you come across a word or phrase that you haven't seen before,  \n",
    "> it will almost certainly turn out to be referring to a very simple concept.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The basic idea is that by using more linear layers, we can have our model do more computation,  \n",
    "and therefore model more complex functions.**  \n",
    "**But there's no point just putting one linear layer directly after another one,**  \n",
    "because when we multiply things together and then add them up multiple times,  \n",
    "that could be replaced by multiplying different things together and adding them up just once!  \n",
    "That is to say, **a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.**  \n",
    "\n",
    "**But if we put a nonlinear function between them, such as `max`, then this is no longer true.**  \n",
    "**Now each linear layer is actually somewhat decoupled from the other ones, and can do its own useful work.**  \n",
    "The `max` function is particularly interesting, because it operates as a simple `if` statement.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> S: Mathematically, we say the composition of two linear functions is another linear function.  \n",
    "> So, we can stack as many linear classifiers as we want on top of each other,  \n",
    "> and without nonlinear functions between them, it will just be the same as one linear classifier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy,  \n",
    "if you can find the right parameters for `w1` and `w2` and if you make these matrices big enough.**    \n",
    "For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together;  \n",
    "to make it closer to the wiggly function, we just have to use shorter lines.  \n",
    "This is known as the ***universal approximation theorem***.  \n",
    "The three lines of code that we have here are known as ***layers***.  \n",
    "**The first and third are known as *linear layers*,  \n",
    "and the second line of code is known variously as a *nonlinearity*, or *activation function*.**    \n",
    "\n",
    "Just like in the previous section, we can replace this code with something a bit simpler, **by taking advantage of PyTorch:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ok, we can simplify things a bit more\n",
    "we already know that we can use nn.linear to replace the weight and bias\n",
    "so let's do that for both \n",
    "and since we're simply taking the result of one function and passing it into the next (res to res)\n",
    "and take the result of that function and pass it to the next and so forth and then return the end (end res)\n",
    "this is called function composition\n",
    "function composition is when you just take the result of one function, pass it to a new one\n",
    "so every pretty much neural network is just doing function composition of linear layers\n",
    "and these (res.max etc) are called activation functions or nonlinearities\n",
    "so pytorch provides something to do function composition for us\n",
    "and it's called nn.sequential \n",
    "so it's gonna do a linear layer, \n",
    "pass the result to a ReLu\n",
    "you pass the result to a linear layer\n",
    "\n",
    "see here I'm not using F.relu, I'm using nn.relu\n",
    "this is identical, returns exactly the same thing\n",
    "but this is a class rather than a function\n",
    "'''\n",
    "simple_net = nn.Sequential(\n",
    "    nn.Linear(28*28,30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`nn.Sequential` creates a module that will call each of the listed layers or functions in turn.**  \n",
    "\n",
    "**`nn.ReLU` is a PyTorch module** that does exactly the same thing as the **`F.relu` function.**  \n",
    "Most functions that can appear in a model also have identical forms that are modules.  \n",
    "Generally, it's just a case of replacing `F` with `nn` and changing the capitalization.  \n",
    "When using `nn.Sequential`, PyTorch requires us to use the module version.  \n",
    "Since modules are classes, we have to instantiate them, which is why you see `nn.ReLU()` in this example.  \n",
    "\n",
    "**Because `nn.Sequential` is a module, we can get its parameters,**  \n",
    "which will return a list of all the parameters of all the modules it contains.  \n",
    "Let's try it out!  \n",
    "As this is a deeper model, we'll use a lower learning rate and a few more epochs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "now that we have a neural net (simple_net) \n",
    "we can use the same learner we had before but this time we pass in the simple_net instead of linear one\n",
    "everything else is the same and we can call fit just like before\n",
    "\n",
    "and generally, as your models get deeper\n",
    "so here we've gone from 1 layer to 2 (at simple_net, calculate only the parametrized layer (withotu relu))\n",
    "you could say it's 3. I'm going to call it 2,  there's 2 trainable layers\n",
    "so I've gone from 1 layer to 2 (started with linear1)\n",
    "I've dropped my learning r ate from 1 to 0.1 because the deeper models (with more layers) all tend to be bumpier\n",
    "less nicely behaved so often you need to use lower learning rates\n",
    "'''\n",
    "learn = Learner(dls, simple_net, opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.306178</td>\n",
       "      <td>0.399517</td>\n",
       "      <td>0.508832</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.142642</td>\n",
       "      <td>0.220382</td>\n",
       "      <td>0.817468</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.079635</td>\n",
       "      <td>0.112032</td>\n",
       "      <td>0.921001</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052750</td>\n",
       "      <td>0.076199</td>\n",
       "      <td>0.943572</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040225</td>\n",
       "      <td>0.059813</td>\n",
       "      <td>0.959274</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.033792</td>\n",
       "      <td>0.050623</td>\n",
       "      <td>0.963199</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030072</td>\n",
       "      <td>0.044810</td>\n",
       "      <td>0.965653</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.027641</td>\n",
       "      <td>0.040804</td>\n",
       "      <td>0.966634</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.025880</td>\n",
       "      <td>0.037863</td>\n",
       "      <td>0.969578</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.024511</td>\n",
       "      <td>0.035601</td>\n",
       "      <td>0.970069</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.023398</td>\n",
       "      <td>0.033796</td>\n",
       "      <td>0.972522</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.022466</td>\n",
       "      <td>0.032315</td>\n",
       "      <td>0.973013</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.021670</td>\n",
       "      <td>0.031068</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.020980</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.974485</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.020374</td>\n",
       "      <td>0.029070</td>\n",
       "      <td>0.974975</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.019836</td>\n",
       "      <td>0.028252</td>\n",
       "      <td>0.975957</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019354</td>\n",
       "      <td>0.027526</td>\n",
       "      <td>0.977429</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.018920</td>\n",
       "      <td>0.026875</td>\n",
       "      <td>0.977920</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.018526</td>\n",
       "      <td>0.026289</td>\n",
       "      <td>0.978410</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.018165</td>\n",
       "      <td>0.025757</td>\n",
       "      <td>0.978901</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017834</td>\n",
       "      <td>0.025275</td>\n",
       "      <td>0.979392</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017527</td>\n",
       "      <td>0.024833</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.017243</td>\n",
       "      <td>0.024428</td>\n",
       "      <td>0.979882</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.016978</td>\n",
       "      <td>0.024054</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.016731</td>\n",
       "      <td>0.023709</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016499</td>\n",
       "      <td>0.023388</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.016280</td>\n",
       "      <td>0.023091</td>\n",
       "      <td>0.980373</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.022814</td>\n",
       "      <td>0.980864</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.015878</td>\n",
       "      <td>0.022556</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.015693</td>\n",
       "      <td>0.022315</td>\n",
       "      <td>0.981354</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.015517</td>\n",
       "      <td>0.022090</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>0.021879</td>\n",
       "      <td>0.982336</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.015190</td>\n",
       "      <td>0.021681</td>\n",
       "      <td>0.982826</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.015037</td>\n",
       "      <td>0.021495</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.014891</td>\n",
       "      <td>0.021319</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.014751</td>\n",
       "      <td>0.021154</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.014616</td>\n",
       "      <td>0.020998</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.014486</td>\n",
       "      <td>0.020850</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.014362</td>\n",
       "      <td>0.020710</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.014241</td>\n",
       "      <td>0.020577</td>\n",
       "      <td>0.983317</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide_output\n",
    "learn.fit(40, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not showing the 40 lines of output here to save room;  \n",
    "**the training process is recorded in `learn.recorder`,**  \n",
    "with the table of output stored in the `values` attribute,  \n",
    "so we can plot **the accuracy over training as:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "and so we've trained for a while, and we can actually find out what that training looks like\n",
    "by looking inside our learner and there's an attribute we create for you called recorder \n",
    "and that's going to record everything that appears in this table (learn.fit results)\n",
    "bascally, these 3 things: training loss, validation loss and the batch accuracy or any metrics \n",
    "so recorded our values, contains that table of results\n",
    "and so item number 2 of each row will be the accuracy  \n",
    "so the capital L class, which I'm using here as a nice little method called .itemgot\n",
    "that will get the second item for every row and then I can plot that to see how the training went\n",
    "'''\n",
    "plt.plot(L(learn.recorder.values).itemgot(2));  # plot accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And we can view the final accuracy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983316957950592"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "and I can get the final accuracy by grabbing the last row of the table and index 2 (0,1,2) - 98.3%\n",
    "\n",
    "1. we now have a function that can solve any problem to any level of accuracy if we can find the right parameters\n",
    "2. and we have a way to find the best or at least the very good set of parameters for any function\n",
    "\n",
    "'''\n",
    "learn.recorder.values[-1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=30, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=30, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "if you want to grab the model\n",
    "you can look inside the model to see the actual model that we just trained and you can see it's got the 3 things in it:\n",
    "the linear\n",
    "the relu\n",
    "the linear\n",
    "\n",
    "I put that into a variable to make it easier to work with\n",
    "'''\n",
    "#learn.model\n",
    "m = learn.model\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7eff384082e0>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "you can grab one layer by indexing\n",
    "you can look at the parameters, this will give something called a generator\n",
    "'''\n",
    "m[0].parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "it's something that will give me a list of the parameters when I ask for them \n",
    "with w,b to destructure them\n",
    "'''\n",
    "w,b = m[0].parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 784]), torch.Size([30]))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so the weight is 30 by 784 because that's what I ask for\n",
    "so one of the things to note here is that to create a neural net (so something that's more than one layer)\n",
    "I actually have 30 outputs, not just 1, right? [simple_net = nn.seq (nn.Linear(xx,30))]\n",
    "so I'm kind of generating lots of features\n",
    "so it's kind of like 30 different linear models here\n",
    "and then I combined those 30 back into 1 (with the 2nd linear)\n",
    "'''\n",
    "w.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.0100e-02,  1.9052e-02,  2.9477e-02, -3.4425e-02,  6.4140e-03, -1.0457e-02,  2.1451e-02,  1.5624e-02,  1.7567e-02,  3.0049e-02,  3.1262e-02, -2.0492e-02,  2.5520e-02, -8.0380e-03,\n",
       "         3.3780e-02, -2.6922e-02, -7.4052e-04, -2.2814e-02, -3.5086e-02, -3.5390e-02, -2.3421e-02, -7.2024e-03,  2.3453e-02, -3.5592e-02,  1.7864e-02,  2.1052e-02, -1.1565e-02,  2.7005e-03,\n",
       "        -1.2803e-02, -8.2192e-03, -1.4972e-02,  2.1738e-02, -2.0701e-02,  2.6458e-02,  1.2580e-02,  2.9975e-02,  1.5550e-03,  2.6571e-02, -2.6625e-02, -5.1075e-03, -7.5398e-03, -1.8958e-02,\n",
       "        -2.4748e-02,  3.2386e-02,  3.0870e-03,  2.0842e-02,  7.0747e-03,  1.8930e-02, -3.0142e-02, -1.5400e-03,  1.8391e-02,  2.7783e-04, -1.3994e-02,  2.0310e-03, -2.8694e-02,  1.3494e-02,\n",
       "         6.0580e-03, -2.8467e-02,  2.0373e-02, -3.0352e-02,  1.8536e-02, -2.3559e-02, -3.3395e-02,  5.8225e-03, -5.0159e-03,  2.8023e-02, -1.6682e-02,  3.3784e-02,  8.7398e-03,  4.3036e-03,\n",
       "         6.7939e-03,  9.9093e-03,  1.8955e-02,  3.0378e-03,  3.1855e-02, -2.9978e-02, -1.5126e-02, -1.1854e-02, -2.4286e-02, -2.6230e-02, -2.2343e-02,  6.9593e-03, -1.2407e-02, -2.8843e-02,\n",
       "         1.7156e-02,  3.1700e-02,  1.2955e-02, -2.4392e-02,  3.4875e-02,  2.2692e-03, -1.7238e-03,  3.0696e-03,  3.1296e-02, -1.7998e-02, -2.3488e-02,  1.3132e-02, -2.7540e-02, -1.3279e-02,\n",
       "        -4.0666e-03,  2.9327e-02, -1.1587e-02, -1.3309e-03, -3.1880e-02,  2.0852e-02, -2.8540e-02,  1.1306e-03,  8.1609e-03, -1.0819e-02,  9.5431e-03,  5.0727e-03, -2.2132e-02, -2.9542e-02,\n",
       "         3.3530e-02, -3.1289e-02, -1.4341e-02,  1.8102e-02, -6.5292e-03, -6.9081e-03,  4.2107e-03, -5.3818e-03,  1.5888e-02, -1.4481e-02,  9.1799e-03, -3.3075e-02, -4.5160e-03,  2.7586e-02,\n",
       "         1.7623e-02,  9.8776e-04,  6.6736e-03,  3.2544e-02, -7.0391e-03,  2.1011e-02,  2.9218e-02, -9.3723e-03,  2.0250e-02,  1.1683e-02, -1.3068e-02, -2.5773e-02, -7.3002e-03,  2.9180e-02,\n",
       "         1.8386e-02,  2.7890e-02, -2.3213e-02, -3.3307e-02, -8.3935e-03, -3.3808e-02,  2.3468e-02, -6.7565e-03, -2.0933e-02, -1.9367e-03, -7.4079e-03,  2.1271e-03,  2.0525e-02, -2.0643e-02,\n",
       "         1.5783e-02,  3.3020e-03,  2.2892e-02,  5.2380e-03,  1.8080e-02, -2.9667e-02,  2.0588e-02,  2.4905e-02,  3.0638e-02,  2.7005e-03,  1.1694e-03,  3.1949e-02, -2.8451e-02, -2.7860e-03,\n",
       "         9.3002e-03, -1.8896e-02,  3.0437e-02,  2.5676e-02,  2.8249e-02,  1.8023e-02,  1.3451e-02,  1.1384e-02,  2.4763e-02,  6.0954e-03,  2.3266e-03,  3.1216e-03,  6.1010e-03, -2.7686e-02,\n",
       "         2.2275e-02,  3.3521e-02,  9.5820e-03,  7.1440e-03, -4.0795e-03, -2.7639e-02, -3.1982e-02, -2.5996e-02, -2.2027e-02, -3.0632e-02,  1.0510e-02,  3.5086e-02, -7.6244e-03,  2.6221e-02,\n",
       "         1.8796e-02,  2.6979e-02,  1.3241e-02,  2.7914e-02,  2.6861e-02, -1.6842e-02,  1.0052e-02,  2.3553e-02, -2.7427e-02, -1.9343e-02,  2.2545e-02, -1.3384e-02, -1.9607e-02, -1.4618e-03,\n",
       "         2.4487e-03, -1.2935e-02, -2.2972e-02, -2.5972e-02, -2.1002e-02,  3.4956e-02,  8.6627e-03, -2.1437e-02,  3.3590e-02,  1.9792e-02, -1.4163e-03, -3.3097e-03, -2.5342e-02, -1.3373e-02,\n",
       "         2.9711e-02, -2.2141e-02,  4.2886e-03,  2.5150e-02, -2.3678e-02,  3.7046e-03,  1.0861e-02,  3.5056e-02, -2.0132e-02, -1.6570e-02,  1.0138e-02,  1.2072e-02,  3.0836e-02,  1.6803e-02,\n",
       "         2.9841e-02,  3.4730e-02, -9.6132e-03,  1.9338e-02,  2.2798e-02, -1.5595e-02, -1.2434e-02,  1.2320e-02,  2.0396e-02,  6.1655e-03, -2.3733e-02, -2.9404e-02,  1.2718e-02, -2.2195e-02,\n",
       "         9.2743e-03,  1.9619e-03,  3.1725e-02,  2.4449e-02,  3.2248e-02,  2.2948e-02, -2.7984e-02,  2.5492e-02,  2.8798e-02,  1.9777e-02, -3.7627e-03, -2.3726e-02, -3.4296e-02,  2.8787e-02,\n",
       "         8.1830e-03, -3.0427e-02,  2.3047e-02,  3.1893e-02,  5.5306e-03,  1.9716e-02, -3.6311e-02, -2.1563e-02, -3.3929e-02, -3.3160e-02,  2.5111e-03, -4.4145e-03, -1.0600e-02,  1.3544e-03,\n",
       "         1.8595e-02,  2.8290e-02, -1.8094e-02,  2.0580e-02, -1.3648e-02,  1.6732e-02, -2.1803e-02, -2.8043e-02,  2.9074e-03, -8.4849e-03, -4.1793e-04, -9.0391e-03, -1.0261e-02, -2.6850e-02,\n",
       "         3.2235e-02, -2.3355e-02,  3.3514e-02, -1.9719e-02,  2.0990e-02, -3.0800e-02,  3.0041e-02,  3.2446e-02,  3.4478e-02,  3.0127e-02,  3.4854e-02, -2.4413e-02, -7.4962e-03,  1.7429e-02,\n",
       "         2.0331e-04, -2.8486e-02, -3.0242e-02,  3.4714e-02,  3.3975e-02,  5.2891e-03,  1.7405e-02,  3.1916e-02, -2.7612e-02, -2.9805e-02,  2.6351e-02,  3.4394e-02, -2.3880e-02, -3.3863e-02,\n",
       "         9.6791e-03, -1.0760e-02, -3.1994e-02, -2.2862e-02, -3.3457e-02,  1.9186e-02, -1.8298e-02,  2.2675e-02, -4.4533e-03, -1.7165e-02,  7.7176e-04,  3.5330e-02,  3.2632e-02, -1.7789e-02,\n",
       "        -4.5266e-04,  2.3900e-02,  3.3423e-02,  2.3244e-02, -1.5952e-02,  1.9251e-02,  3.1234e-03,  2.1334e-02, -5.7725e-03,  6.2473e-03, -3.3475e-02, -2.4823e-02,  2.7904e-02, -2.1428e-02,\n",
       "        -2.4428e-02, -2.3628e-02,  9.8815e-03,  4.8129e-03,  1.6339e-02,  2.4855e-02, -1.5063e-02,  7.9751e-03,  3.3191e-02,  3.0620e-04, -3.8358e-03,  2.2900e-02, -4.9415e-03, -1.5830e-02,\n",
       "        -3.5562e-02,  2.8699e-03,  1.0342e-02, -2.3263e-02, -5.4888e-03, -1.3148e-02,  1.6970e-02,  6.5005e-04, -2.8536e-02,  3.3215e-02,  2.4699e-02, -2.3269e-02,  2.9153e-02, -3.8155e-03,\n",
       "         9.5395e-03,  1.3381e-02, -2.4006e-02, -2.8903e-02,  1.9880e-03,  5.0263e-03,  2.2895e-02, -8.1852e-03,  3.1173e-02,  3.2590e-02,  3.3139e-02,  1.2684e-02, -1.2354e-02, -5.0985e-03,\n",
       "         2.6791e-03,  3.3060e-02,  1.8296e-03, -2.8309e-02, -4.9953e-03, -2.3072e-02, -2.4657e-02, -1.2896e-02,  2.5578e-02, -3.3363e-02,  6.5889e-03,  2.6956e-02, -1.7974e-02,  1.1916e-02,\n",
       "         2.9188e-02,  2.9236e-03,  4.8159e-03, -1.0457e-02, -1.7739e-02, -2.0173e-03, -7.9490e-03,  1.0371e-02,  2.9917e-02, -2.7803e-02,  3.1787e-02,  2.4621e-02, -3.3973e-02, -3.2844e-02,\n",
       "         2.3110e-02, -8.0960e-04,  2.5327e-02,  2.7596e-02, -6.5490e-03, -2.9048e-02,  2.6788e-03, -1.2365e-02, -6.3723e-03,  1.3840e-03,  3.7096e-02, -1.9973e-02,  5.5125e-03,  2.2910e-02,\n",
       "        -1.2642e-02, -3.2968e-02, -1.6270e-02, -1.6600e-02,  3.0828e-02, -7.8388e-03,  1.7049e-02,  2.7660e-02, -2.1496e-02, -7.1072e-03, -1.2663e-02,  1.2439e-02,  2.4467e-02, -2.5202e-02,\n",
       "        -2.3859e-02, -3.6376e-04, -1.0673e-02,  3.3861e-02,  2.9044e-02, -1.4667e-02,  2.3828e-02, -7.6031e-04,  2.1271e-02, -4.9105e-03,  2.5785e-02, -9.9433e-04,  3.4853e-02, -8.2993e-03,\n",
       "         2.4509e-02,  3.2708e-02, -2.1295e-02, -2.4674e-02,  9.2792e-04, -1.9536e-02,  5.9921e-03,  8.0147e-03,  1.8139e-02,  3.4997e-03, -2.0968e-02, -1.4981e-02,  2.5049e-02,  3.1914e-02,\n",
       "         1.6478e-02, -9.8413e-03,  2.1152e-02, -2.0721e-02,  2.8163e-02, -8.3458e-03, -5.8735e-03,  3.4351e-02, -3.1913e-02,  1.2480e-02, -9.5424e-03,  6.9360e-03,  4.6267e-03, -1.2927e-02,\n",
       "        -3.0776e-03, -8.9449e-04, -1.6727e-03,  3.4695e-02, -7.6762e-04, -1.6695e-02,  1.5070e-02, -2.2361e-02, -1.4973e-02,  1.1589e-03,  1.7213e-02,  4.0898e-03,  7.5423e-03, -8.5386e-03,\n",
       "         3.2487e-02, -2.7469e-02, -2.6263e-02, -2.7123e-02,  2.0052e-02,  1.3386e-02,  8.0526e-03,  5.7796e-04, -1.7362e-02, -3.0342e-02,  2.4202e-02,  2.1593e-02,  1.7607e-02, -1.1796e-02,\n",
       "        -3.0211e-02, -1.2164e-02, -3.3959e-02, -8.3000e-03,  3.2813e-02,  6.1056e-03,  3.7412e-02, -6.1167e-03, -1.7980e-02, -2.7798e-02,  3.4728e-02, -3.3302e-02, -2.1834e-02, -1.4584e-02,\n",
       "         1.1960e-02,  2.3910e-02,  1.5613e-02,  2.6374e-03,  3.3811e-02,  2.2980e-02, -1.5910e-02,  2.8421e-05,  3.0841e-02, -2.0946e-02, -1.1640e-04, -5.2644e-03, -3.4909e-02,  2.2653e-02,\n",
       "         1.8018e-02,  1.6996e-02, -1.9426e-02,  3.3780e-02, -2.0592e-02, -1.6257e-02, -2.9827e-02,  3.3317e-02,  1.9841e-02, -8.9863e-03, -2.9572e-02,  5.2923e-03, -3.3699e-02,  2.6461e-02,\n",
       "        -2.5501e-02,  1.2259e-02,  1.5505e-03,  2.9444e-02, -1.4601e-02, -2.3223e-03, -2.5993e-02, -1.5846e-02,  3.5217e-02,  3.2394e-02,  1.3928e-02, -2.9676e-02,  1.8758e-02, -3.4917e-02,\n",
       "        -2.5517e-02, -3.2348e-02, -1.3994e-02, -1.2474e-02, -2.4258e-02, -1.9634e-02, -1.3562e-02, -1.4130e-02,  1.6854e-02,  2.7017e-02,  7.3784e-03,  5.3604e-03,  2.8601e-02, -4.5662e-03,\n",
       "        -2.1988e-02, -2.9223e-02, -2.5917e-02,  9.3631e-03,  2.8669e-02, -3.1183e-03, -4.4533e-03,  3.1306e-02, -1.7065e-02,  2.8505e-02, -2.3615e-02,  3.1433e-02, -3.4268e-02, -2.4335e-02,\n",
       "         1.3552e-02,  1.9954e-02, -1.4725e-02, -1.3079e-02,  3.3251e-02,  2.5521e-02, -3.3144e-02,  2.9529e-02,  7.9410e-03, -2.8344e-02, -4.2633e-03, -1.1169e-03,  1.3612e-02, -2.8213e-02,\n",
       "        -1.0653e-02, -9.5090e-03, -1.1727e-02,  9.7677e-03,  4.5918e-03,  1.3089e-02, -6.6265e-03,  1.3034e-04,  9.0692e-03,  1.8363e-02, -1.8899e-02, -1.2218e-02,  1.3894e-02, -1.8385e-02,\n",
       "         2.1835e-02,  6.4048e-03,  3.2354e-02,  3.2491e-02, -1.1982e-02,  2.1772e-02,  5.2871e-03,  1.4337e-02,  9.5214e-03,  4.9568e-03,  1.0718e-03,  4.8552e-03, -3.2840e-02, -2.0897e-02,\n",
       "         1.8857e-03, -1.3880e-02,  6.5363e-03, -1.0569e-02,  6.4096e-03,  2.4683e-02,  1.2461e-02,  2.0968e-02, -2.1183e-02,  2.8970e-03, -2.9254e-02,  1.6804e-02, -2.8932e-02,  2.1945e-02,\n",
       "         9.7876e-03,  2.1583e-03,  7.8484e-03,  1.2332e-02,  2.9425e-02, -2.0639e-02, -2.1718e-02, -1.1277e-02, -3.4467e-02, -6.6153e-03, -2.3350e-02,  1.1798e-03,  2.6016e-03,  2.8054e-02,\n",
       "        -1.7759e-02, -3.3795e-02,  8.3266e-03,  7.7279e-03, -3.0559e-02,  6.0052e-03,  3.2611e-02, -3.0394e-02, -2.6931e-02,  1.4825e-03,  2.1644e-02,  3.2306e-02, -1.7513e-02, -3.2649e-02,\n",
       "         1.3864e-02, -1.1775e-02, -2.6132e-03, -3.2011e-02, -2.1418e-02, -1.2311e-02, -2.3495e-02,  1.5973e-03, -1.2327e-02,  2.1344e-02,  2.0224e-02,  1.3612e-02,  2.7021e-02, -3.0458e-02,\n",
       "        -1.6825e-02, -8.1205e-03,  3.3772e-02,  2.9821e-02, -3.5859e-03, -3.8089e-03, -2.5602e-02,  3.0489e-02, -1.0103e-02, -7.3859e-03,  3.3688e-02,  4.2877e-04, -1.2581e-02, -3.3731e-03,\n",
       "         3.2805e-02,  2.8000e-02, -3.4536e-02,  1.7392e-02, -1.3081e-02,  1.6717e-02,  2.5319e-02,  2.7518e-02, -2.2179e-02, -1.9486e-02,  1.3334e-02,  3.4252e-02,  3.9211e-04, -9.3778e-03,\n",
       "         2.7552e-02, -1.3500e-02, -2.4855e-02,  1.4765e-02, -1.4204e-02,  2.7967e-02,  2.9331e-04, -1.8660e-02, -7.3153e-03,  2.0472e-02, -3.2026e-02, -2.9011e-03,  2.2659e-02,  2.2470e-02,\n",
       "         6.8929e-03, -9.6212e-03, -1.4527e-02,  2.3179e-02, -2.4254e-03,  7.9853e-03,  1.3444e-02,  6.9830e-03, -1.2130e-02,  2.5526e-03, -2.8085e-02,  2.6966e-02, -9.1442e-03,  1.9278e-02,\n",
       "         2.3549e-02,  1.9741e-02,  2.7247e-02,  2.9003e-02, -2.2124e-02, -2.8421e-02,  8.0246e-03,  3.3794e-02, -1.9599e-02,  3.2354e-02, -2.2026e-05,  3.0739e-02,  4.9774e-03,  2.8501e-02,\n",
       "        -2.7780e-02,  1.9464e-02,  1.5982e-02, -2.7337e-02,  3.1889e-02, -3.3556e-04,  2.3811e-02, -4.6703e-03, -2.9452e-02,  2.3689e-02, -3.3725e-03, -1.8790e-02,  2.5523e-02,  1.4083e-02],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "so you could look at one of those by having a look at here\n",
    "so there's the numbers in the first row\n",
    "'''\n",
    "w[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0100e-02,  1.9052e-02,  2.9477e-02, -3.4425e-02,  6.4140e-03, -1.0457e-02,  2.1451e-02,  1.5624e-02,  1.7567e-02,  3.0049e-02,  3.1262e-02, -2.0492e-02,  2.5520e-02, -8.0380e-03,\n",
       "          3.3780e-02, -2.6922e-02, -7.4052e-04, -2.2814e-02, -3.5086e-02, -3.5390e-02, -2.3421e-02, -7.2024e-03,  2.3453e-02, -3.5592e-02,  1.7864e-02,  2.1052e-02, -1.1565e-02,  2.7005e-03],\n",
       "        [-1.2803e-02, -8.2192e-03, -1.4972e-02,  2.1738e-02, -2.0701e-02,  2.6458e-02,  1.2580e-02,  2.9975e-02,  1.5550e-03,  2.6571e-02, -2.6625e-02, -5.1075e-03, -7.5398e-03, -1.8958e-02,\n",
       "         -2.4748e-02,  3.2386e-02,  3.0870e-03,  2.0842e-02,  7.0747e-03,  1.8930e-02, -3.0142e-02, -1.5400e-03,  1.8391e-02,  2.7783e-04, -1.3994e-02,  2.0310e-03, -2.8694e-02,  1.3494e-02],\n",
       "        [ 6.0580e-03, -2.8467e-02,  2.0373e-02, -3.0352e-02,  1.8536e-02, -2.3559e-02, -3.3395e-02,  5.8225e-03, -5.0159e-03,  2.8023e-02, -1.6682e-02,  3.3784e-02,  8.7398e-03,  4.3036e-03,\n",
       "          6.7939e-03,  9.9093e-03,  1.8955e-02,  3.0378e-03,  3.1855e-02, -2.9978e-02, -1.5126e-02, -1.1854e-02, -2.4286e-02, -2.6230e-02, -2.2343e-02,  6.9593e-03, -1.2407e-02, -2.8843e-02],\n",
       "        [ 1.7156e-02,  3.1700e-02,  1.2955e-02, -2.4392e-02,  3.4875e-02,  2.2692e-03, -1.7238e-03,  3.0696e-03,  3.1296e-02, -1.7998e-02, -2.3488e-02,  1.3132e-02, -2.7540e-02, -1.3279e-02,\n",
       "         -4.0666e-03,  2.9327e-02, -1.1587e-02, -1.3309e-03, -3.1880e-02,  2.0852e-02, -2.8540e-02,  1.1306e-03,  8.1609e-03, -1.0819e-02,  9.5431e-03,  5.0727e-03, -2.2132e-02, -2.9542e-02],\n",
       "        [ 3.3530e-02, -3.1289e-02, -1.4341e-02,  1.8102e-02, -6.5292e-03, -6.9081e-03,  4.2107e-03, -5.3818e-03,  1.5888e-02, -1.4481e-02,  9.1799e-03, -3.3075e-02, -4.5160e-03,  2.7586e-02,\n",
       "          1.7623e-02,  9.8776e-04,  6.6736e-03,  3.2544e-02, -7.0391e-03,  2.1011e-02,  2.9218e-02, -9.3723e-03,  2.0250e-02,  1.1683e-02, -1.3068e-02, -2.5773e-02, -7.3002e-03,  2.9180e-02],\n",
       "        [ 1.8386e-02,  2.7890e-02, -2.3213e-02, -3.3307e-02, -8.3935e-03, -3.3808e-02,  2.3468e-02, -6.7565e-03, -2.0933e-02, -1.9367e-03, -7.4079e-03,  2.1271e-03,  2.0525e-02, -2.0643e-02,\n",
       "          1.5783e-02,  3.3020e-03,  2.2892e-02,  5.2380e-03,  1.8080e-02, -2.9667e-02,  2.0588e-02,  2.4905e-02,  3.0638e-02,  2.7005e-03,  1.1694e-03,  3.1949e-02, -2.8451e-02, -2.7860e-03],\n",
       "        [ 9.3002e-03, -1.8896e-02,  3.0437e-02,  2.5676e-02,  2.8249e-02,  1.8023e-02,  1.3451e-02,  1.1384e-02,  2.4763e-02,  6.0954e-03,  2.3266e-03,  3.1216e-03,  6.1010e-03, -2.7686e-02,\n",
       "          2.2275e-02,  3.3521e-02,  9.5820e-03,  7.1440e-03, -4.0795e-03, -2.7639e-02, -3.1982e-02, -2.5996e-02, -2.2027e-02, -3.0632e-02,  1.0510e-02,  3.5086e-02, -7.6244e-03,  2.6221e-02],\n",
       "        [ 1.8796e-02,  2.6979e-02,  1.3241e-02,  2.7914e-02,  2.6861e-02, -1.6842e-02,  1.0052e-02,  2.3553e-02, -2.7427e-02, -1.9343e-02,  2.2545e-02, -1.3384e-02, -1.9607e-02, -1.4618e-03,\n",
       "          2.4487e-03, -1.2935e-02, -2.2972e-02, -2.5972e-02, -2.1002e-02,  3.4956e-02,  8.6627e-03, -2.1437e-02,  3.3590e-02,  1.9792e-02, -1.4163e-03, -3.3097e-03, -2.5342e-02, -1.3373e-02],\n",
       "        [ 2.9711e-02, -2.2141e-02,  4.2886e-03,  2.5150e-02, -2.3678e-02,  3.7046e-03,  1.0861e-02,  3.5056e-02, -2.0132e-02, -1.6570e-02,  1.0138e-02,  1.2072e-02,  3.0836e-02,  1.6803e-02,\n",
       "          2.9841e-02,  3.4730e-02, -9.6132e-03,  1.9338e-02,  2.2798e-02, -1.5595e-02, -1.2434e-02,  1.2320e-02,  2.0396e-02,  6.1655e-03, -2.3733e-02, -2.9404e-02,  1.2718e-02, -2.2195e-02],\n",
       "        [ 9.2743e-03,  1.9619e-03,  3.1725e-02,  2.4449e-02,  3.2248e-02,  2.2948e-02, -2.7984e-02,  2.5492e-02,  2.8798e-02,  1.9777e-02, -3.7627e-03, -2.3726e-02, -3.4296e-02,  2.8787e-02,\n",
       "          8.1830e-03, -3.0427e-02,  2.3047e-02,  3.1893e-02,  5.5306e-03,  1.9716e-02, -3.6311e-02, -2.1563e-02, -3.3929e-02, -3.3160e-02,  2.5111e-03, -4.4145e-03, -1.0600e-02,  1.3544e-03],\n",
       "        [ 1.8595e-02,  2.8290e-02, -1.8094e-02,  2.0580e-02, -1.3648e-02,  1.6732e-02, -2.1803e-02, -2.8043e-02,  2.9074e-03, -8.4849e-03, -4.1793e-04, -9.0391e-03, -1.0261e-02, -2.6850e-02,\n",
       "          3.2235e-02, -2.3355e-02,  3.3514e-02, -1.9719e-02,  2.0990e-02, -3.0800e-02,  3.0041e-02,  3.2446e-02,  3.4478e-02,  3.0127e-02,  3.4854e-02, -2.4413e-02, -7.4962e-03,  1.7429e-02],\n",
       "        [ 2.0331e-04, -2.8486e-02, -3.0242e-02,  3.4714e-02,  3.3975e-02,  5.2891e-03,  1.7405e-02,  3.1916e-02, -2.7612e-02, -2.9805e-02,  2.6351e-02,  3.4394e-02, -2.3880e-02, -3.3863e-02,\n",
       "          9.6791e-03, -1.0760e-02, -3.1994e-02, -2.2862e-02, -3.3457e-02,  1.9186e-02, -1.8298e-02,  2.2675e-02, -4.4533e-03, -1.7165e-02,  7.7176e-04,  3.5330e-02,  3.2632e-02, -1.7789e-02],\n",
       "        [-4.5266e-04,  2.3900e-02,  3.3423e-02,  2.3244e-02, -1.5952e-02,  1.9251e-02,  3.1234e-03,  2.1334e-02, -5.7725e-03,  6.2473e-03, -3.3475e-02, -2.4823e-02,  2.7904e-02, -2.1428e-02,\n",
       "         -2.4428e-02, -2.3628e-02,  9.8815e-03,  4.8129e-03,  1.6339e-02,  2.4855e-02, -1.5063e-02,  7.9751e-03,  3.3191e-02,  3.0620e-04, -3.8358e-03,  2.2900e-02, -4.9415e-03, -1.5830e-02],\n",
       "        [-3.5562e-02,  2.8699e-03,  1.0342e-02, -2.3263e-02, -5.4888e-03, -1.3148e-02,  1.6970e-02,  6.5005e-04, -2.8536e-02,  3.3215e-02,  2.4699e-02, -2.3269e-02,  2.9153e-02, -3.8155e-03,\n",
       "          9.5395e-03,  1.3381e-02, -2.4006e-02, -2.8903e-02,  1.9880e-03,  5.0263e-03,  2.2895e-02, -8.1852e-03,  3.1173e-02,  3.2590e-02,  3.3139e-02,  1.2684e-02, -1.2354e-02, -5.0985e-03],\n",
       "        [ 2.6791e-03,  3.3060e-02,  1.8296e-03, -2.8309e-02, -4.9953e-03, -2.3072e-02, -2.4657e-02, -1.2896e-02,  2.5578e-02, -3.3363e-02,  6.5889e-03,  2.6956e-02, -1.7974e-02,  1.1916e-02,\n",
       "          2.9188e-02,  2.9236e-03,  4.8159e-03, -1.0457e-02, -1.7739e-02, -2.0173e-03, -7.9490e-03,  1.0371e-02,  2.9917e-02, -2.7803e-02,  3.1787e-02,  2.4621e-02, -3.3973e-02, -3.2844e-02],\n",
       "        [ 2.3110e-02, -8.0960e-04,  2.5327e-02,  2.7596e-02, -6.5490e-03, -2.9048e-02,  2.6788e-03, -1.2365e-02, -6.3723e-03,  1.3840e-03,  3.7096e-02, -1.9973e-02,  5.5125e-03,  2.2910e-02,\n",
       "         -1.2642e-02, -3.2968e-02, -1.6270e-02, -1.6600e-02,  3.0828e-02, -7.8388e-03,  1.7049e-02,  2.7660e-02, -2.1496e-02, -7.1072e-03, -1.2663e-02,  1.2439e-02,  2.4467e-02, -2.5202e-02],\n",
       "        [-2.3859e-02, -3.6376e-04, -1.0673e-02,  3.3861e-02,  2.9044e-02, -1.4667e-02,  2.3828e-02, -7.6031e-04,  2.1271e-02, -4.9105e-03,  2.5785e-02, -9.9433e-04,  3.4853e-02, -8.2993e-03,\n",
       "          2.4509e-02,  3.2708e-02, -2.1295e-02, -2.4674e-02,  9.2792e-04, -1.9536e-02,  5.9921e-03,  8.0147e-03,  1.8139e-02,  3.4997e-03, -2.0968e-02, -1.4981e-02,  2.5049e-02,  3.1914e-02],\n",
       "        [ 1.6478e-02, -9.8413e-03,  2.1152e-02, -2.0721e-02,  2.8163e-02, -8.3458e-03, -5.8735e-03,  3.4351e-02, -3.1913e-02,  1.2480e-02, -9.5424e-03,  6.9360e-03,  4.6267e-03, -1.2927e-02,\n",
       "         -3.0776e-03, -8.9449e-04, -1.6727e-03,  3.4695e-02, -7.6762e-04, -1.6695e-02,  1.5070e-02, -2.2361e-02, -1.4973e-02,  1.1589e-03,  1.7213e-02,  4.0898e-03,  7.5423e-03, -8.5386e-03],\n",
       "        [ 3.2487e-02, -2.7469e-02, -2.6263e-02, -2.7123e-02,  2.0052e-02,  1.3386e-02,  8.0526e-03,  5.7796e-04, -1.7362e-02, -3.0342e-02,  2.4202e-02,  2.1593e-02,  1.7607e-02, -1.1796e-02,\n",
       "         -3.0211e-02, -1.2164e-02, -3.3959e-02, -8.3000e-03,  3.2813e-02,  6.1056e-03,  3.7412e-02, -6.1167e-03, -1.7980e-02, -2.7798e-02,  3.4728e-02, -3.3302e-02, -2.1834e-02, -1.4584e-02],\n",
       "        [ 1.1960e-02,  2.3910e-02,  1.5613e-02,  2.6374e-03,  3.3811e-02,  2.2980e-02, -1.5910e-02,  2.8421e-05,  3.0841e-02, -2.0946e-02, -1.1640e-04, -5.2644e-03, -3.4909e-02,  2.2653e-02,\n",
       "          1.8018e-02,  1.6996e-02, -1.9426e-02,  3.3780e-02, -2.0592e-02, -1.6257e-02, -2.9827e-02,  3.3317e-02,  1.9841e-02, -8.9863e-03, -2.9572e-02,  5.2923e-03, -3.3699e-02,  2.6461e-02],\n",
       "        [-2.5501e-02,  1.2259e-02,  1.5505e-03,  2.9444e-02, -1.4601e-02, -2.3223e-03, -2.5993e-02, -1.5846e-02,  3.5217e-02,  3.2394e-02,  1.3928e-02, -2.9676e-02,  1.8758e-02, -3.4917e-02,\n",
       "         -2.5517e-02, -3.2348e-02, -1.3994e-02, -1.2474e-02, -2.4258e-02, -1.9634e-02, -1.3562e-02, -1.4130e-02,  1.6854e-02,  2.7017e-02,  7.3784e-03,  5.3604e-03,  2.8601e-02, -4.5662e-03],\n",
       "        [-2.1988e-02, -2.9223e-02, -2.5917e-02,  9.3631e-03,  2.8669e-02, -3.1183e-03, -4.4533e-03,  3.1306e-02, -1.7065e-02,  2.8505e-02, -2.3615e-02,  3.1433e-02, -3.4268e-02, -2.4335e-02,\n",
       "          1.3552e-02,  1.9954e-02, -1.4725e-02, -1.3079e-02,  3.3251e-02,  2.5521e-02, -3.3144e-02,  2.9529e-02,  7.9410e-03, -2.8344e-02, -4.2633e-03, -1.1169e-03,  1.3612e-02, -2.8213e-02],\n",
       "        [-1.0653e-02, -9.5090e-03, -1.1727e-02,  9.7677e-03,  4.5918e-03,  1.3089e-02, -6.6265e-03,  1.3034e-04,  9.0692e-03,  1.8363e-02, -1.8899e-02, -1.2218e-02,  1.3894e-02, -1.8385e-02,\n",
       "          2.1835e-02,  6.4048e-03,  3.2354e-02,  3.2491e-02, -1.1982e-02,  2.1772e-02,  5.2871e-03,  1.4337e-02,  9.5214e-03,  4.9568e-03,  1.0718e-03,  4.8552e-03, -3.2840e-02, -2.0897e-02],\n",
       "        [ 1.8857e-03, -1.3880e-02,  6.5363e-03, -1.0569e-02,  6.4096e-03,  2.4683e-02,  1.2461e-02,  2.0968e-02, -2.1183e-02,  2.8970e-03, -2.9254e-02,  1.6804e-02, -2.8932e-02,  2.1945e-02,\n",
       "          9.7876e-03,  2.1583e-03,  7.8484e-03,  1.2332e-02,  2.9425e-02, -2.0639e-02, -2.1718e-02, -1.1277e-02, -3.4467e-02, -6.6153e-03, -2.3350e-02,  1.1798e-03,  2.6016e-03,  2.8054e-02],\n",
       "        [-1.7759e-02, -3.3795e-02,  8.3266e-03,  7.7279e-03, -3.0559e-02,  6.0052e-03,  3.2611e-02, -3.0394e-02, -2.6931e-02,  1.4825e-03,  2.1644e-02,  3.2306e-02, -1.7513e-02, -3.2649e-02,\n",
       "          1.3864e-02, -1.1775e-02, -2.6132e-03, -3.2011e-02, -2.1418e-02, -1.2311e-02, -2.3495e-02,  1.5973e-03, -1.2327e-02,  2.1344e-02,  2.0224e-02,  1.3612e-02,  2.7021e-02, -3.0458e-02],\n",
       "        [-1.6825e-02, -8.1205e-03,  3.3772e-02,  2.9821e-02, -3.5859e-03, -3.8089e-03, -2.5602e-02,  3.0489e-02, -1.0103e-02, -7.3859e-03,  3.3688e-02,  4.2877e-04, -1.2581e-02, -3.3731e-03,\n",
       "          3.2805e-02,  2.8000e-02, -3.4536e-02,  1.7392e-02, -1.3081e-02,  1.6717e-02,  2.5319e-02,  2.7518e-02, -2.2179e-02, -1.9486e-02,  1.3334e-02,  3.4252e-02,  3.9211e-04, -9.3778e-03],\n",
       "        [ 2.7552e-02, -1.3500e-02, -2.4855e-02,  1.4765e-02, -1.4204e-02,  2.7967e-02,  2.9331e-04, -1.8660e-02, -7.3153e-03,  2.0472e-02, -3.2026e-02, -2.9011e-03,  2.2659e-02,  2.2470e-02,\n",
       "          6.8929e-03, -9.6212e-03, -1.4527e-02,  2.3179e-02, -2.4254e-03,  7.9853e-03,  1.3444e-02,  6.9830e-03, -1.2130e-02,  2.5526e-03, -2.8085e-02,  2.6966e-02, -9.1442e-03,  1.9278e-02],\n",
       "        [ 2.3549e-02,  1.9741e-02,  2.7247e-02,  2.9003e-02, -2.2124e-02, -2.8421e-02,  8.0246e-03,  3.3794e-02, -1.9599e-02,  3.2354e-02, -2.2026e-05,  3.0739e-02,  4.9774e-03,  2.8501e-02,\n",
       "         -2.7780e-02,  1.9464e-02,  1.5982e-02, -2.7337e-02,  3.1889e-02, -3.3556e-04,  2.3811e-02, -4.6703e-03, -2.9452e-02,  2.3689e-02, -3.3725e-03, -1.8790e-02,  2.5523e-02,  1.4083e-02]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "we could reshape that into the original shape of the images\n",
    "'''\n",
    "w[0].view(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAwlUlEQVR4nO2d6W9d53Xuf2eeR/Ic8pA8HEWKg0bKkSVLHmIoDpzRLtKkQJoG6ccCRdHhD2j/gaIo+qUIkLZA06Zt4CAN0jSxHY+yZdmSaVESzXk8nM7hmedp3w/CWiHT5F75XgnXH/QChG348HDvvd53Dc/zrLVNhmEYPFr/X5f5//cFPFqPjPCpWI+M8ClYj4zwKViPjPApWI+M8ClYj4zwKViPjPApWI+M8ClY1vv9YLlcxjAMZmdn+d73vsf+/j67u7vs7+9z7Ngx/uqv/orPfOYzZDIZfvGLX7C0tERXVxejo6OYTCauXr3K9evXaTab+P1+HA4HhmHQbDZpt9ukUinW1tZoNBq0Wi3a7TZ9fX388R//MU899RTr6+t873vf486dO5w7d44rV67gcDj46U9/yksvvURvby9/+Zd/yTPPPEMul2N2dpbd3V2Gh4cZHh7G6XTi9XpxuVyYTCa9r2azSbFYpFarkcvl2NnZIZ/Pc+3aNa5fv05PTw9//ud/zqlTp9jb2+O73/0ut2/fptFoUKvVsNvtXLhwgSeeeALDMNjY2CCZTALwZ3/2Z/f1bP+vToLchMlkwmQyYTb/9q8RVET+efgB/DbE5Ne/9zf9rvy32WzGYrHo//tN32kYxv/4+fVr+PXv/t/dy2+75/v5jt+07vsk5PN5vZB4PI7f72dgYIBqtYrX62Vrawu73U6tVsPn83H8+HFKpRKzs7M0Gg3S6TRut5tWq0W5XCafz+N2uwmHw3oqdnd3sVqtTExMMDg4iN/vJxgMksvlaDQa9Pf3Y7fbGR4eprOzE5vNxqlTp7DZbFitVhKJBK+99hqNRoNCoUC9Xmdra4tisYjdbqezs5NQKARAvV6n0WjgdDrx+/3Y7XYsFgv1ep1qtYphGDgcDgC2t7eJRCKk02m6u7sxDINSqUQul8NisdDT00NnZyf1ep12u00+n/+tG+z/yQi7u7sAtNttTp8+TavVwul04nK5KBQK3Lhxg5/+9Kf09fXx3HPPMTY2xs2bN3n55Zc5ODggHA7j8/loNBrs7++zvb1NPB5nZGSESCQCwMbGBoZhcPnyZb785S9jNptJJpPs7OzQbDY5deoUk5OTRKNRIpEINpuNZ599lmeffZZsNsu//du/8Q//8A/EYjFOnz5NOBxmZ2eH7e1tWq0WXV1ddHV10Wq1SKVSFAoFjh07xosvvojX68VisVCpVCiVSgB4vV51wYVCAYfDwbFjxxgfHyeVSrG+vo5hGBw7doxoNEqlUsEwDFKp1MMxQrVaBe6dBJ/PB4Ddbsdms9Fut2k0GuRyOd1pFosFgFqtRqVSwWw24/F4qNfrWK1WDMPAZDLhcDhwuVzYbDYsFgvtdhu/308gEMAwDJLJJNlsFqvVisPhwO12Y7fbMZvN+vs2m41ms0m9Xmd7exuTycTY2BjNZpNKpaKnwmq16kbKZrOUy2WKxSLtdvvIvRqGgc1mw+l06ncXi0XMZrNuvHq9jtPp1M9arVYsFgsWi+V/657/n4ywuroKQFdXF93d3ZjNZtbX15mbm8NsNjM6OsqZM2doNpusrKywsLBAu93mypUrmEwmurq6iEQilEolrl69SiQSIRgMEgqF8Hq92O122u02zWaTUqlEuVymXq/zwQcfMDMzg9frpa+vD6/Xi9frJZPJqOEsFgvZbBabzcbp06dxu90UCgWazSaGYdDV1UW73aZQKDA/P4/H4+Hs2bOMjo4SDAYxDINcLqeu1Gq1EgqFmJycxGazEYlEcLvdWK1WXC4XZrOZQqFAo9GgXq+Ty+VIp9O0223Gxsbo7u5+OEZYXl7GZDIRCAQIh8NYrVZmZ2e5fv06Pp+Pb3/725w4cYKdnR3+5V/+haWlJc6dO8fzzz+vD87pdFKr1TAMA4vFgtvtprOzU3c33MtWyuUyhUKBQqHAzMwMP/3pT4lGo1y8eJFIJILdbmd3dxez2Uyz2aTZbNJqtbDZbBw/flwfTCaTIRwO09fXh9lsZnZ2lpmZGfr6+vj93/99nnjiCer1OtlslnQ6TbPZxOPx4PF4CAQC+Hw+DfxySuVvWa1W6vU65XKZZDKJz+fD4XDQ29vL8ePHH44RarUaANlsVt1DNpul1WrpQxUXUS6XyeVylEolGo3GkYtvNBoA2Gw2AAqFArVajXa7TTQapdVq4fF4MJvN+mOxWDTF9Pv9+lBMJtMRVyLf32q11E253W4cDgcmk0mTia6uLgzDoFKp0Gw27z0Iq1Wv0TAM6vU69Xpd3arJZKLZbFKtVmm1WnrN8v8kY7Jarer2HrgR9vf3AUin01y/fl0ftjwciQuFQoFEIsHdu3cJhULMzs4SDofxeDw4nU7q9TrJZBK73U6hUGBubo5SqURPTw+f//zn8fl8jIyMaNwZGBhgenqagYEBrly5Qk9PD9VqVd1BPp/XU7O+vs7a2hpdXV2cPn2azs5ODMOg1WphNpt57rnn+L3f+z0ajYae2FgsxvT0NKFQiIODAzY3NzUwAzgcDmKxGH6/n1KpxNbWFvl8nlKpRKvVwm634/V6CQQCGiM/6frEKere3h7Ly8s0m03Gx8cZHh7G4XCoEeR47u3tsbGxwfLyMgcHB9jtdpxOJ+12m3K5rJnInTt3SCQSfPnLX2Z6elpTVrvdTrPZJBKJ0Nvby8DAAKOjo0SjUQqFAqlUinq9jsPhwGq1auq7urqKz+ejt7eXkZERCoUC+/v7mM1mJicnGRwcpFQq8f3vf59f/vKXTExMcPbsWTweD7lcTn8KhQKlUgmHw0Gr1SIWi5HJZJibmyObzer9SLIgQfxwzfLAjZDNZgHw+/1cvHgRi8VCOBzWwJrP50kkEhwcHBCNRpmYmKC/vx+Px6PZTKPRwDCMI1lGMBikWCxqTt5utymVSuqmOjo6GB8fp7u7W09aOp1mY2ODcrlMuVymWq1SKpVwOp0MDg4SCASo1Wrk83n29/fZ3NwEIB6Pa3Ig7qRUKrG6uqoVs7i+w+40lUpht9spl8t4PB4N0F6vV91erVbTWNRqtQA4ceLEgzXC3bt3MZlMfOELX+Bb3/oWwWBQ3UC5XGZubo433nhDM5Rz587pMbVYLORyOfL5vPpml8tFq9Wiv78fr9dLKBSi1WpRrVbZ2tpibW0Nt9vNmTNnuHTpEtVqlWQyyf7+Pnfv3uX69euUSiVcLhculwur1UokEiEej2O1WslkMuTzeTY2Nrh165b6e7PZTLVa5eDgAMMw2Nzc5J/+6Z+wWq0MDAxw5swZotGoQjImk4l6vc7u7i4+n4+hoSECgYBes9lsplQqsb+/T7FY5N1332VlZQXDMPjnf/7nB2uEXC6H2WzG5XIxNDSE3+8nm82STCb1Aa+urtLZ2cng4CBdXV0aqFutFvl8nkajoUFVHp7X66XZbGrO3Ww2yWQyJBIJPWWdnZ0Ui0W2t7e1eFtaWiKTydDV1UVnZydOp5OOjg66urqo1+sUi0XdxSsrKzSbTX2wsmtNJhPFYpHl5WWq1SpWq5Xp6Wl1r+VyGUBrBYvFciRzCgaDmM1mrbIPDg5YWVnh6tWr9/tYP5kRotEoJpOJbDbLO++8Q2dnp6ZzwWCQ8+fP09fXh91uJxAIYLVa9SQYhkEmk1FYwuPxEAqF8Pv9jI+PU61W6ejooFar0Ww2sVqtRKNRXC4X6XQam81GNpvl448/JplMYrFYuHjxIq1Wi2KxSKlUwmw2axCu1WpkMhkqlQoAY2NjwL0sZnV1FYvFwsDAAAMDA6RSKW7evMnOzo7GKIfDwebmJgcHB9hsNjo6OvB4PNhsNgqFAhaLhWQyqQVsZ2cnnZ2duFwuRkdHNQN84EaQG9nZ2eG73/0ubreb559/nitXruD3++nt7cVqtVIqldjc3KRQKBAKhYjH4wAsLCywurqK3W4nGo1iNpuJRCKMjY3hcDgUwazX69hsNoaGhmi326yurjI/P086nWZubo58Ps/U1BQvvPACDoeD69ev8/7776sRGo0GpVKJnZ0dstksHR0dnDx5Uq/t9u3bhEIhXnjhBcbGxtjc3CSfz+sJfPfdd2m1WqTTadLpNF6vl7GxMa2NUqkU2WyWxcVFPvjgAwzD4Jvf/CZf+tKXaLfbXLp0iY6OjodjBLfbDaCBsVAo6JEXF2Oz2bDZbJpnt9ttzRRkh4rLkeAlvyPHularYbFYNDvKZDIUCgXy+TyVSoVKpYLVaiUYDGp6LPWCVLD1el1hDJPJhNvtxmazUalUqNfrtFotdYXBYJBAIEAoFKJWq1EsFhVet9lsuFwuzYDgXjEpCcL6+vqRbM9kMmG32/F4PP8DCnkgRggEAgD09fURDoc1I/jRj36ExWIhEAjg8XgoFApsbW1RKpXo7u5mdHQUi8XCwsKCFlq5XI6VlRXgf8K/ZrNZ8SbDMDSLsVqtdHR0aJXtcrm0yq5UKlSrVQ38ZrMZm81GT0+P1huGYWg2FwwGcblcioM9//zznD9/nlqtRqlUUsypUqngdDo5fvw4kUiEfD7P7OwsmUxGOYh2u63cSrPZ5KOPPmJ+fh7DMPjDP/zDh2OEsbExzp07h8Vi4T//8z/54Q9/SKPRUMKk0WhQrVZpNpt0dnbS3d2t+bPX68VsNlMsFjWt3NnZoVgs0tPTw+DgIE6nk2q1qg+43W4rSBYOh7WydrvduvuKxSK5XI5EIkE+nycSiXD+/Hl1IYeN0NHRgd/v15rF7XZz+vRprYjlpDQaDZrNJmazGZ/Ph91uJ5lMcvv2bXK5nBqh1WqRTCa1yJudneXmzZv3+1g/mRFarRYmk0kfnsViUfcihZJAFIfZMbvdjt1u19IeUHfRarUUU5K0z26302q1qFQqipJKtiKuzm63K6TQbrdxuVw0m0196F1dXYRCIUKh0BGE0+/34/P5cDqdioqKyxFkVxBQQV8FtnC5XHo6pU4Ih8N6D+I+PR7Pw4sJ6XQauBcTlpaWMJlM5HI5xsbGqNVqpFIprapNJpPm3U8++SRer5fNzU22t7cVMCuXy3R3d/Pss88Si8UUJ2q1Wnz44YfMz8/jcDg4ceIEw8PDWK1WNSbc4x5arRb1ep2hoSEANajP5+PYsWN6Wmw225FNUC6XefXVV1leXtYMLRwOE41GicVimEwmbty4wWuvvQbcywxDoRDtdpt2u01XV5dW5SaTifPnzxOPx3WjHjt27OEYoVgsAvfgCyEt4vE4XV1dNJtNCoWCBlsJ1OFwmKmpKQ1UyWRSQbBkMklfXx+nT58mHo+rP67VaszOzrKzs6M7rKOjQ8E4yVBWVlYol8s0m026u7txOBwMDw8TCoVwOp0Eg0EN+ILnCJvWbre5ffs2//Ef/0FXVxfPPPMM0WiUyclJwuEwZrOZO3fu8KMf/Yh2u611j9xPOBxWZs9mszEyMqJGMpvNDw/KLhQKwL3d1tXVhcViweFwKDztcDj0yA8MDOB2u4lEIlQqFc0gDMOg3W5TqVQoFovqwlqtlqKXctzFAMVikYODA93FZrOZdDpNMpmkXq8r9yAoqGQ6ApVLVianpl6vk8lk8Pv9TE9PE41GiUajhMNhAK2ky+WyBm+/34/H48Hn8+mJsdlsR+AYIffb7bZCMA/cCFtbW5hMJi5fvsxXv/pVvF6vopbFYlHx9OHhYb761a8yODiomZLsfAl4yWSSra0t4vG4IpKS+olfFSJG8n2pmEulEm63W1PTTCbD3t6exiCA0dFRvvKVrxCPx9nc3GR2dlZJIgH9RkZGuHz5slbvZrOZg4MDrl69SqVSIZVK0d/fj81mIx6PEw6HicViTE1N6cmUGLW4uMhbb72Fw+FgenpaXdoDN0K5XMZkMuFyuejv71e+eG9vj0ajocHP7XYzODhIf38/iUSChYUFCoXCEXq0VqtRLpcplUpUq9Ujgfxwfi6pYqlUIpvNcvv2bdLpNNFoVOPEzs4OiUSCRqNBpVJR3iOZTNLR0cHu7i4rKyvk83lqtRq1Wo1IJMLZs2cZGxvTAq/dbqtBxS2KAMDr9eLz+QgEAvj9frxer54ESUQ2NzdxuVxaGD4UIwwPD2MymbDZbCSTSYrFIvPz8ywuLlKpVMjlchqs3377bWKxGKlUiq2tLZrNJrFYjPHxcbLZLKlUSjGc999/n729PUwmExaLBcMwyGazCo4dZuTGx8ep1+scHByQSCQoFou43W6F07u6uggEAjgcDvL5PDMzM2xubrK3t0e5XCYYDBIOh3G73ezu7nLr1i06OzsZGhrSIC7G/PXU2O/3K2whKbS4K7iH0EpBKPfT19f3YI0wNTUF3EvV5ubmMJlM3Lx5k3feeYdms6kM1traGsvLyxiGgdvtxufz4fV6OXXqFE899RSlUkl5hnK5zM9+9jOsVqvuNIfDQX9/v562oaEhTT0llXz33Xf527/9WxKJBGNjY/T29hIOh7ly5QrDw8MkEgl+8IMfsLy8TCqVIpFIAPf48f7+fgDW1ta4c+cOU1NTnD59mkgkotRts9nE6/Xi8XiOaKxyuRy3bt1ie3sbt9uN3+/XXX/q1CmazSbZbJbt7W0Azp0792CNIMHGMAxVKAgMcFgU1Ww2FTGtVquarkp2A/c4iY6ODq1ym80mtVpNyRmhJyXwHyZPLBaLVrxC6FitVs2IAoGAqitSqZSCe4evQ2obgTlk18tphF/BKSaT6Yh8JZ/Pk06nFeOy2+26AeUkSap+v+sTkzqiKjCZTEQiEb75zW9Sq9XY2Nhgd3eX7u5u+vr6cLvd5HI5Dg4OjshfTCYTTz75JL29vWQyGZaXl8nn82oYu92O3+9XtyBuTh60ZGUvvviiwh8rKytK1vf19WkM2d3dZWJigqmpKdVH7e/v43Q6OX36NH19fbRaLd566y0VbWWzWZrNpkLtwhtLVnYYRDx58iRer5dqtUqtVqNarSqE8VB0R7lcTv+5v7+P1WrlhRde4Atf+ALVapX/+q//IpvNat7d09PD3Nwc165dU2i3Xq9jt9sZHx/nxIkTpNNp3n33XXZ2dggGgwpxCG9cqVTIZDI0m03FhMxmM36/nytXrtBqtfjBD37Aj3/8Y6LRKHt7e1rRHxwckMlkiEQims29/vrrvP766wSDQc6ePcvJkydZW1vjr//6r3nvvfe0jjiskRUW0OFwUKvVVEoTjUbp7OwkEAjoiSuXy6RSKba3tx+OEcQdBYNBFWIJzi7VaLVapVqtksvl8Pl81Go1LZTE9QgZbrVaqVar+Hw+6vU6wWCQYDCIyWRSIwg6WiwWcblchEIhdSlOp1PRUHFN4mJE5zQ8PEwkEsHlcuF0OjXHF2GCAH/CgQim1Wq1tNKW2uewXEeyuUwmo1nV/v6+wjler/e+DfCJjDAyMgJAJBJhcHAQj8dDLBajo6MDi8VCuVxmZWWFvb09xeHdbrdiQkKStNttcrkc1WqV7u5uLl26xPHjx7HZbPog19bW+Oijj5TkabfbDAwM8OKLLxKJRPB4PKpfHRwc5Ny5c7pJ0uk0LpeL73znO6oADwaDWCwWPvOZzzA1NaXuc2ZmBpPJxGOPPcb58+dZWFjg2rVrFAoFRkdH6e3txeFw0NnZid/vVyVJuVym1Wrxve99T1PuWq2G2+3m1KlTTE9PfyIV3n0bobu7G5PJxOTkJBMTEzidTnUR1WpVA2EqlWJzcxOz2cyJEye4dOkSHo+HVCrFwsIClUqFpaUlkskkTz31FM899xw9PT3qahqNBo1Gg8XFRTKZDOl0mlKpxJNPPonJZMLj8ahsst1u09HRQTwe15suFAoEAgFGRkZwOp1K8hiGQXd3N06nk3K5zM2bN3nttdfo7+/nxRdfpKurC5fLxerqKg6HQ/lmQQjECEtLS6TTaW7duqUuWE5nPB7n4sWLTE5OPpw6QR54NptVBbbAEELyiLaou7sbj8dDNBrVhwBoduP1eikUCpjNZi3aJENpNBr4fD4ee+wxDcyFQoF4PK70aGdnp3LYTqeTvr4+LBaL8gyAyiKFKBLoRPROqVRKT9phAZvgXJVKhdXVVVVXyGaDexpc0dA2Gg1CoRDhcJje3t4j/MUDN4LstLt37/LLX/5SyfRSqaR4TE9PDyMjIzz11FP09vaqVrVWqxEKhejt7aVWq9FoNDCbzdjtdmWnJJgCDA4O8id/8icYhqEBemdnh7fffpv9/X0ef/xxvvjFL6q0/rOf/aw+HIvFQj6f5/333yeZTGolD6iWqFKpsLy8TDabPVK1u1wu3UB7e3u8//77BINBstnsEWmjkP0DAwOUSiWmpqYYGxtTZFWAzIdiBBHOzs/Pk8lkSCaTZDIZLBYL3d3dhMNhgsGgFlCCiFYqFUKh0BHNjs/n0weWyWRIpVLs7OxgsVgYHx+np6cHk8mk/tZsNrO/v8+NGzfo6uqiWCxqLu90OoGju29ra4tEIqE7Fu6BcwcHB1p1C1YlgVaU4yIgWF5exuv1MjIyoqCd2+1WZFbEw5FIRKtuIYseihHOnDmDYRg4nU6y2SzhcJhwOMzBwQFut1sFWoJgihjr2LFjVKtVzSLa7TZer5d4PE53dzcjIyN0dHRQLBZJJBJUKhXcbjeVSkWJE7fbrQirED7ZbFY1rqK+FvypXq/T19en7snj8WAYhvIgAuKFw2GcTieLi4skk0kajYZW7VINt9ttrbjl4QubJwWlVNci8/yk676NIGqyQCCgVWM+n1cE9ZlnnmFkZIRqtapKBY/HwxNPPEGr1WJmZkbhjlgsRjAYVKliIBBQkdbW1hYff/wxb731Fp2dnfzu7/4uExMTWklLRb66ukogEGB/f18RWnF1PT09PPnkkwp3CDPn8/kU0hYKtlwuMzMzQ7VaZXBwkOnpaWw2GysrK8rgffjhh9y6dUufhdzD8PAwHo+HcDhMIBDQNP2hdeoI/SeZgLgVwzBUjSZ6o0wmQ6PROKJSkOrTbDbjcDjweDy43e7/kUUYhkE+n2drawu4F2CFDhXNkoiRJVFIp9NqIDkpUlkLtGAYhp4KiROSidXrdRUYH66Qpf6RUyYKQWlkAbSeEE5cTsJDyY5EQXD79m3ee+89isUikUhEc2hRPwg0IQikaPzPnDmj/V6HH8Jbb72lFfW3v/1tms0miUSCRCKBxWJhZmZGqcypqSnGx8e1oEun0+zs7GgskcYTaWDJZrMqTpMHOjIyQq1WI5lMks/ndaNI/9re3p6eNpPJRCgU4plnnmF0dJTt7W1efvllNjc39cQLxNHT06P3JHL7B26E119/HcMwmJub46233qJcLvPUU08xMDCgGiA5JVLpys612Wz4/X6GhoZotVoqtl1bW+PVV19lbW2NL33pS/zBH/wBbreb2dlZXnnlFfL5PHfv3mVzc5Px8XG+8Y1vEI/HWV1d5d1331Xtz9LSEh6Ph0gkQjgcxmQysbCwANzjhwcGBlRFHYvFVI8E9074yMgIFouF3d1d1tbWFHsC6Ojo4LnnnuOJJ55geXmZxcVFtre3qdVq7O3tUa/XVYYjUPwnneN130bIZDIAWhnKzhJVQjabxe12K+97uMnDbDbTbrfVZQhiKUYTVyWkzmG+WVqX5G8eRj+r1arCJz6fj1AoREdHB+12W32+QM2GYSjQJhmRuB3ZvUIgCbXZ399PLBZTlyoIbldXl9YKouyTDlHhtT/JMt3v+LVvfetbAArdmkwmzbmF1BcGSriAnp4eVUnv7e2xtbWlWh/hjz/++GPVCgk08sorr/DKK6/gdru5fPkyExMT+iANw2B1dZW5uTnNgvr6+ggEApw8eZJoNEqtViOdTh8hX9rtNh9++CEzMzPYbDbGxsbo6+tT5XaxWCSdTiv4Njk5yYkTJ7TolAJzYWGBbDbLwcGBdpVGo1GV2Vy+fFnJHIkb/6d13ydhfX1d5RwjIyPY7XZu377N3bt3NStptVpatAheIztDmDiz2axEjbSkCmH/8ccfUy6XuXPnDjMzM3R3d/P1r3+dCxcukM1m+eCDD9jb21N8yjAMOjs7efzxx/H5fPT19SkgaLfbVRAs9cbm5iY///nPFTY/duwYuVyOvb097RJNJBLY7XY++9nPcunSJUwmkza92Gw2JicnsVgsrKysqBxyY2OD9957T5tZIpHIwwnMJ0+eBCAUCqnKQDCaarXK+vo6BwcH5HI5tre3VfgViUSwWq0UCgXlFiT3r1Qq7O/vqwRdNKhOp1NBOtGHimB4f3+fer2uouJ2u83GxgYej4daraZKQYEhDnffB4NBBgcHNZtzu92a38tpEVGZVOlms1nFCCIwcDgc6pbcbrfCGYKL+f1+TCYTnZ2dD9YI3/nOd4B7zNLm5ib1ep3jx48zOTlJJpPh5ZdfZnV1lZWVFer1On6/n1QqpUF7bm6Oubk5zYwMwyCdTrOwsEAulyMWizE6OqpFlPSJZTIZlpaWWFtb49q1aywvLzM9Pc2FCxfweDysr6/z85//XNNTQVx/53d+R0VlwthNTU1pkTUxMUFnZ6fqhCQ2RCIRbQj/8Y9/rOJjj8dDZ2cnsVhMMz6Px6OyneXlZZLJJD/5yU945ZVXAPjRj370YI0gfm5vb0/FryI1FGSz1WqpNN5ms9Hf369UaD6fJ5lMKglfLBb1dBwcHCiMIQHP5/PhcrnUv2cyGba2ttje3ubkyZNK6q+srLC2tqZq8WKxyMWLFxXqONzp6ff7icVi2O12lejIKRBOQu4jnU4rYCjyyHq9rkYVKEROrQiZ5+fnVS55v+u+jTA3NwegwU50OolEgmq1ysDAAF/72tc0eOXzec10bDYbwWBQK99cLsfMzIxKRGTySj6fV22niI4l45Iu/UAgQG9vr7qRaDSqvcuSKMTjcYrFIqlUimKxqKmkoKmHszVpdiwWi3oCbTYb6XRa01Thj0ulEouLi5qYSF1gGAY9PT3azfPQOvpfeukl4N6JmJycxOv1sra2xu3bt3G73TzzzDNMTEywvLzMv/7rvzI7O6tAmMjU/X4/BwcHvPnmm1y/fp1Lly7xjW98g+HhYZaWlvjv//5vCoUCvb29Oh1gb2+PbDaL2WxmYmKCsbEx4vG4yhxHRka05UqUdna7nUwmoyrpV199lVKpxIULFzhz5syRxnBpGMxms8Tjcc6cOYPH4zkyVWB7e1thGpH4+P1+otGoutZ4PI7dbqenp4eOjo6HE5hTqRSAKp2F1BHhlGh6pAqVizuMgooPFbclu14CpOwgQSvlu8UNCFTtdDq1/hDsRmTt0rQi4uJKpUIymaRUKinsfrhWEFWI0JrCJ8OveiU8Ho/C9vl8nnw+j9lsVt2qfF5qlmg0et8G+ERGWFlZwWQy0d/fTyQSwefzcenSJQYGBrR1VXyqdMA0Gg1WVlZUyCvEiEyCGRkZUdzG6XTy7LPPUq1WWV1d5dq1a5oZyYASMa5oTKWfLBaLqeYnn8/j9Xo1BRYYQ2Q4N27c0BbZgYEBNjY2WFhYYG1tjWAwyO7urnacikRyYGCA3t5eVldX+fDDD1lcXCQWi3Hx4kVtMhepveBVD8UIa2trmEwmLl68qExSR0cHY2Nj6h+lu9Pn89HR0UGz2WRjY0NPjsvlwmKxHBmx02g0yGazuFwuTpw4QavVYnt7m/fff19bo+RHxF+hUEjxGbmWw6MUAoEAfX19GuDdbjcHBwe88sorvPrqq8pll0oltre3mZmZIZFI0NPTw9ramkLyhxsg5XcEwX322Wc5fvw4gUBAY4oozkWK+cCNIO6n1WodaZ6AX/UjmM1mFYRJQBY/LdmHII6Hb6peryv0IJi9EPmCSYkiQirgQqGAYRhaTxyeDiPQtaCfkokd7goVeaXH46G/v18llwLPyH1Jk4soL6LR6BEtrqCm0jshEwE+ybpv2OLFF18EIBwO09PTcyS9s9vtqtdcXFzkJz/5CfPz86ol8ng8nDx5krNnz6qSWo7w1tYW5XKZxx57jCtXruB0OpmdneXq1atHlHjhcJgTJ07g8/lYW1vjvffeU31pKBRSZWC1WiUej/Pkk0/qbI0f/OAH5HI5hoeHOXbsGA6Hg0AgoG5NYs/CwgIzMzNUKhUdbOXz+Thz5gxdXV0606larR4ZJhKPx3Xo1Msvv8y1a9cwDIO/+7u/uy8j3PdJ6O3tVXrz6tWr2p4kyuWenh5CoRCJRIKNjQ1SqZRmKoIX9fX1aWpYqVSUC15fX8fhcHD+/HlVfg8MDBzZ6T09PUxOTmplvL6+ztbWltYoUhkLRDE5OYnVamV5eZm3336bXC7H1NQUTz31FFarVUE7aUKXk/z973+fzc1Nenp66O/v18pamiMlpd7Y2OC1116jUqkoQdVoNFhbW1NF3/2u+zbC4OAgcK+PWchxyWosFos20lWrVR0EIvFBdKLCv/p8PqUvJVVMpVLMz88TDofZ398nl8tpz7N0i8rItEqlwujoqDarSDEm1bhAJZJ9nThxgmKxSDQa1c+IwAvQ+sHpdGq3jiCk0mEkEIwQQ9lsVvGynZ0d3G63qkakfet+1327o48//hjDMJifn+e1116jWCxqxVyr1VhaWmJjY4N4PM7jjz9OLBbTYX0mk4np6WmOHTum0vdiscjc3Bx///d/z+3bt1WiLt3+kvJ+/vOfZ3h4mEwmw5tvvkkikWBycpLp6WlVxYl7ODwurVKpqDg3kUhgGAYjIyNa+Ut6KnED0A5QKcpkmo2waqFQiImJCUKhkHYbSWKRy+WwWq10d3cTDAYB+NM//dP7MsJ9nwTBVLLZLD6fT12F6GzK5TKJRIJIJEJfX5+OuRFo2OfzaeomJ0J4BBkMmM/ndXqXEEJChebzedbX11lcXKS/v5/u7m7cbrcaAX5V2cpAkGq1qrM4zGazpquy72RymNQI0uwo6e76+roOThRdq1zL4WlgOzs7zMzMEAgE6OrqIh6PP5xiTVJCv9+vwixxRwIHS75/9+5ddnd3FRG12WzaeyZg19bWFoVCgVOnTjE0NMT+/j6Li4sqbTx79qwq4qSh5Pjx4wwNDTE5OamQiAwcabfbR6aPHeaIhelbW1tTKaasUCjE4OCgNo7I5MdUKoXT6dTOI0mvRRkuq9VqKSYVDoeJx+MMDAzctwH+r4wQDoe5ePGi3pgItwTfL5VKSj2Oj4/z2GOPEQgESCaTFAoFMpkMv/zlL7l58ybj4+N86Utfor+/n2vXrilkPDExwec+9zlKpRL//u//zptvvsn09DR/9Ed/RDwePyJ9WVxc5PXXX6darWp63N3dzdNPP01nZ6caQfjsn/3sZzSbTUVGT5w4oR37Gxsb/OIXvyCRSDA4OEgsFiMUCvH000+r0Gt1dVW5ZXFpkUiEer2u7mpoaOjhnATZPdInIEaQixHhk4xGS6fTOmNODCgGExZLBtvK6BqHw6GjLkVrJK6lVCppzn64814GDlYqFR1A5XA4VIpzePKL7HCBKiQLk/5jUVqnUinVx8qsCnG7UrcAGtylGd7v9x8ZqvjAjSDdJxaL5QhvC2iRBPf0pqOjoyp3FyRTen+9Xq/Org4Gg2xvb2ufl8DXdrtdJe6nT5/GarUSDoe5e/eu9jdLbSDDR0T1IKN93nrrLcWvDg8MfPHFF6nX6+zs7Ggv2507d8jlciwtLREOh1WTKq20S0tL2pQoPRBSkJpMJo4fP87ly5cVJpEOpQduBIF1D6sJJDiJK5AKV1R45XJZe5DtdjtdXV14PB5GR0fp6emhXq8r95zL5bQt1mazqQZIOvpTqRQ3btwgk8kwMTHBpUuXtJFPNohAKalUijfeeEOnGst1vfDCCzz//POUy2Vee+018vk85XKZDz74gMXFRbLZrO5mOYnlcpmPP/6YtbU1PRlCAIkQYWhoSKU45XJZadUHboTDoiZBOw+rHsxms+Lsh0+KNFaIuwCUkzYMQ/NxcUMyz0KCn7SoSjYlchWBTmRil/xd+TncZyzqjkajofWM0+nUTXGY0PH7/VpfyEAU2dUC6NlsNmXUxI2Jyz28KaRJ8YEZYWtrS1uVBMIVAhx+1QwojdhyOkSAe3gQoeiOotEo586do7Ozk+3tbebn53Vkv9/vp16vazN4OBzmwoUL+P1+EokEb7zxhsrqRe4oIrRQKKQnSOIH3Ott/vGPf6za2QsXLmifss1m01l2h0dGtFotEokEOzs7+Hw+7XE4ODhgYWFBq2Tp+rl9+7bWJX/zN3/zYI0gwJaokKW3YHd3F5vNxrFjx+jq6lKCRLQ/EoAdDofWBNlslkKhgNvtJhaLKRYlVag8TMMwNHUViUp3d7cqM5LJJNFoVBsOJR32er3KdEkdIQZdWFigr6+PJ554QkfsyMkul8va6iSTI6VTqF6v4/P5FAkQWWWr1WJvb4/t7W2KxSIzMzPcvn0b4MEbQcbHiOxRYGtpshMCvdVq0d3dremh9ADIHDxBMgcHB4nH40e0oUIXHhwcaB2xs7OjjSLixlwul6rABYsS2EEkL5JZCbYlBWMoFMLn8ykKeri2ODzkSjItee3A1taWTpF3uVwqkREjS8HX3d2tio/7XfdtBGmSCIVCKuVIJpPqs7e2tkgmk3R3d3Pu3Dn8fj+rq6v68Dc2Nvjoo4+IxWIqKxSeQZTTgulcu3aNX/ziF5TLZebn51lbW8Pr9ZJMJlXu+M1vfpNms8kHH3zA1atXtSIeHBzU0T+1Wo2hoSGGh4ex2WyazgKaxtrtdt0EtVqNnZ0drcCFt7527Rp37tw5wmlIciKdSqlUinA4zFe+8hXOnDnzcIwgebJACYDusMMyQnlPQjgcplgs0tHRodhMNpslEAgQi8WIxWKaw4tGSFLKVCrF0tKSki4S7A7PnJAUVeZei85JKls5YcFgUN+1IDWJ8Bgy2EoeqHAP1WqVbDaLw+HQOCGxTxIGmYQpp317e1vR3vsNyJ/YCHKhkn61Wi2Wl5dViSbH2mw2q/grl8vhcDgIhUKcPHmSgYEBHXf5zjvv6Dgeh8Oh8+akvVWg8Hg8ztjYGENDQ5q6CovVbDZZXFxkZ2dHU9v9/X3tFhL/vri4CKDjn6VOEDW3QNU7Ozusrq4qFDE4OIjVamVqagqv16sGkSJ0Z2cHQNHXjo4OBfTgV8MbH7gRcrmcgmPiCqTzxeFwUCgUiMViDA4OUq/XVZ96/vx5otEo+XyeH/7wh/zjP/4jw8PDfPGLXyQej6v/tlgsNJtNtre3cTqdR95z4HA4tE9aiPuPPvqIDz/8kFarxdLSEl6vl8cee4ynn36anp4eFhcXeemll8jn85w8eZITJ05Qq9W4desWS0tL2ntXr9dV3OxyuRgcHFQt1OOPP86ZM2dIJpO88cYbasC7d+/SarW4fPkyp0+fxuv1UqlUVDLa09PzYI0gvKkEIpkjLXm4ZBEyv1pmGAk6Ke+ukc/u7e0RDodVASHYvLgm6RsQF3J4gJQ0GUqlLqeiXC5rjDo8x0LEY7lcTmEWucZaraZSzMOqcYEyhLy3Wq06K1vk/nLdMqxQ6hiZkna/676N8JOf/ARA+8z8fj9PPPEE/f39lEolVlZWlCyPRCJ0dHSwurrK3bt3dTybdNyMj48fmTctc5NWV1dpNBoMDQ3xF3/xF0e6ZsrlMjdu3NDBUSJ9h3vTVKS1VlThlUpFJ4QJEXR4OIgwZSLHkepXkgQhaUSUJoXm9PQ0hmFw7do11tfXyefzOh3GZDJpVgfwuc997sEa4aWXXtLJXyMjI3g8Hqanp3nyyScplUr87Gc/w2w268uD/H4/1WqVmZkZcrmcAm4y+O/wWzyq1Sp7e3vMzc3Rbrf5+te/zmc/+1larRaLi4ssLS2RzWaZnZ09MubMYrEQi8U4fvy4Qs4icxTh1ubmJsViUfVCMl5NirtGo6GxRlgzqZ6LxSKtVovNzU0F9aanpwkEArTbbd577z09BUJubW9v68zX+12fyB1JU0g2m1VKUfoUDkMRsqO8Xq+SL9JKJfIQoQ4loMrfEN8srqFcLuswWNGEShEmbqujo0NT3cNvB5HiUIYTyqQWQHVLAseL+xEDCDwhnLVMojn86hihUWXgrlz/Q2uh9Xg8wL3ZEVevXtUBH1LgSN+wXLzP5+Ps2bMMDg5SrVY1mGWzWW7duqUB/MqVK3R0dOBwOFTyeOvWLX1jyOuvv87i4qLqWcPhMKlUSt/x09vby7PPPkuz2eTmzZvMzMwovCzV/de+9jXMZjPvvfceV69exWKx6IRHCfK5XA6Xy6XDa/v6+ojFYpTLZR2WnslkCIVCKsv/whe+oH0Pb775pvILIhZ+4EaQ2iCbzWo/mAQ7aUuV8ZWyQ30+HwMDA7TbbWZnZ1lbWyOTyXD9+nXm5+e5cOECzzzzjFKf0gGztbWlTeh3797lxo0b9Pb2cvHiRTo7O5W8gXsV/MDAAJVKhatXr7K4uKh/32Kx0NHRwdTUFHa7ndnZWW7cuIHdbqe7uxuv16uVcSaTIRaLqXovGAxqn4GMWBBWsFwu6ytjBFq5deuWptRSU93v+sSvffz1xjgRWf22JS7r8O/L7/2mY3v4u+Tff50TuN/rPHwNn+Tz/6fP/abPHr7W+/2ben33q7Z4tB7eevQq4E/BemSET8F6ZIRPwXpkhE/BemSET8F6ZIRPwXpkhE/BemSET8F6ZIRPwfpfYKJpEd3PwVMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "and we could even have a look and there it is\n",
    "so you can see this is something which is kind of learning to find things at the top and the bottom and the middle (me: pointing to the image)\n",
    "and we can look at the 2nd one w[1] - no idea what is that showing\n",
    "I probably got far more than I need, which is why they're not that obvious\n",
    "\n",
    "but you can see, here's another thing that's looking pretty similar w[4]\n",
    "king of looking for this little bit in the middle, \n",
    "so yeah, this is the basic idea, to understand the features that are not the 1st layer but later layers you have to be a bit more sophisticated\n",
    "but you can see the first layer ones , you can just plot them\n",
    "'''\n",
    "show_image(w[0].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have something that is rather **magical:**    \n",
    "\n",
    "1. **A function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters**    \n",
    "1. **A way to find the best set of parameters for any function (stochastic gradient descent)**  \n",
    "\n",
    "This is why deep learning can do things which seem rather magical, such fantastic things.  \n",
    "Believing that this combination of simple techniques can really solve any problem is one of the biggest steps that we find many students have to take.  \n",
    "It seems too good to be true — surely things should be more difficult and complicated than this?  \n",
    "Our recommendation: try it out!  \n",
    "We just tried it on the MNIST dataset and you have seen the results.  \n",
    "And since we are doing everything from scratch ourselves (except for calculating the gradients) you know that there is no special magic hiding behind the scenes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is no need to stop at just two linear layers.**  \n",
    "**We can add as many as we want, as long as we add a nonlinearity between each pair of linear layers.**  \n",
    "As you will learn, however, the deeper the model gets, the harder it is to optimize the parameters in practice.  \n",
    "Later in this book you will learn about some simple but brilliantly effective techniques for training deeper models.\n",
    "\n",
    "We already know that a single nonlinearity with two linear layers is enough to approximate any function.  \n",
    "**So why would we use deeper models?**  \n",
    "**The reason is performance.**  \n",
    "**<u>With a deeper model (that is, one with more layers) we do not need to use as many parameters;</u>**  \n",
    "it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices, and few layers.  \n",
    "\n",
    "**That means that we can train the model more quickly, and it will take up less memory.**  \n",
    "In the 1990s researchers were so focused on the universal approximation theorem that very few were experimenting with more than one nonlinearity.  \n",
    "This theoretical but not practical foundation held back the field for years.  \n",
    "Some researchers, however, did experiment with deep models, and eventually were able to show that these models could perform much better in practice.  \n",
    "**Eventually, theoretical results were developed which showed why this happens.**  \n",
    "Today, it is extremely unusual to find anybody using a neural network with just one nonlinearity.\n",
    "\n",
    "**Here is what happens when we train an 18-layer model using the same approach we saw in <<chapter_intro>>:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltr01/mambaforge/envs/fastai23/lib/python3.11/site-packages/fastai/data/transforms.py:225: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(col):\n",
      "/home/ltr01/mambaforge/envs/fastai23/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ltr01/mambaforge/envs/fastai23/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.075454</td>\n",
       "      <td>0.021230</td>\n",
       "      <td>0.995584</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ltr01/mambaforge/envs/fastai23/lib/python3.11/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ok, so just to compare we could use the full fastai toolkit   \n",
    "grab out data loaders by using data loaders from folder as we've done before\n",
    "and create a cnn_learner and a resnet and fit it for a single epoch and whoa, 99.7%\n",
    "\n",
    "we did 40 epochs and got 98.3%, as I said, \n",
    "using all the tricks you can really speed things up and make things a lot better\n",
    "so by the end of this course or at least both parts of this course\n",
    "you'll be able to, from scratch, get this 99.7% in a single epoch\n",
    "\n",
    "\n",
    "'''\n",
    "dls = ImageDataLoaders.from_folder(path)\n",
    "learn = vision_learner(dls, resnet18, pretrained=False,\n",
    "                    loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nearly 100% accuracy!**  \n",
    "**That's a big difference compared to our simple neural net.**  \n",
    "But as you'll learn in the remainder of this book,  \n",
    "**there are just a few little tricks you need to use to get such great results from scratch yourself.**  \n",
    "You already know the key foundational pieces.  \n",
    "(Of course, even once you know all the tricks,  \n",
    "**you'll nearly always want to work with the pre-built classes provided by PyTorch and fastai,  \n",
    "because they save you having to think about all the little details yourself.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jargon Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations: you now know how to create and train a deep neural network from scratch!  \n",
    "We've gone through quite a few steps to get to this point, **but you might be surprised at how simple it really is.**  \n",
    "\n",
    "Now that we are at this point, it is a good opportunity to define, and review, some jargon and key concepts.  \n",
    "\n",
    "A neural network contains a lot of numbers, but they are only of two types:  \n",
    "numbers that are calculated, and the parameters that these numbers are calculated from.  \n",
    "This gives us the two most important pieces of jargon to learn:\n",
    "\n",
    "- **Activations:: Numbers that are calculated (both by linear and nonlinear layers)**\n",
    "- **Parameters:: Numbers that are randomly initialized, and optimized (that is, the numbers that define the model)**\n",
    "\n",
    "We will often talk in this book about activations and parameters.  \n",
    "Remember that they have very specific meanings.  \n",
    "They are numbers.  \n",
    "They are not abstract concepts, but they are actual specific numbers that are in your model.  \n",
    "**Part of becoming a good deep learning practitioner is getting used to the idea of actually looking at your activations and parameters,**  \n",
    "and plotting them and testing whether they are behaving correctly.\n",
    "\n",
    "**Our activations and parameters are all contained in *tensors*.**  \n",
    "These are simply regularly shaped arrays—for example, a matrix.  \n",
    "Matrices have rows and columns; we call these the *axes* or *dimensions*.  \n",
    "**The number of dimensions of a tensor is its *rank*.**  \n",
    "There are some special tensors:\n",
    "\n",
    "- Rank zero: scalar\n",
    "- Rank one: vector\n",
    "- Rank two: matrix\n",
    "\n",
    "**A neural network contains a number of layers.  \n",
    "Each layer is either *linear* or *nonlinear*.  \n",
    "We generally alternate between these two kinds of layers in a neural network.**  \n",
    "Sometimes people refer to both a linear layer and its subsequent nonlinearity together as a single layer.  \n",
    "Yes, this is confusing.  \n",
    "**Sometimes a nonlinearity is referred to as an *activation function*.**\n",
    "\n",
    "<<dljargon1>> summarizes the key concepts related to SGD.\n",
    "\n",
    "```asciidoc\n",
    "[[dljargon1]]\n",
    ".Deep learning vocabulary\n",
    "[options=\"header\"]\n",
    "|=====\n",
    "|Term | Meaning\n",
    "|ReLU | Function that returns 0 for negative numbers and doesn't change positive numbers.\n",
    "|Mini-batch | A small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch).\n",
    "|Forward pass | Applying the model to some input and computing the predictions.\n",
    "|Loss | A value that represents how well (or badly) our model is doing.\n",
    "|Gradient | The derivative of the loss with respect to some parameter of the model.\n",
    "|Backward pass | Computing the gradients of the loss with respect to all model parameters.\n",
    "|Gradient descent | Taking a step in the directions opposite to the gradients to make the model parameters a little bit better.\n",
    "|Learning rate | The size of the step we take when applying SGD to update the parameters of the model.\n",
    "|=====\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "relu: function that returns 0 for negatives\n",
    "mini-batch: a few inputs and labels, which optionally are randomnly selected \n",
    "forward pass: is the bit where we calculate the predictions\n",
    "loss: the function that we're going to take the derivative of \n",
    "gradient: the derivative of the loss with respect to each parameter\n",
    "backward pass: is when we calculate those gradients\n",
    "gradient descent: full thing of taking a step in the direction opposite to the gradients after calculating the loss\n",
    "learning rate: the size of the step that we take\n",
    "\n",
    "other things to know: perhaps the 2 most important pieces of jargon are:\n",
    "    parameters = the numbers that we're learning \n",
    "    activations = the numbers that we're calculating, so every relu that's calculated, every matrix multiplication element that's calculated\n",
    "    \n",
    "so activations and parameters are all of the numbers that are in a neural network\n",
    "so be careful when I say from here on in these lessons activations or parameters you make sure you know what those mean\n",
    "because that's the entire set of numbers that exist inside a neural network \n",
    "so activations are calculated, parameters are learned\n",
    "\n",
    "we're doing this stuff with tensors, which are regularly shapped arrays\n",
    "rank zero we call scalar, rank 1 vector, rank 2 matrix \n",
    "rank 5 tensors are very common in deep learning \n",
    "\n",
    "non linearity to choose: relu, leaky relu\n",
    "some are taking longer and are more accurate, some are faster and a bit less accurate\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> note: _Choose Your Own Adventure_ Reminder:  \n",
    "> Did you choose to skip over chapters 2 & 3, in your excitement to peek under the hood?  \n",
    "> **Well, here's your reminder to head back to chapter 2 now, because you'll be needing to know that stuff very soon!**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How is a grayscale image represented on a computer? How about a color image?\n",
    "1. How are the files and folders in the `MNIST_SAMPLE` dataset structured? Why?\n",
    "1. Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "1. What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
    "1. What is a \"rank-3 tensor\"?\n",
    "1. What is the difference between tensor rank and shape? How do you get the rank from the shape?\n",
    "1. What are RMSE and L1 norm?\n",
    "1. How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\n",
    "1. Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.\n",
    "1. What is broadcasting?\n",
    "1. Are metrics generally calculated using the training set, or the validation set? Why?\n",
    "1. What is SGD?\n",
    "1. Why does SGD use mini-batches?\n",
    "1. What are the seven steps in SGD for machine learning?\n",
    "1. How do we initialize the weights in a model?\n",
    "1. What is \"loss\"?\n",
    "1. Why can't we always use a high learning rate?\n",
    "1. What is a \"gradient\"?\n",
    "1. Do you need to know how to calculate gradients yourself?\n",
    "1. Why can't we use accuracy as a loss function?\n",
    "1. Draw the sigmoid function. What is special about its shape?\n",
    "1. What is the difference between a loss function and a metric?\n",
    "1. What is the function to calculate new weights using a learning rate?\n",
    "1. What does the `DataLoader` class do?\n",
    "1. Write pseudocode showing the basic steps taken in each epoch for SGD.\n",
    "1. Create a function that, if passed two arguments `[1,2,3,4]` and `'abcd'`, returns `[(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]`. What is special about that output data structure?\n",
    "1. What does `view` do in PyTorch?\n",
    "1. What are the \"bias\" parameters in a neural network? Why do we need them?\n",
    "1. What does the `@` operator do in Python?\n",
    "1. What does the `backward` method do?\n",
    "1. Why do we have to zero the gradients?\n",
    "1. What information do we have to pass to `Learner`?\n",
    "1. Show Python or pseudocode for the basic steps of a training loop.\n",
    "1. What is \"ReLU\"? Draw a plot of it for values from `-2` to `+2`.\n",
    "1. What is an \"activation function\"?\n",
    "1. What's the difference between `F.relu` and `nn.ReLU`?\n",
    "1. The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity.  \n",
    "   So why do we normally use more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create your own implementation of `Learner` from scratch, based on the training loop shown in this chapter.\n",
    "1. Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s).  \n",
    "   This is a significant project and will take you quite a bit of time to complete!  \n",
    "   You'll need to do some of your own research to figure out how to overcome some obstacles you'll meet on the way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "2020 1h:10min \n",
    "now you know how to create and train a neural net\n",
    "let's cycle back and look deeper at some applications\n",
    "so we're going to try to interpolate in \n",
    "from one end we've done they're kind of from scratch version\n",
    "at the other end we've done the kind of 4 lines of code version\n",
    "and we'll gradually nibble at each end until we find ourselved in the middle and we've touched on all of it\n",
    "so let's go back up to the kind of the 4 lines of code version and delve a little deeper\n",
    "this is chapter 5\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
